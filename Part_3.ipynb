{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Andreas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Andreas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\Andreas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "basedir = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(basedir,'reviews.csv'))\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_id</th>\n      <th>id</th>\n      <th>date</th>\n      <th>reviewer_id</th>\n      <th>reviewer_name</th>\n      <th>comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2818</td>\n      <td>1191</td>\n      <td>2009-03-30</td>\n      <td>10952</td>\n      <td>Lam</td>\n      <td>Daniel is really cool. The place was nice and ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2818</td>\n      <td>1771</td>\n      <td>2009-04-24</td>\n      <td>12798</td>\n      <td>Alice</td>\n      <td>Daniel is the most amazing host! His place is ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2818</td>\n      <td>1989</td>\n      <td>2009-05-03</td>\n      <td>11869</td>\n      <td>Natalja</td>\n      <td>We had such a great time in Amsterdam. Daniel ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2818</td>\n      <td>2797</td>\n      <td>2009-05-18</td>\n      <td>14064</td>\n      <td>Enrique</td>\n      <td>Very professional operation. Room is very clea...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2818</td>\n      <td>3151</td>\n      <td>2009-05-25</td>\n      <td>17977</td>\n      <td>Sherwin</td>\n      <td>Daniel is highly recommended.  He provided all...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "def process_reviews(df):\n",
    "  tokenized = []\n",
    "  tagged = []\n",
    "  lower_tagged = []\n",
    "\n",
    "  mylen = len(df)\n",
    "  count = 0\n",
    "  for index, row in df.iterrows():\n",
    "    token = word_tokenize(row.comments)\n",
    "    tokenized.append(token)\n",
    "    tagged.append(pos_tag(token))\n",
    "    # lower_tagged.append(list(set(pos_tag([item.lower() for item in token]))))\n",
    "    lower_tagged.append(pos_tag([item.lower() for item in token]))\n",
    "    count += 1\n",
    "    \n",
    "    if count % 20000 == 0:      \n",
    "      print(f'{count} out of {mylen}')\n",
    "\n",
    "  df['tokenized'] = tokenized\n",
    "  df['tagged'] = tagged\n",
    "  df['lower_tagged'] = lower_tagged\n",
    "\n",
    "\n",
    "  # df['tokenized'] = [ word_tokenize(row.comments) for index, row in df.iterrows()]\n",
    "  # print('Tokenizing done!\\n')\n",
    "  # df['tagged'] = [pos_tag(row.tokenized) for index, row in df.iterrows()]\n",
    "  # print('Tagging done!\\n')\n",
    "  # df['lower_tagged'] = list(set([pos_tag([item.lower() for item in row.tokenized]) for index, row in df.iterrows()]))\n",
    "  # print('Lower tagging done!\\n')\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rGYB8gx5Qq-P",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20000 out of 20000\n",
      "<ipython-input-7-e3008de9489c>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokenized'] = tokenized\n",
      "<ipython-input-7-e3008de9489c>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tagged'] = tagged\n",
      "<ipython-input-7-e3008de9489c>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lower_tagged'] = lower_tagged\n"
     ]
    }
   ],
   "source": [
    "df = process_reviews(df[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       listing_id        id        date  reviewer_id reviewer_name  \\\n",
       "0            2818      1191  2009-03-30        10952           Lam   \n",
       "1            2818      1771  2009-04-24        12798         Alice   \n",
       "2            2818      1989  2009-05-03        11869       Natalja   \n",
       "3            2818      2797  2009-05-18        14064       Enrique   \n",
       "4            2818      3151  2009-05-25        17977       Sherwin   \n",
       "...           ...       ...         ...          ...           ...   \n",
       "19995      481664  51662420  2015-10-22     46551788        Martin   \n",
       "19996      481664  51885927  2015-10-25     37047887          Susi   \n",
       "19997      481664  52491199  2015-10-30      8500879      Kristine   \n",
       "19998      481664  53267155  2015-11-07     46466936     Alexander   \n",
       "19999      481664  55206755  2015-11-29     45639757         Vidya   \n",
       "\n",
       "                                                comments  \\\n",
       "0      Daniel is really cool. The place was nice and ...   \n",
       "1      Daniel is the most amazing host! His place is ...   \n",
       "2      We had such a great time in Amsterdam. Daniel ...   \n",
       "3      Very professional operation. Room is very clea...   \n",
       "4      Daniel is highly recommended.  He provided all...   \n",
       "...                                                  ...   \n",
       "19995  I had an amazing stay on Eltjo and Liselores b...   \n",
       "19996  Thank you so much for that wonderful stay. The...   \n",
       "19997  Eltjo and Liselore were very nice people and t...   \n",
       "19998  The hosts are friendly) all as on foto) далее ...   \n",
       "19999  Amazing experience! Eltjo and liselore were aw...   \n",
       "\n",
       "                                               tokenized  \\\n",
       "0      [Daniel, is, really, cool, ., The, place, was,...   \n",
       "1      [Daniel, is, the, most, amazing, host, !, His,...   \n",
       "2      [We, had, such, a, great, time, in, Amsterdam,...   \n",
       "3      [Very, professional, operation, ., Room, is, v...   \n",
       "4      [Daniel, is, highly, recommended, ., He, provi...   \n",
       "...                                                  ...   \n",
       "19995  [I, had, an, amazing, stay, on, Eltjo, and, Li...   \n",
       "19996  [Thank, you, so, much, for, that, wonderful, s...   \n",
       "19997  [Eltjo, and, Liselore, were, very, nice, peopl...   \n",
       "19998  [The, hosts, are, friendly, ), all, as, on, fo...   \n",
       "19999  [Amazing, experience, !, Eltjo, and, liselore,...   \n",
       "\n",
       "                                                  tagged  \\\n",
       "0      [(Daniel, NNP), (is, VBZ), (really, RB), (cool...   \n",
       "1      [(Daniel, NNP), (is, VBZ), (the, DT), (most, R...   \n",
       "2      [(We, PRP), (had, VBD), (such, JJ), (a, DT), (...   \n",
       "3      [(Very, RB), (professional, JJ), (operation, N...   \n",
       "4      [(Daniel, NNP), (is, VBZ), (highly, RB), (reco...   \n",
       "...                                                  ...   \n",
       "19995  [(I, PRP), (had, VBD), (an, DT), (amazing, JJ)...   \n",
       "19996  [(Thank, NNP), (you, PRP), (so, RB), (much, JJ...   \n",
       "19997  [(Eltjo, NNP), (and, CC), (Liselore, NNP), (we...   \n",
       "19998  [(The, DT), (hosts, NNS), (are, VBP), (friendl...   \n",
       "19999  [(Amazing, JJ), (experience, NN), (!, .), (Elt...   \n",
       "\n",
       "                                            lower_tagged  \n",
       "0      [(didnt, VBP), (thanks, NNS), (and, CC), (is, ...  \n",
       "1      [(and, CC), (is, VBZ), (!, .), (of, IN), (help...  \n",
       "2      [(it, PRP), (were, VBD), (provided, VBN), (and...  \n",
       "3      [(and, CC), (close, JJ), (is, VBZ), (helpful, ...  \n",
       "4      [(it, PRP), (all, DT), (and, CC), (buses, NNS)...  \n",
       "...                                                  ...  \n",
       "19995  [(and, CC), (return, VB), (looking, VBG), (!, ...  \n",
       "19996  [(were, VBD), (and, CC), (this, DT), (is, VBZ)...  \n",
       "19997  [(it, PRP), (were, VBD), (and, CC), (for, IN),...  \n",
       "19998  [(необычность, NN), (желанием, NNP), (чаще, NN...  \n",
       "19999  [(were, VBD), (and, CC), (when, WRB), (amsterd...  \n",
       "\n",
       "[20000 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_id</th>\n      <th>id</th>\n      <th>date</th>\n      <th>reviewer_id</th>\n      <th>reviewer_name</th>\n      <th>comments</th>\n      <th>tokenized</th>\n      <th>tagged</th>\n      <th>lower_tagged</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2818</td>\n      <td>1191</td>\n      <td>2009-03-30</td>\n      <td>10952</td>\n      <td>Lam</td>\n      <td>Daniel is really cool. The place was nice and ...</td>\n      <td>[Daniel, is, really, cool, ., The, place, was,...</td>\n      <td>[(Daniel, NNP), (is, VBZ), (really, RB), (cool...</td>\n      <td>[(didnt, VBP), (thanks, NNS), (and, CC), (is, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2818</td>\n      <td>1771</td>\n      <td>2009-04-24</td>\n      <td>12798</td>\n      <td>Alice</td>\n      <td>Daniel is the most amazing host! His place is ...</td>\n      <td>[Daniel, is, the, most, amazing, host, !, His,...</td>\n      <td>[(Daniel, NNP), (is, VBZ), (the, DT), (most, R...</td>\n      <td>[(and, CC), (is, VBZ), (!, .), (of, IN), (help...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2818</td>\n      <td>1989</td>\n      <td>2009-05-03</td>\n      <td>11869</td>\n      <td>Natalja</td>\n      <td>We had such a great time in Amsterdam. Daniel ...</td>\n      <td>[We, had, such, a, great, time, in, Amsterdam,...</td>\n      <td>[(We, PRP), (had, VBD), (such, JJ), (a, DT), (...</td>\n      <td>[(it, PRP), (were, VBD), (provided, VBN), (and...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2818</td>\n      <td>2797</td>\n      <td>2009-05-18</td>\n      <td>14064</td>\n      <td>Enrique</td>\n      <td>Very professional operation. Room is very clea...</td>\n      <td>[Very, professional, operation, ., Room, is, v...</td>\n      <td>[(Very, RB), (professional, JJ), (operation, N...</td>\n      <td>[(and, CC), (close, JJ), (is, VBZ), (helpful, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2818</td>\n      <td>3151</td>\n      <td>2009-05-25</td>\n      <td>17977</td>\n      <td>Sherwin</td>\n      <td>Daniel is highly recommended.  He provided all...</td>\n      <td>[Daniel, is, highly, recommended, ., He, provi...</td>\n      <td>[(Daniel, NNP), (is, VBZ), (highly, RB), (reco...</td>\n      <td>[(it, PRP), (all, DT), (and, CC), (buses, NNS)...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>481664</td>\n      <td>51662420</td>\n      <td>2015-10-22</td>\n      <td>46551788</td>\n      <td>Martin</td>\n      <td>I had an amazing stay on Eltjo and Liselores b...</td>\n      <td>[I, had, an, amazing, stay, on, Eltjo, and, Li...</td>\n      <td>[(I, PRP), (had, VBD), (an, DT), (amazing, JJ)...</td>\n      <td>[(and, CC), (return, VB), (looking, VBG), (!, ...</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>481664</td>\n      <td>51885927</td>\n      <td>2015-10-25</td>\n      <td>37047887</td>\n      <td>Susi</td>\n      <td>Thank you so much for that wonderful stay. The...</td>\n      <td>[Thank, you, so, much, for, that, wonderful, s...</td>\n      <td>[(Thank, NNP), (you, PRP), (so, RB), (much, JJ...</td>\n      <td>[(were, VBD), (and, CC), (this, DT), (is, VBZ)...</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>481664</td>\n      <td>52491199</td>\n      <td>2015-10-30</td>\n      <td>8500879</td>\n      <td>Kristine</td>\n      <td>Eltjo and Liselore were very nice people and t...</td>\n      <td>[Eltjo, and, Liselore, were, very, nice, peopl...</td>\n      <td>[(Eltjo, NNP), (and, CC), (Liselore, NNP), (we...</td>\n      <td>[(it, PRP), (were, VBD), (and, CC), (for, IN),...</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>481664</td>\n      <td>53267155</td>\n      <td>2015-11-07</td>\n      <td>46466936</td>\n      <td>Alexander</td>\n      <td>The hosts are friendly) all as on foto) далее ...</td>\n      <td>[The, hosts, are, friendly, ), all, as, on, fo...</td>\n      <td>[(The, DT), (hosts, NNS), (are, VBP), (friendl...</td>\n      <td>[(необычность, NN), (желанием, NNP), (чаще, NN...</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>481664</td>\n      <td>55206755</td>\n      <td>2015-11-29</td>\n      <td>45639757</td>\n      <td>Vidya</td>\n      <td>Amazing experience! Eltjo and liselore were aw...</td>\n      <td>[Amazing, experience, !, Eltjo, and, liselore,...</td>\n      <td>[(Amazing, JJ), (experience, NN), (!, .), (Elt...</td>\n      <td>[(were, VBD), (and, CC), (when, WRB), (amsterd...</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 0\n",
    "# print(len(df.tagged[num]), len(set(df.lower_tagged[num])))\n",
    "# list(set(df.lower_tagged[num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df.lower_tagged[0][1][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "  cent_list, cont_list = [], []\n",
    "\n",
    "  for review in df.lower_tagged:\n",
    "    cent_list.extend([word for word in [list_of_words[0] for list_of_words in review if list_of_words[1][0] == 'N']])\n",
    "    cont_list.extend([word for word in [list_of_words[0] for list_of_words in review if (list_of_words[1][0] == 'J') or (list_of_words[1][0] == 'V')]])\n",
    "    \n",
    "  cent_dict = Counter(cent_list)\n",
    "  cont_dict = Counter(cont_list)\n",
    "\n",
    "  cent_vocab = [key for key, value in sorted(cent_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "  cont_vocab = [key for key, value in sorted(cont_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "\n",
    "  return cent_vocab, cont_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F_R5l4IVSk9-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "comments = df.comments\n",
    "type(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    Daniel is really cool. The place was nice and ...\n",
       "1    Daniel is the most amazing host! His place is ...\n",
       "2    We had such a great time in Amsterdam. Daniel ...\n",
       "3    Very professional operation. Room is very clea...\n",
       "4    Daniel is highly recommended.  He provided all...\n",
       "Name: comments, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ddnfCbQWRd5R"
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "  sentences = []\n",
    "  comments = df.comments\n",
    "\n",
    "  for comment in comments:\n",
    "    sentences.extend([sentence for sentence in comment.split('.')])\n",
    "  \n",
    "  print('yolo')\n",
    "  # print(sentences)\n",
    "  \n",
    "  coocs = {}\n",
    "\n",
    "  count = 0\n",
    "  for center_word in cent_vocab:\n",
    "    count += 1\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "      if center_word in sentence:\n",
    "        words_in_sentence = word_tokenize(sentence)\n",
    "        words.extend([word for word in words_in_sentence if word in cont_vocab])\n",
    "    \n",
    "    center_word_dict = dict(Counter(words))\n",
    "    coocs[center_word] = center_word_dict\n",
    "    print(f'{count} out of 1000')\n",
    "    \n",
    "  # cent_dict = Counter(cent_list)\n",
    "  # cont_dict = Counter(cont_list)\n",
    "\n",
    "  \n",
    "  return coocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "  sentences = []\n",
    "  comments = df.comments\n",
    "\n",
    "  for comment in comments:\n",
    "    sentences.extend([sentence for sentence in comment.split('.')])\n",
    "  \n",
    "  print('yolo')\n",
    "  # print(sentences)\n",
    "\n",
    "  sentences_per_center_word = {center_word : Filter(sentences, [center_word]) for center_word in cent_vocab}\n",
    "  \n",
    "  print('swag')\n",
    "\n",
    "  words = []\n",
    "  coocs = {}\n",
    "\n",
    "  count = 0\n",
    "  for center_word, sentences in sentences_per_center_word.items():\n",
    "    count += 1\n",
    "    if count % 100 == 0 : print(f'{count} out of {len(sentences_per_center_word)}')\n",
    "    # print(value)\n",
    "    for sentence in sentences:\n",
    "      for word in word_tokenize(sentence):\n",
    "        if word in cont_vocab:\n",
    "          words.extend(word)\n",
    "    coocs[center_word] = dict(Counter(words))\n",
    "\n",
    "  # coocs = {key: dict(Counter([word for word in word_tokenize(value) if word in cont_vocab])) for key, value in sentences_per_center_word.items()}\n",
    "\n",
    "\n",
    "  #   coocs = {}\n",
    "\n",
    "  #   count = 0\n",
    "  #   for center_word in cent_vocab:\n",
    "  #     count += 1\n",
    "  #     words = []\n",
    "  #     for sentence in Filter(sentences, [center_word]):\n",
    "  #         words_in_sentence = word_tokenize(sentence)\n",
    "  #         words.extend([word for word in words_in_sentence if word in cont_vocab])\n",
    "      \n",
    "  #     center_word_dict = dict(Counter(words))\n",
    "  #     coocs[center_word] = center_word_dict\n",
    "  #     print(f'{count} out of 1000')\n",
    "    \n",
    "  # cent_dict = Counter(cent_list)\n",
    "  # cont_dict = Counter(cont_list)\n",
    "\n",
    "  \n",
    "  return coocs \n",
    "\n",
    "def Filter(string, substr):\n",
    "    return [str for str in string if any(sub in str for sub in substr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "iTT_TOkaSoXL",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "yolo\n",
      "swag\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-54b4f6c20a7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcoocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_coocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcent_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-a0f27bae768f>\u001b[0m in \u001b[0;36mget_coocs\u001b[1;34m(df, cent_vocab, cont_vocab)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# print(value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcont_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m           \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     return [\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     ]\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     return [\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     ]\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocs['coffee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "  coocdf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "  for index, row in coocdf.iterrows():\n",
    "    for word in cont_vocab:\n",
    "      try:\n",
    "        coocdf[word][index] = coocs[index][word]\n",
    "      except: \n",
    "        coocdf[word][index] = 0\n",
    "\n",
    "  return coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwAflxldSrbg"
   },
   "outputs": [],
   "source": [
    "coocdf = cooc_dict2df(coocs)\n",
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "  pmidf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "  N = 0\n",
    "  for index, row in coocdf.iterrows():\n",
    "    N += sum(row)\n",
    "\n",
    "  for index, row in coocdf.iterrows():\n",
    "    for word in cont_vocab:\n",
    "      try:\n",
    "        pmi = df[index][word] / (sum(df[word])/N / sum(row)/N)\n",
    "        pmidf[word][index] = np.log([pmi])[0] \n",
    "        print(pmidf[word][index])\n",
    "      except: \n",
    "        pmidf[word][index] = 0\n",
    "      \n",
    "  return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGftXjXRSuQw"
   },
   "outputs": [],
   "source": [
    "pmidf = cooc2pmi(coocdf)\n",
    "pmidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in cont_vocab:\n",
    "    if len(pmidf[name][pmidf[name] > 0]) > 0:\n",
    "        print(pmidf[name][pmidf[name] > 0 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "  top_words = sorted([df[word][center_word] for word in cont_vocab])[:N]\n",
    "  return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I038zG1Sw62"
   },
   "outputs": [],
   "source": [
    "topk(pmidf, 'coffee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfcm5-7b0HKO"
   },
   "source": [
    "# 3.b Ethical, social and legal implications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3uf-Qq4tYg"
   },
   "source": [
    "Local authorities in touristic hotspots like Amsterdam, NYC or Barcelona regulate the price of recreational apartments for rent to, among others, ensure that fair rent prices are kept for year-long residents. Consider your price recommender for hosts in Question 2c. Imagine that Airbnb recommends a new host to put the price of your flat at a price which is above the official regulations established by the local government. Upon inspection, you realize that the inflated price you have been recommended comes from many apartments in the area only being offered during an annual event which brings many tourists, and which causes prices to rise. \n",
    "\n",
    "In this context, critically reflect on the compliance of this recommender system with **one of the five actions** outlined in the **UK’s Data Ethics Framework**. You should prioritize the action that, in your opinion, is the weakest. Then, justify your choice by critically analyzing the three **key principles** outlined in the Framework, namely _transparency_, _accountability_ and _fairness_. Finally, you should propose and critically justify a solution that would improve the recommender system in at least one of these principles. You are strongly encouraged to follow a scholarly approach, e.g., with peer-reviewed references as support. \n",
    "\n",
    "Your report should be between 500 and 750 words long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6QJyuP6I1Ht"
   },
   "source": [
    "### Your answer here. No Python, only Markdown.\n",
    "\n",
    "Write your answer after the line.\n",
    "\n",
    "---\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python395jvsc74a57bd03f75a622fdbe68ac4774c6ea619d86cc770141a8bef94a85fce2870eb7cb09bf",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3f75a622fdbe68ac4774c6ea619d86cc770141a8bef94a85fce2870eb7cb09bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}