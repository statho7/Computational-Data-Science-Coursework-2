{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shouldnt',\n",
       " 'dont',\n",
       " 'youll',\n",
       " 'isnt',\n",
       " 'arent',\n",
       " 'youd',\n",
       " 'havent',\n",
       " 'hadnt',\n",
       " 'shant',\n",
       " 'wont',\n",
       " 'didnt',\n",
       " 'mightnt',\n",
       " 'youre',\n",
       " 'mustnt',\n",
       " 'shouldve',\n",
       " 'thatll',\n",
       " 'doesnt',\n",
       " 'wasnt',\n",
       " 'youve',\n",
       " 'its',\n",
       " 'hasnt',\n",
       " 'wouldnt',\n",
       " 'werent',\n",
       " 'shes',\n",
       " 'couldnt',\n",
       " 'neednt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We want to find the alternative forms of stopwords that have the \"'\" symbol in them \n",
    "in order to be able to add also to stopwords the word without this symbol'''\n",
    "\n",
    "pattern = r'\\w+\\'\\w+'\n",
    "\n",
    "new_stopwords = []\n",
    "for word in sw:\n",
    "    # If it finds a word that contains \"'\" it appends the word in new_stopwords list\n",
    "    if len(re.findall(pattern,word)) == 1:\n",
    "        new_stopwords.append(re.findall(pattern,word)[0].replace('\\'',''))\n",
    "new_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arent',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'couldnt',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'didnt',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doesnt',\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'dont',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hadnt',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'havent',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'isnt',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mightnt',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'mustnt',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'neednt',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shant',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'shouldnt',\n",
       " 'shouldve',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thatll',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'wasnt',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'werent',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wont',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'wouldnt',\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'youve'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After checking those \"new\" words we add them to the stopwords variables named sw\n",
    "for word in new_stopwords:\n",
    "    sw.add(word)\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "basedir = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(basedir,'reviews.csv'))\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "def process_reviews(df):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    # Initialize 3 lists one for each column we will create\n",
    "    tokenized_col = []\n",
    "    tagged_col = []\n",
    "    lower_tagged_col = []\n",
    "\n",
    "\n",
    "    mylen = len(df)\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through the given dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # tokenize the words for the comments of a row\n",
    "        token = word_tokenize(row.comments)\n",
    "        # Append the tokenized words to the proper list\n",
    "        tokenized_col.append(token)\n",
    "        # Tag the tokenized words of the row and then append them to the proper list\n",
    "        tagged_col.append(pos_tag(token))\n",
    "        # lower_tagged.append(list(set(pos_tag([item.lower() for item in token]))))\n",
    "        # Make the tagged words lowercased and then if they are not stopwords append them to the lower_tagged_col list\n",
    "        lower_tagged_col.append(pos_tag([item.lower() for item in token if item.lower() not in sw]))\n",
    "        count += 1\n",
    "\n",
    "        if count % 50000 == 0: print(f'{count} out of {mylen}')\n",
    "\n",
    "    # Set as values of the 3 new columns the proper list we created for each one\n",
    "    df['tokenized'] = tokenized_col\n",
    "    df['tagged'] = tagged_col\n",
    "    df['lower_tagged'] = lower_tagged_col\n",
    "\n",
    "    # Return the modified dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rGYB8gx5Qq-P",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 out of 452143\n",
      "100000 out of 452143\n",
      "150000 out of 452143\n",
      "200000 out of 452143\n",
      "250000 out of 452143\n",
      "300000 out of 452143\n",
      "350000 out of 452143\n",
      "400000 out of 452143\n",
      "450000 out of 452143\n"
     ]
    }
   ],
   "source": [
    "df = process_reviews(df)\n",
    "# df = process_reviews(df[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    '''\n",
    "    Based on the lower_tagged column of the dataframe (df) that this function receives it creates a vocabulary of \n",
    "    ‘center’ (the x in the PMI equation) and ‘context’ (the y in the PMI equation) words. \n",
    "    The vocabulary of center words will be the 1,000 most frequent NOUNS (words with a PoS tag starting with ‘N’), \n",
    "    and the context words will be the 1,000 most frequent words tagged as either VERB or ADJECTIVE \n",
    "    (words with any PoS tag starting with either ‘J’ or ‘V’).\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: The ‘center’ and ‘context’ vocabularies as lists.\n",
    "    '''\n",
    "    \n",
    "    # Initialize 2 empty lists for the 2 vocabularies to be filled\n",
    "    cent_list, cont_list = [], []\n",
    "\n",
    "    # Iterate through the 'lower_tagged' column of the df provided\n",
    "    for review in df.lower_tagged:\n",
    "        \n",
    "        '''For every word in a review (a record in the 'lower_tagged' column) that the condition is true for either \n",
    "        center or context list is appended to the appropriate list '''\n",
    "        cent_list.extend([word for word in [list_of_words[0] for list_of_words in review if list_of_words[1][0] == 'N']])\n",
    "        cont_list.extend([word for word in [list_of_words[0] for list_of_words in review if (list_of_words[1][0] == 'J') \n",
    "                                            or (list_of_words[1][0] == 'V')]])\n",
    "    \n",
    "    # We create 2 dictionaries that holds information about the frequency of the words in the 2 lists we have\n",
    "    cent_dict = Counter(cent_list)\n",
    "    cont_dict = Counter(cont_list)\n",
    "\n",
    "    # We sort the dictionaries based on their value for frequency of the words and then keep the 1000 most frequent in each list\n",
    "    cent_vocab = [key for key, value in sorted(cent_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "    cont_vocab = [key for key, value in sorted(cont_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "\n",
    "    # Return the lists\n",
    "    return cent_vocab, cont_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F_R5l4IVSk9-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place', 'apartment', 'location', 'stay', 'amsterdam']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cent_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'nice', 'recommend', 'clean', 'good']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samewords = [name for name in cent_vocab if name in cont_vocab]\n",
    "len(samewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place',\n",
       " 'apartment',\n",
       " 'location',\n",
       " 'stay',\n",
       " 'amsterdam',\n",
       " 'host',\n",
       " 'home',\n",
       " 'très',\n",
       " 'center',\n",
       " '’',\n",
       " 'walk',\n",
       " 'centre',\n",
       " 'tram',\n",
       " 'experience',\n",
       " 'à',\n",
       " 'hosts',\n",
       " 'neighborhood',\n",
       " 'clean',\n",
       " 'bien',\n",
       " 'perfect',\n",
       " 'la',\n",
       " 'bed',\n",
       " 'bathroom',\n",
       " 'et',\n",
       " 'e',\n",
       " 'sehr',\n",
       " 'thank',\n",
       " 'breakfast',\n",
       " 'lot',\n",
       " 'ist',\n",
       " 'street',\n",
       " 'boat',\n",
       " 'tips',\n",
       " 'der',\n",
       " 'visit',\n",
       " 'coffee',\n",
       " 'arrival',\n",
       " 'bus',\n",
       " 'need',\n",
       " 'muy',\n",
       " 'min',\n",
       " 'appartement',\n",
       " 'die',\n",
       " 'que',\n",
       " 'transport',\n",
       " 'airbnb',\n",
       " 'view',\n",
       " 'cozy',\n",
       " 'check',\n",
       " 'shops',\n",
       " 'kitchen',\n",
       " 'bars',\n",
       " 'minute',\n",
       " 'helpful',\n",
       " 'get',\n",
       " 'es',\n",
       " 'dans',\n",
       " 'bit',\n",
       " 'le',\n",
       " 'lots',\n",
       " 'je',\n",
       " 'stairs',\n",
       " 'con',\n",
       " 'neighbourhood',\n",
       " 'convenient',\n",
       " 'zu',\n",
       " 'du',\n",
       " 'wir',\n",
       " 'houseboat',\n",
       " 'beautiful',\n",
       " 'pictures',\n",
       " 'des',\n",
       " 'super',\n",
       " 'train',\n",
       " 'park',\n",
       " 'il',\n",
       " 'door',\n",
       " 'couple',\n",
       " 'supermarket',\n",
       " 'séjour',\n",
       " 'bike',\n",
       " 'recommend',\n",
       " 'metro',\n",
       " 'è',\n",
       " 'airport',\n",
       " 'situé',\n",
       " 'help',\n",
       " 'casa',\n",
       " 'friends',\n",
       " 'alles',\n",
       " 'balcony',\n",
       " 'photos',\n",
       " 'garden',\n",
       " 'comfy',\n",
       " 'appartment',\n",
       " 'feel',\n",
       " 'corner',\n",
       " 'nice',\n",
       " 'stop',\n",
       " 'para',\n",
       " 'use',\n",
       " 'pour',\n",
       " 'les',\n",
       " 'cosy',\n",
       " 'avec',\n",
       " 'pas',\n",
       " 'di',\n",
       " 'shower',\n",
       " 'und',\n",
       " 'el',\n",
       " 'amazing',\n",
       " 'cafes',\n",
       " 'friend',\n",
       " 'ride',\n",
       " 'welcoming',\n",
       " \"l'appartement\",\n",
       " 'canal',\n",
       " 'het',\n",
       " 'living',\n",
       " 'wohnung',\n",
       " 'son',\n",
       " 'haben',\n",
       " 'logement',\n",
       " 'wine',\n",
       " 'l',\n",
       " 'ein',\n",
       " 'au',\n",
       " 'vondelpark',\n",
       " 'da',\n",
       " 'luggage',\n",
       " 'centraal',\n",
       " 'light',\n",
       " 'für',\n",
       " '..',\n",
       " 'tidy',\n",
       " 'excellent',\n",
       " 'ce',\n",
       " 'lage',\n",
       " 'windows',\n",
       " 'todo',\n",
       " 'fridge',\n",
       " 'bon',\n",
       " 'recommande',\n",
       " 'eine',\n",
       " 'cute',\n",
       " 'gut',\n",
       " 'van',\n",
       " 'qui',\n",
       " 'tourist',\n",
       " 'fun',\n",
       " 'jordaan',\n",
       " 'ferry',\n",
       " 'sur',\n",
       " 'mais',\n",
       " 'restaurant',\n",
       " 'trams',\n",
       " 'par',\n",
       " 'se',\n",
       " 'si',\n",
       " 'toilet',\n",
       " 'return',\n",
       " 'das',\n",
       " 'était',\n",
       " 'von',\n",
       " 'chambre',\n",
       " 'petit',\n",
       " 'end',\n",
       " 'tea',\n",
       " 'work',\n",
       " 'travel',\n",
       " 'wonderful',\n",
       " 'meet',\n",
       " 'reach',\n",
       " 'merci',\n",
       " 'building',\n",
       " 'parfait',\n",
       " 'te',\n",
       " 'cat',\n",
       " 'eat',\n",
       " 'welcome',\n",
       " 'enjoy',\n",
       " 'see',\n",
       " 'plenty',\n",
       " 'stops',\n",
       " 'hope',\n",
       " 'warm',\n",
       " 'chez',\n",
       " 'dem',\n",
       " 'comfortable',\n",
       " 'foot',\n",
       " 'wifi',\n",
       " 'waren',\n",
       " 'steps',\n",
       " 'auf',\n",
       " 'lo',\n",
       " 'bei',\n",
       " 'al',\n",
       " 'issue',\n",
       " 'noise',\n",
       " 'ideal',\n",
       " 'cool',\n",
       " 'contact',\n",
       " 'sind',\n",
       " 'sauber',\n",
       " 'keys',\n",
       " 'dutch',\n",
       " 'needs',\n",
       " 'por',\n",
       " 'sleep',\n",
       " 'etc',\n",
       " 'été',\n",
       " 'accueil',\n",
       " 'vraiment',\n",
       " 'rent',\n",
       " 'square',\n",
       " 'dam',\n",
       " 'window',\n",
       " 'top',\n",
       " 'side',\n",
       " 'bags',\n",
       " 'im',\n",
       " 'relax',\n",
       " 'find',\n",
       " 'en',\n",
       " 'check-in',\n",
       " 'staying',\n",
       " 'rest',\n",
       " 'в',\n",
       " 'note',\n",
       " 'felt',\n",
       " 'touch',\n",
       " 'é',\n",
       " 'cook',\n",
       " 'shopping',\n",
       " 'pleasant',\n",
       " 'awesome',\n",
       " 'fait',\n",
       " 'kann',\n",
       " 'transports',\n",
       " 'maps',\n",
       " 'future',\n",
       " 'calm',\n",
       " 'quiet',\n",
       " 'responsive',\n",
       " 'front',\n",
       " 'einen',\n",
       " 'voor',\n",
       " 'stylish',\n",
       " 'check-out',\n",
       " 'evening',\n",
       " 'easy',\n",
       " 'offer',\n",
       " 'wie',\n",
       " 'zimmer',\n",
       " 'superb',\n",
       " 'accommodating',\n",
       " 'answer',\n",
       " 'fue',\n",
       " 'bright',\n",
       " 'walking',\n",
       " 'daniel',\n",
       " 'downtown',\n",
       " 'den',\n",
       " 'mind',\n",
       " 'bnb',\n",
       " 'aux',\n",
       " 'right',\n",
       " 'spotless',\n",
       " 'su',\n",
       " 'sit',\n",
       " 'look',\n",
       " 'tolle',\n",
       " 'roof',\n",
       " 'love',\n",
       " 'tour',\n",
       " 'frank',\n",
       " '....',\n",
       " 'sont',\n",
       " 'anna',\n",
       " 'er',\n",
       " 'nicht',\n",
       " 'make',\n",
       " 'anne',\n",
       " 'fine',\n",
       " 'las',\n",
       " 'downstairs',\n",
       " 'einem',\n",
       " 'está',\n",
       " 'op',\n",
       " 'table',\n",
       " 'nos',\n",
       " 'start',\n",
       " 'taxi',\n",
       " 'list',\n",
       " 'non',\n",
       " 'boyfriend',\n",
       " 'cette',\n",
       " 'solo',\n",
       " 'rob',\n",
       " 'cafés',\n",
       " 'gute',\n",
       " 'explore',\n",
       " 'emplacement',\n",
       " 'ruben',\n",
       " '/',\n",
       " '*',\n",
       " 'drink',\n",
       " 'listing',\n",
       " 'minuten',\n",
       " 'guest',\n",
       " 'ok',\n",
       " 'очень',\n",
       " 'david',\n",
       " 'confortable',\n",
       " 'girlfriend',\n",
       " 'unterkunft',\n",
       " 'immer',\n",
       " 'om',\n",
       " 'los',\n",
       " 'map',\n",
       " 'watch',\n",
       " 'un',\n",
       " 'go',\n",
       " 'flat',\n",
       " 'charming',\n",
       " 'sie',\n",
       " 'deux',\n",
       " 'offers',\n",
       " 'climb',\n",
       " 'simple',\n",
       " 'og',\n",
       " 'peaceful',\n",
       " 'let',\n",
       " 'parking',\n",
       " 'hôtes',\n",
       " 'guide',\n",
       " '‘',\n",
       " 'sweet',\n",
       " 'cafe',\n",
       " 'habitación',\n",
       " 'ou',\n",
       " 'liegt',\n",
       " 'rooftop',\n",
       " 'accurate',\n",
       " 'spent',\n",
       " 'info',\n",
       " 'cet',\n",
       " 'hatten',\n",
       " 'relaxing',\n",
       " 'take',\n",
       " 'cooking',\n",
       " 'karin',\n",
       " 'lit',\n",
       " 'brilliant',\n",
       " 'hausboot',\n",
       " 'neat',\n",
       " 'facile',\n",
       " 'good',\n",
       " 'booking',\n",
       " '“',\n",
       " 'c',\n",
       " 'quartier',\n",
       " 'accueillant',\n",
       " 'close',\n",
       " 'aber',\n",
       " 'wish',\n",
       " 'apt',\n",
       " 'sure',\n",
       " 'steep',\n",
       " 'uns',\n",
       " '+',\n",
       " 'upstairs',\n",
       " 'sa',\n",
       " 'n',\n",
       " 'cheese',\n",
       " 'passé',\n",
       " 'gare',\n",
       " 'haus',\n",
       " 'quick',\n",
       " 'bottle',\n",
       " 'robert',\n",
       " 'feeling',\n",
       " 'slept',\n",
       " 'show',\n",
       " 'wait']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samewords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "    '''\n",
    "    This function with the 1,000-word vocabularies of center and context words, creates a co-occurrence matrix \n",
    "    where, for each center word, we keep track of how many of the context words co-occur with it. \n",
    "\n",
    "    Args:\n",
    "        df: The dataframe that holds the reviews and we will base the creation of co-occurence matrix.\n",
    "        cent_vocab: The vocabulary of center words.\n",
    "        cont_vocab: The vocabulary of context words.\n",
    "\n",
    "    Returns: A co-occurrence matrix of center and context words.\n",
    "    '''\n",
    "    \n",
    "    # Initialize an empty list to append all the sentences of all comments\n",
    "    sentences = []    \n",
    "    \n",
    "    start = pd.to_datetime('today')\n",
    "    \n",
    "    # Iterate through all comments, split them to sentences and then append those sentences to the list we created above\n",
    "    for comment in df.comments:\n",
    "        sentences.extend([sentence for sentence in comment.split('.')])\n",
    "        \n",
    "    end = pd.to_datetime('today')\n",
    "    diff = (end-start).total_seconds()\n",
    "    \n",
    "    print(f'yolo in {diff/60} minutes')\n",
    "    \n",
    "    start = pd.to_datetime('today')\n",
    "  \n",
    "    '''Create a dict where we have as keys the 1,000 center_words and as value the sentences they occur.\n",
    "       In order to Filter the sentences we call the Filter() function'''\n",
    "    sentences_per_center_word = {center_word : Filter(sentences, center_word) for center_word in cent_vocab}\n",
    "\n",
    "    end = pd.to_datetime('today')\n",
    "    diff = (end-start).total_seconds()\n",
    "    \n",
    "    print(f'swag in {diff/60} minutes')\n",
    "    \n",
    "    # Initialize an empty dictionary for the co-occurence matrix\n",
    "    coocs = {}\n",
    "\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    diff = 0\n",
    "    \n",
    "    # Iterate through the dictionary that keeps the sentences for each center_word\n",
    "    for center_word, sentences in sentences_per_center_word.items():\n",
    "        # Initialize an empty list for the context words that co-occur with the center_word\n",
    "        words = []\n",
    "        count += 1\n",
    "        start = pd.to_datetime('today')\n",
    "        count2 = 0\n",
    "        \n",
    "        # Iterate through the sentences the center_word occurs\n",
    "        for sentence in sentences:\n",
    "            count2 += 1\n",
    "            \n",
    "            # Create a list with the context words that co-occur with the center_word\n",
    "            words_of_sentence = [word for word in word_tokenize(sentence) if word in cont_vocab and word != center_word]\n",
    "            \n",
    "            '''If the list we created above is not empty (it was succesful in searching for contenxt words) \n",
    "            extend the words list with it.'''\n",
    "            if len(words_of_sentence) > 0: words.extend(words_of_sentence)\n",
    "                \n",
    "        end = pd.to_datetime('today')\n",
    "        diff += (end-start).total_seconds()\n",
    "        \n",
    "        \n",
    "        coocs[center_word] = dict(Counter(words))\n",
    "        \n",
    "        if count % 50 == 0 : print(f'{count} center_word out of {len(sentences_per_center_word)} in {diff/60} minutes')\n",
    "\n",
    "            \n",
    "    print(diff/60)\n",
    "    \n",
    "    return coocs \n",
    "\n",
    "def Filter(sentences, center_word):\n",
    "    '''\n",
    "    This function receives all the sentences in the comments column and \n",
    "    creates a list of the sentences the center_word occurs.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of all the sentences in the comments.\n",
    "        center_word: A center word to find its sentences.\n",
    "\n",
    "    Returns: A list of the sentences that the center_word occurs.\n",
    "    '''\n",
    "    \n",
    "    # Initialize a list to collect the sentences a center_word is in.\n",
    "    sentences_for_center_word = []\n",
    "    \n",
    "    # Iterate through the sentences\n",
    "    for sentence in sentences:\n",
    "        # If the center_word is one of the words in the sentence append it to the sentences_for_center_word list.\n",
    "        if center_word in sentence.split():\n",
    "            sentences_for_center_word.append(sentence)\n",
    "            \n",
    "    return sentences_for_center_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo in 0.01371175 minutes\n",
      "swag in 29.90134768333333 minutes\n",
      "50 center_word out of 1000 in 10.251476966666669 minutes\n",
      "100 center_word out of 1000 in 14.385687400000005 minutes\n",
      "150 center_word out of 1000 in 17.387095833333337 minutes\n",
      "200 center_word out of 1000 in 20.44182581666667 minutes\n",
      "250 center_word out of 1000 in 21.77110443333333 minutes\n",
      "300 center_word out of 1000 in 23.452622233333337 minutes\n",
      "350 center_word out of 1000 in 24.697094650000004 minutes\n",
      "400 center_word out of 1000 in 26.012559566666663 minutes\n",
      "450 center_word out of 1000 in 26.98496018333333 minutes\n",
      "500 center_word out of 1000 in 28.061973733333335 minutes\n",
      "550 center_word out of 1000 in 28.789692616666667 minutes\n",
      "600 center_word out of 1000 in 29.34931948333333 minutes\n",
      "650 center_word out of 1000 in 29.768646766666667 minutes\n",
      "700 center_word out of 1000 in 30.138031300000005 minutes\n",
      "750 center_word out of 1000 in 30.99384561666667 minutes\n",
      "800 center_word out of 1000 in 31.451505449999992 minutes\n",
      "850 center_word out of 1000 in 31.89139901666666 minutes\n",
      "900 center_word out of 1000 in 32.897855950000015 minutes\n",
      "950 center_word out of 1000 in 33.560916550000016 minutes\n",
      "1000 center_word out of 1000 in 33.95062640000001 minutes\n",
      "33.95062640000001\n"
     ]
    }
   ],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "    '''\n",
    "    This function takes as input the dictionary of co-occurence matrix for center and context words. \n",
    "    It converts the dictionary to a 1000x1000 pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        coocs: The dictionary of co-occurence matrix for center and context words.\n",
    "\n",
    "    Returns: A 1000x1000 pandas DataFrame.\n",
    "    '''\n",
    "    \n",
    "    # Initialize a pandas DataFrame with columns the context words and indexes the center words\n",
    "    coocdf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "    # Iterate through the dataframe we created previously to fill it with values\n",
    "    for index, row in coocdf.iterrows():\n",
    "        for word in cont_vocab:\n",
    "            ''' If the pair of index(center word) and word (context word) co-occurs \n",
    "                it will add the value to proper place in the dataframe.\n",
    "                Otherwise the Error of coocs not having a value for this pair will be caught \n",
    "                and the value of 0 will added to the corresponding place.'''\n",
    "            \n",
    "            try:\n",
    "                coocdf[word][index] = coocs[index][word]\n",
    "            except: \n",
    "                coocdf[word][index] = 0\n",
    "\n",
    "    # Return the pandas DataFrame\n",
    "    return coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "id": "cwAflxldSrbg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocdf = cooc_dict2df(coocs)\n",
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "    '''\n",
    "    This function converts the raw co-occurence counts pandas DataFrame \n",
    "    to a DataFrame that keeps the information for the PMI scores.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to convert from raw co-occurence counts to PMI scores.\n",
    "\n",
    "    Returns: A pandas DataFrame with PMI scores for the pairs of center and context words.\n",
    "    '''\n",
    "    \n",
    "    pmidf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "    N = 0\n",
    "    for index, row in df.iterrows():\n",
    "        N += sum(row)\n",
    "    \n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "#         count += 1\n",
    "#         print(row)\n",
    "#         print(sum(row))\n",
    "#         if count == 100: break\n",
    "        for word in cont_vocab:\n",
    "#             print(f'sum(df[word]) - {word} -  {sum(df[word])}')\n",
    "#             print(f'sum(row) - {word} - {sum(row)}')\n",
    "#             pmi = (df[word][index] / N) / ((sum(df[word])/N) * (sum(row)/N))\n",
    "#             if pmi == 0:\n",
    "#                 pmidf[word][index] = 0\n",
    "#             else:\n",
    "#                 pmidf[word][index] = np.log([pmi])[0] \n",
    "            try:\n",
    "                pmi = df[word][index] / (sum(df[word])/N / sum(row)/N)\n",
    "                if pmi == 0:\n",
    "                    pmidf[word][index] = 0\n",
    "                else:\n",
    "                    pmidf[word][index] = np.log([pmi])[0] \n",
    "            except: \n",
    "                print(word)\n",
    "                pmidf[word][index] = 0\n",
    "      \n",
    "    return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGftXjXRSuQw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pmidf = cooc2pmi(coocdf)\n",
    "pmidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>great</th>\n",
       "      <th>nice</th>\n",
       "      <th>recommend</th>\n",
       "      <th>clean</th>\n",
       "      <th>good</th>\n",
       "      <th>stay</th>\n",
       "      <th>comfortable</th>\n",
       "      <th>easy</th>\n",
       "      <th>perfect</th>\n",
       "      <th>quiet</th>\n",
       "      <th>...</th>\n",
       "      <th>climb</th>\n",
       "      <th>chilled</th>\n",
       "      <th>downstairs</th>\n",
       "      <th>well-located</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>based</th>\n",
       "      <th>andrea</th>\n",
       "      <th>avant</th>\n",
       "      <th>taxi</th>\n",
       "      <th>wi-fi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>44.688360</td>\n",
       "      <td>44.561319</td>\n",
       "      <td>45.415802</td>\n",
       "      <td>44.752547</td>\n",
       "      <td>44.404542</td>\n",
       "      <td>45.314411</td>\n",
       "      <td>44.417571</td>\n",
       "      <td>43.974104</td>\n",
       "      <td>44.836389</td>\n",
       "      <td>44.397590</td>\n",
       "      <td>...</td>\n",
       "      <td>43.581010</td>\n",
       "      <td>44.346990</td>\n",
       "      <td>43.678172</td>\n",
       "      <td>44.995769</td>\n",
       "      <td>43.858426</td>\n",
       "      <td>44.256807</td>\n",
       "      <td>43.956814</td>\n",
       "      <td>41.736784</td>\n",
       "      <td>43.298902</td>\n",
       "      <td>43.361207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apartment</th>\n",
       "      <td>44.520872</td>\n",
       "      <td>44.571744</td>\n",
       "      <td>44.683024</td>\n",
       "      <td>45.077289</td>\n",
       "      <td>44.270379</td>\n",
       "      <td>44.521475</td>\n",
       "      <td>44.761877</td>\n",
       "      <td>44.050415</td>\n",
       "      <td>44.635966</td>\n",
       "      <td>44.447049</td>\n",
       "      <td>...</td>\n",
       "      <td>44.549823</td>\n",
       "      <td>44.219910</td>\n",
       "      <td>43.987436</td>\n",
       "      <td>45.316012</td>\n",
       "      <td>43.960780</td>\n",
       "      <td>44.415948</td>\n",
       "      <td>43.263743</td>\n",
       "      <td>0</td>\n",
       "      <td>43.715372</td>\n",
       "      <td>43.980322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>44.353178</td>\n",
       "      <td>43.570040</td>\n",
       "      <td>43.177345</td>\n",
       "      <td>43.599478</td>\n",
       "      <td>44.279607</td>\n",
       "      <td>43.244221</td>\n",
       "      <td>43.322161</td>\n",
       "      <td>43.745508</td>\n",
       "      <td>44.647580</td>\n",
       "      <td>44.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>42.314099</td>\n",
       "      <td>43.054414</td>\n",
       "      <td>42.749225</td>\n",
       "      <td>41.047222</td>\n",
       "      <td>42.391361</td>\n",
       "      <td>43.704465</td>\n",
       "      <td>43.573094</td>\n",
       "      <td>40.579874</td>\n",
       "      <td>42.858670</td>\n",
       "      <td>42.417871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay</th>\n",
       "      <td>44.377028</td>\n",
       "      <td>44.059229</td>\n",
       "      <td>45.178240</td>\n",
       "      <td>43.721475</td>\n",
       "      <td>43.973800</td>\n",
       "      <td>0</td>\n",
       "      <td>44.283084</td>\n",
       "      <td>43.500678</td>\n",
       "      <td>44.428278</td>\n",
       "      <td>43.299373</td>\n",
       "      <td>...</td>\n",
       "      <td>43.144603</td>\n",
       "      <td>44.087918</td>\n",
       "      <td>42.776582</td>\n",
       "      <td>43.617593</td>\n",
       "      <td>43.852802</td>\n",
       "      <td>43.939460</td>\n",
       "      <td>43.114943</td>\n",
       "      <td>0</td>\n",
       "      <td>43.171917</td>\n",
       "      <td>42.971321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amsterdam</th>\n",
       "      <td>37.518569</td>\n",
       "      <td>37.811938</td>\n",
       "      <td>38.038388</td>\n",
       "      <td>37.002249</td>\n",
       "      <td>37.712443</td>\n",
       "      <td>38.148398</td>\n",
       "      <td>36.830614</td>\n",
       "      <td>37.412721</td>\n",
       "      <td>37.889113</td>\n",
       "      <td>37.308277</td>\n",
       "      <td>...</td>\n",
       "      <td>37.593829</td>\n",
       "      <td>37.546744</td>\n",
       "      <td>36.560830</td>\n",
       "      <td>37.043629</td>\n",
       "      <td>36.395338</td>\n",
       "      <td>36.907664</td>\n",
       "      <td>0</td>\n",
       "      <td>36.576281</td>\n",
       "      <td>37.291101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petits</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.389007</td>\n",
       "      <td>31.197182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.170500</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.117727</td>\n",
       "      <td>35.481172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>35.840567</td>\n",
       "      <td>35.946178</td>\n",
       "      <td>35.093309</td>\n",
       "      <td>35.735356</td>\n",
       "      <td>35.793139</td>\n",
       "      <td>35.691793</td>\n",
       "      <td>35.400541</td>\n",
       "      <td>35.414908</td>\n",
       "      <td>35.332635</td>\n",
       "      <td>34.614513</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36.105743</td>\n",
       "      <td>35.812976</td>\n",
       "      <td>0</td>\n",
       "      <td>35.647484</td>\n",
       "      <td>36.852957</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.697398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>découvrir</th>\n",
       "      <td>30.439857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.302433</td>\n",
       "      <td>0</td>\n",
       "      <td>36.655523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>35.589380</td>\n",
       "      <td>35.429716</td>\n",
       "      <td>36.589016</td>\n",
       "      <td>35.469165</td>\n",
       "      <td>35.458749</td>\n",
       "      <td>37.001308</td>\n",
       "      <td>34.971050</td>\n",
       "      <td>35.252607</td>\n",
       "      <td>35.692177</td>\n",
       "      <td>33.953220</td>\n",
       "      <td>...</td>\n",
       "      <td>34.931919</td>\n",
       "      <td>0</td>\n",
       "      <td>35.844831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.729252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visitors</th>\n",
       "      <td>33.637459</td>\n",
       "      <td>33.357632</td>\n",
       "      <td>35.337677</td>\n",
       "      <td>33.190341</td>\n",
       "      <td>34.115113</td>\n",
       "      <td>33.907447</td>\n",
       "      <td>33.783918</td>\n",
       "      <td>33.127117</td>\n",
       "      <td>33.965208</td>\n",
       "      <td>32.994551</td>\n",
       "      <td>...</td>\n",
       "      <td>35.320323</td>\n",
       "      <td>0</td>\n",
       "      <td>33.525185</td>\n",
       "      <td>35.799743</td>\n",
       "      <td>0</td>\n",
       "      <td>34.970631</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               great       nice  recommend      clean       good       stay  \\\n",
       "place      44.688360  44.561319  45.415802  44.752547  44.404542  45.314411   \n",
       "apartment  44.520872  44.571744  44.683024  45.077289  44.270379  44.521475   \n",
       "location   44.353178  43.570040  43.177345  43.599478  44.279607  43.244221   \n",
       "stay       44.377028  44.059229  45.178240  43.721475  43.973800          0   \n",
       "amsterdam  37.518569  37.811938  38.038388  37.002249  37.712443  38.148398   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "petits             0          0  32.389007  31.197182          0          0   \n",
       "show       35.840567  35.946178  35.093309  35.735356  35.793139  35.691793   \n",
       "découvrir  30.439857          0          0          0          0          0   \n",
       "wait       35.589380  35.429716  36.589016  35.469165  35.458749  37.001308   \n",
       "visitors   33.637459  33.357632  35.337677  33.190341  34.115113  33.907447   \n",
       "\n",
       "          comfortable       easy    perfect      quiet  ...      climb  \\\n",
       "place       44.417571  43.974104  44.836389  44.397590  ...  43.581010   \n",
       "apartment   44.761877  44.050415  44.635966  44.447049  ...  44.549823   \n",
       "location    43.322161  43.745508  44.647580  44.000041  ...  42.314099   \n",
       "stay        44.283084  43.500678  44.428278  43.299373  ...  43.144603   \n",
       "amsterdam   36.830614  37.412721  37.889113  37.308277  ...  37.593829   \n",
       "...               ...        ...        ...        ...  ...        ...   \n",
       "petits              0          0  32.170500          0  ...          0   \n",
       "show        35.400541  35.414908  35.332635  34.614513  ...          0   \n",
       "découvrir           0          0          0          0  ...          0   \n",
       "wait        34.971050  35.252607  35.692177  33.953220  ...  34.931919   \n",
       "visitors    33.783918  33.127117  33.965208  32.994551  ...  35.320323   \n",
       "\n",
       "             chilled downstairs well-located accommodate      based  \\\n",
       "place      44.346990  43.678172    44.995769   43.858426  44.256807   \n",
       "apartment  44.219910  43.987436    45.316012   43.960780  44.415948   \n",
       "location   43.054414  42.749225    41.047222   42.391361  43.704465   \n",
       "stay       44.087918  42.776582    43.617593   43.852802  43.939460   \n",
       "amsterdam  37.546744  36.560830    37.043629   36.395338  36.907664   \n",
       "...              ...        ...          ...         ...        ...   \n",
       "petits             0          0            0           0          0   \n",
       "show       36.105743  35.812976            0   35.647484  36.852957   \n",
       "découvrir          0          0            0           0          0   \n",
       "wait               0  35.844831            0           0          0   \n",
       "visitors           0  33.525185    35.799743           0  34.970631   \n",
       "\n",
       "              andrea      avant       taxi      wi-fi  \n",
       "place      43.956814  41.736784  43.298902  43.361207  \n",
       "apartment  43.263743          0  43.715372  43.980322  \n",
       "location   43.573094  40.579874  42.858670  42.417871  \n",
       "stay       43.114943          0  43.171917  42.971321  \n",
       "amsterdam          0  36.576281  37.291101          0  \n",
       "...              ...        ...        ...        ...  \n",
       "petits             0  37.117727  35.481172          0  \n",
       "show               0          0  36.697398          0  \n",
       "découvrir          0  37.302433          0  36.655523  \n",
       "wait               0          0  36.729252          0  \n",
       "visitors           0          0          0          0  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.02694901656134"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pmidf.iloc[2,:]\n",
    "pmidf['place']['room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.68836008719678"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf['great']['place']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.888947057666115"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf['place']['nice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.256807340404556"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf['based']['place']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "        center_word: The dataframe we want to modify.\n",
    "        N: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    dicts_ = {word: df[word][center_word] for word in cont_vocab}\n",
    "    top_words = [key for key, value in sorted(dicts_.items(), key=lambda item: item[1], reverse=True)][:N]\n",
    "\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "1I038zG1Sw62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recommand',\n",
       " 'recomend',\n",
       " 'recommend',\n",
       " 'recommendable',\n",
       " 'stay',\n",
       " 'reccomend',\n",
       " 'maarten',\n",
       " 'looking',\n",
       " 'nicole',\n",
       " 'affordable']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prime',\n",
       " 'superb',\n",
       " 'ideal',\n",
       " 'convenient',\n",
       " 'perfect',\n",
       " 'terrific',\n",
       " 'walkable',\n",
       " 'brilliant',\n",
       " 'fantastic',\n",
       " 'central']"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tea',\n",
       " 'nespresso',\n",
       " 'microwave',\n",
       " 'complimentary',\n",
       " 'fridge',\n",
       " 'shops',\n",
       " 'supplied',\n",
       " 'nick',\n",
       " 'cheese',\n",
       " 'including']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enjoyed',\n",
       " 'enjoyable',\n",
       " 'hesitate',\n",
       " 'future',\n",
       " 'memorable',\n",
       " 'hope',\n",
       " 'letting',\n",
       " 'love',\n",
       " 'longer',\n",
       " 'pleasant']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'stay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aux',\n",
       " 'ses',\n",
       " 'sont',\n",
       " 'ont',\n",
       " 'des',\n",
       " 'apprécié',\n",
       " 'hôtes',\n",
       " 'les',\n",
       " 'cafés',\n",
       " 'avec']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'petits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zimmer',\n",
       " 'sehr',\n",
       " 'wie',\n",
       " 'und',\n",
       " 'alles',\n",
       " 'wohnung',\n",
       " 'ist',\n",
       " 'zentral',\n",
       " 'unterkunft',\n",
       " 'allem']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'sauber')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfcm5-7b0HKO"
   },
   "source": [
    "# 3.b Ethical, social and legal implications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3uf-Qq4tYg"
   },
   "source": [
    "Local authorities in touristic hotspots like Amsterdam, NYC or Barcelona regulate the price of recreational apartments for rent to, among others, ensure that fair rent prices are kept for year-long residents. Consider your price recommender for hosts in Question 2c. Imagine that Airbnb recommends a new host to put the price of your flat at a price which is above the official regulations established by the local government. Upon inspection, you realize that the inflated price you have been recommended comes from many apartments in the area only being offered during an annual event which brings many tourists, and which causes prices to rise. \n",
    "\n",
    "In this context, critically reflect on the compliance of this recommender system with **one of the five actions** outlined in the **UK’s Data Ethics Framework**. You should prioritize the action that, in your opinion, is the weakest. Then, justify your choice by critically analyzing the three **key principles** outlined in the Framework, namely _transparency_, _accountability_ and _fairness_. Finally, you should propose and critically justify a solution that would improve the recommender system in at least one of these principles. You are strongly encouraged to follow a scholarly approach, e.g., with peer-reviewed references as support. \n",
    "\n",
    "Your report should be between 500 and 750 words long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6QJyuP6I1Ht"
   },
   "source": [
    "### Your answer here. No Python, only Markdown.\n",
    "\n",
    "Write your answer after the line.\n",
    "\n",
    "---\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3f75a622fdbe68ac4774c6ea619d86cc770141a8bef94a85fce2870eb7cb09bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}