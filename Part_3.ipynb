{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['youre',\n",
       " 'havent',\n",
       " 'shouldnt',\n",
       " 'wouldnt',\n",
       " 'shant',\n",
       " 'wasnt',\n",
       " 'dont',\n",
       " 'werent',\n",
       " 'arent',\n",
       " 'neednt',\n",
       " 'didnt',\n",
       " 'youve',\n",
       " 'youd',\n",
       " 'wont',\n",
       " 'youll',\n",
       " 'hadnt',\n",
       " 'mightnt',\n",
       " 'hasnt',\n",
       " 'shes',\n",
       " 'couldnt',\n",
       " 'isnt',\n",
       " 'shouldve',\n",
       " 'doesnt',\n",
       " 'thatll',\n",
       " 'its',\n",
       " 'mustnt']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We want to find the alternative forms of stopwords that have the \"'\" symbol in them \n",
    "in order to be able to add also to stopwords the word without this symbol'''\n",
    "\n",
    "pattern = r'\\w+\\'\\w+'\n",
    "\n",
    "new_stopwords = []\n",
    "for word in sw:\n",
    "    # If it finds a word that contains \"'\" it appends the word in new_stopwords list\n",
    "    if len(re.findall(pattern,word)) == 1:\n",
    "        new_stopwords.append(re.findall(pattern,word)[0].replace('\\'',''))\n",
    "new_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arent',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'couldnt',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'didnt',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doesnt',\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'dont',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hadnt',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'havent',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'isnt',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mightnt',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'mustnt',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'neednt',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shant',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'shouldnt',\n",
       " 'shouldve',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thatll',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'wasnt',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'werent',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wont',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'wouldnt',\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'youve'}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After checking those \"new\" words we add them to the stopwords variables named sw\n",
    "for word in new_stopwords:\n",
    "    sw.add(word)\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "basedir = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(basedir,'reviews.csv'))\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "def process_reviews(df):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    # Initialize 3 lists one for each column we will create\n",
    "    tokenized_col = []\n",
    "    tagged_col = []\n",
    "    lower_tagged_col = []\n",
    "\n",
    "\n",
    "    mylen = len(df)\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through the given dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # tokenize the words for the comments of a row\n",
    "        token = word_tokenize(row.comments)\n",
    "        # Append the tokenized words to the proper list\n",
    "        tokenized_col.append(token)\n",
    "        # Tag the tokenized words of the row and then append them to the proper list\n",
    "        tagged_col.append(pos_tag(token))\n",
    "        # lower_tagged.append(list(set(pos_tag([item.lower() for item in token]))))\n",
    "        # Make the tagged words lowercased and then if they are not stopwords append them to the lower_tagged_col list\n",
    "        lower_tagged_col.append(pos_tag([item.lower() for item in token if item.lower() not in sw]))\n",
    "        count += 1\n",
    "\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} out of {mylen}')\n",
    "\n",
    "    # Set as values of the 3 new columns the proper list we created for each one\n",
    "    df['tokenized'] = tokenized_col\n",
    "    df['tagged'] = tagged_col\n",
    "    df['lower_tagged'] = lower_tagged_col\n",
    "\n",
    "    # Return the modified dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "rGYB8gx5Qq-P",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 out of 452143\n",
      "2000 out of 452143\n",
      "3000 out of 452143\n",
      "4000 out of 452143\n",
      "5000 out of 452143\n",
      "6000 out of 452143\n",
      "7000 out of 452143\n",
      "8000 out of 452143\n",
      "9000 out of 452143\n",
      "10000 out of 452143\n",
      "11000 out of 452143\n",
      "12000 out of 452143\n",
      "13000 out of 452143\n",
      "14000 out of 452143\n",
      "15000 out of 452143\n",
      "16000 out of 452143\n",
      "17000 out of 452143\n",
      "18000 out of 452143\n",
      "19000 out of 452143\n",
      "20000 out of 452143\n",
      "21000 out of 452143\n",
      "22000 out of 452143\n",
      "23000 out of 452143\n",
      "24000 out of 452143\n",
      "25000 out of 452143\n",
      "26000 out of 452143\n",
      "27000 out of 452143\n",
      "28000 out of 452143\n",
      "29000 out of 452143\n",
      "30000 out of 452143\n",
      "31000 out of 452143\n",
      "32000 out of 452143\n",
      "33000 out of 452143\n",
      "34000 out of 452143\n",
      "35000 out of 452143\n",
      "36000 out of 452143\n",
      "37000 out of 452143\n",
      "38000 out of 452143\n",
      "39000 out of 452143\n",
      "40000 out of 452143\n",
      "41000 out of 452143\n",
      "42000 out of 452143\n",
      "43000 out of 452143\n",
      "44000 out of 452143\n",
      "45000 out of 452143\n",
      "46000 out of 452143\n",
      "47000 out of 452143\n",
      "48000 out of 452143\n",
      "49000 out of 452143\n",
      "50000 out of 452143\n",
      "51000 out of 452143\n",
      "52000 out of 452143\n",
      "53000 out of 452143\n",
      "54000 out of 452143\n",
      "55000 out of 452143\n",
      "56000 out of 452143\n",
      "57000 out of 452143\n",
      "58000 out of 452143\n",
      "59000 out of 452143\n",
      "60000 out of 452143\n",
      "61000 out of 452143\n",
      "62000 out of 452143\n",
      "63000 out of 452143\n",
      "64000 out of 452143\n",
      "65000 out of 452143\n",
      "66000 out of 452143\n",
      "67000 out of 452143\n",
      "68000 out of 452143\n",
      "69000 out of 452143\n",
      "70000 out of 452143\n",
      "71000 out of 452143\n",
      "72000 out of 452143\n",
      "73000 out of 452143\n",
      "74000 out of 452143\n",
      "75000 out of 452143\n",
      "76000 out of 452143\n",
      "77000 out of 452143\n",
      "78000 out of 452143\n",
      "79000 out of 452143\n",
      "80000 out of 452143\n",
      "81000 out of 452143\n",
      "82000 out of 452143\n",
      "83000 out of 452143\n",
      "84000 out of 452143\n",
      "85000 out of 452143\n",
      "86000 out of 452143\n",
      "87000 out of 452143\n",
      "88000 out of 452143\n",
      "89000 out of 452143\n",
      "90000 out of 452143\n",
      "91000 out of 452143\n",
      "92000 out of 452143\n",
      "93000 out of 452143\n",
      "94000 out of 452143\n",
      "95000 out of 452143\n",
      "96000 out of 452143\n",
      "97000 out of 452143\n",
      "98000 out of 452143\n",
      "99000 out of 452143\n",
      "100000 out of 452143\n",
      "101000 out of 452143\n",
      "102000 out of 452143\n",
      "103000 out of 452143\n",
      "104000 out of 452143\n",
      "105000 out of 452143\n",
      "106000 out of 452143\n",
      "107000 out of 452143\n",
      "108000 out of 452143\n",
      "109000 out of 452143\n",
      "110000 out of 452143\n",
      "111000 out of 452143\n",
      "112000 out of 452143\n",
      "113000 out of 452143\n",
      "114000 out of 452143\n",
      "115000 out of 452143\n",
      "116000 out of 452143\n",
      "117000 out of 452143\n",
      "118000 out of 452143\n",
      "119000 out of 452143\n",
      "120000 out of 452143\n",
      "121000 out of 452143\n",
      "122000 out of 452143\n",
      "123000 out of 452143\n",
      "124000 out of 452143\n",
      "125000 out of 452143\n",
      "126000 out of 452143\n",
      "127000 out of 452143\n",
      "128000 out of 452143\n",
      "129000 out of 452143\n",
      "130000 out of 452143\n",
      "131000 out of 452143\n",
      "132000 out of 452143\n",
      "133000 out of 452143\n",
      "134000 out of 452143\n",
      "135000 out of 452143\n",
      "136000 out of 452143\n",
      "137000 out of 452143\n",
      "138000 out of 452143\n",
      "139000 out of 452143\n",
      "140000 out of 452143\n",
      "141000 out of 452143\n",
      "142000 out of 452143\n",
      "143000 out of 452143\n",
      "144000 out of 452143\n",
      "145000 out of 452143\n",
      "146000 out of 452143\n",
      "147000 out of 452143\n",
      "148000 out of 452143\n",
      "149000 out of 452143\n",
      "150000 out of 452143\n",
      "151000 out of 452143\n",
      "152000 out of 452143\n",
      "153000 out of 452143\n",
      "154000 out of 452143\n",
      "155000 out of 452143\n",
      "156000 out of 452143\n",
      "157000 out of 452143\n",
      "158000 out of 452143\n",
      "159000 out of 452143\n",
      "160000 out of 452143\n",
      "161000 out of 452143\n",
      "162000 out of 452143\n",
      "163000 out of 452143\n",
      "164000 out of 452143\n",
      "165000 out of 452143\n",
      "166000 out of 452143\n",
      "167000 out of 452143\n",
      "168000 out of 452143\n",
      "169000 out of 452143\n",
      "170000 out of 452143\n",
      "171000 out of 452143\n",
      "172000 out of 452143\n",
      "173000 out of 452143\n",
      "174000 out of 452143\n",
      "175000 out of 452143\n",
      "176000 out of 452143\n",
      "177000 out of 452143\n",
      "178000 out of 452143\n",
      "179000 out of 452143\n",
      "180000 out of 452143\n",
      "181000 out of 452143\n",
      "182000 out of 452143\n",
      "183000 out of 452143\n",
      "184000 out of 452143\n",
      "185000 out of 452143\n",
      "186000 out of 452143\n",
      "187000 out of 452143\n",
      "188000 out of 452143\n",
      "189000 out of 452143\n",
      "190000 out of 452143\n",
      "191000 out of 452143\n",
      "192000 out of 452143\n",
      "193000 out of 452143\n",
      "194000 out of 452143\n",
      "195000 out of 452143\n",
      "196000 out of 452143\n",
      "197000 out of 452143\n",
      "198000 out of 452143\n",
      "199000 out of 452143\n",
      "200000 out of 452143\n",
      "201000 out of 452143\n",
      "202000 out of 452143\n",
      "203000 out of 452143\n",
      "204000 out of 452143\n",
      "205000 out of 452143\n",
      "206000 out of 452143\n",
      "207000 out of 452143\n",
      "208000 out of 452143\n",
      "209000 out of 452143\n",
      "210000 out of 452143\n",
      "211000 out of 452143\n",
      "212000 out of 452143\n",
      "213000 out of 452143\n",
      "214000 out of 452143\n",
      "215000 out of 452143\n",
      "216000 out of 452143\n",
      "217000 out of 452143\n",
      "218000 out of 452143\n",
      "219000 out of 452143\n",
      "220000 out of 452143\n",
      "221000 out of 452143\n",
      "222000 out of 452143\n",
      "223000 out of 452143\n",
      "224000 out of 452143\n",
      "225000 out of 452143\n",
      "226000 out of 452143\n",
      "227000 out of 452143\n",
      "228000 out of 452143\n",
      "229000 out of 452143\n",
      "230000 out of 452143\n",
      "231000 out of 452143\n",
      "232000 out of 452143\n",
      "233000 out of 452143\n",
      "234000 out of 452143\n",
      "235000 out of 452143\n",
      "236000 out of 452143\n",
      "237000 out of 452143\n",
      "238000 out of 452143\n",
      "239000 out of 452143\n",
      "240000 out of 452143\n",
      "241000 out of 452143\n",
      "242000 out of 452143\n",
      "243000 out of 452143\n",
      "244000 out of 452143\n",
      "245000 out of 452143\n",
      "246000 out of 452143\n",
      "247000 out of 452143\n",
      "248000 out of 452143\n",
      "249000 out of 452143\n",
      "250000 out of 452143\n",
      "251000 out of 452143\n",
      "252000 out of 452143\n",
      "253000 out of 452143\n",
      "254000 out of 452143\n",
      "255000 out of 452143\n",
      "256000 out of 452143\n",
      "257000 out of 452143\n",
      "258000 out of 452143\n",
      "259000 out of 452143\n",
      "260000 out of 452143\n",
      "261000 out of 452143\n",
      "262000 out of 452143\n",
      "263000 out of 452143\n",
      "264000 out of 452143\n",
      "265000 out of 452143\n",
      "266000 out of 452143\n",
      "267000 out of 452143\n",
      "268000 out of 452143\n",
      "269000 out of 452143\n",
      "270000 out of 452143\n",
      "271000 out of 452143\n",
      "272000 out of 452143\n",
      "273000 out of 452143\n",
      "274000 out of 452143\n",
      "275000 out of 452143\n",
      "276000 out of 452143\n",
      "277000 out of 452143\n",
      "278000 out of 452143\n",
      "279000 out of 452143\n",
      "280000 out of 452143\n",
      "281000 out of 452143\n",
      "282000 out of 452143\n",
      "283000 out of 452143\n",
      "284000 out of 452143\n",
      "285000 out of 452143\n",
      "286000 out of 452143\n",
      "287000 out of 452143\n",
      "288000 out of 452143\n",
      "289000 out of 452143\n",
      "290000 out of 452143\n",
      "291000 out of 452143\n",
      "292000 out of 452143\n",
      "293000 out of 452143\n",
      "294000 out of 452143\n",
      "295000 out of 452143\n",
      "296000 out of 452143\n",
      "297000 out of 452143\n",
      "298000 out of 452143\n",
      "299000 out of 452143\n",
      "300000 out of 452143\n",
      "301000 out of 452143\n",
      "302000 out of 452143\n",
      "303000 out of 452143\n",
      "304000 out of 452143\n",
      "305000 out of 452143\n",
      "306000 out of 452143\n",
      "307000 out of 452143\n",
      "308000 out of 452143\n",
      "309000 out of 452143\n",
      "310000 out of 452143\n",
      "311000 out of 452143\n",
      "312000 out of 452143\n",
      "313000 out of 452143\n",
      "314000 out of 452143\n",
      "315000 out of 452143\n",
      "316000 out of 452143\n",
      "317000 out of 452143\n",
      "318000 out of 452143\n",
      "319000 out of 452143\n",
      "320000 out of 452143\n",
      "321000 out of 452143\n",
      "322000 out of 452143\n",
      "323000 out of 452143\n",
      "324000 out of 452143\n",
      "325000 out of 452143\n",
      "326000 out of 452143\n",
      "327000 out of 452143\n",
      "328000 out of 452143\n",
      "329000 out of 452143\n",
      "330000 out of 452143\n",
      "331000 out of 452143\n",
      "332000 out of 452143\n",
      "333000 out of 452143\n",
      "334000 out of 452143\n",
      "335000 out of 452143\n",
      "336000 out of 452143\n",
      "337000 out of 452143\n",
      "338000 out of 452143\n",
      "339000 out of 452143\n",
      "340000 out of 452143\n",
      "341000 out of 452143\n",
      "342000 out of 452143\n",
      "343000 out of 452143\n",
      "344000 out of 452143\n",
      "345000 out of 452143\n",
      "346000 out of 452143\n",
      "347000 out of 452143\n",
      "348000 out of 452143\n",
      "349000 out of 452143\n",
      "350000 out of 452143\n",
      "351000 out of 452143\n",
      "352000 out of 452143\n",
      "353000 out of 452143\n",
      "354000 out of 452143\n",
      "355000 out of 452143\n",
      "356000 out of 452143\n",
      "357000 out of 452143\n",
      "358000 out of 452143\n",
      "359000 out of 452143\n",
      "360000 out of 452143\n",
      "361000 out of 452143\n",
      "362000 out of 452143\n",
      "363000 out of 452143\n",
      "364000 out of 452143\n",
      "365000 out of 452143\n",
      "366000 out of 452143\n",
      "367000 out of 452143\n",
      "368000 out of 452143\n",
      "369000 out of 452143\n",
      "370000 out of 452143\n",
      "371000 out of 452143\n",
      "372000 out of 452143\n",
      "373000 out of 452143\n",
      "374000 out of 452143\n",
      "375000 out of 452143\n",
      "376000 out of 452143\n",
      "377000 out of 452143\n",
      "378000 out of 452143\n",
      "379000 out of 452143\n",
      "380000 out of 452143\n",
      "381000 out of 452143\n",
      "382000 out of 452143\n",
      "383000 out of 452143\n",
      "384000 out of 452143\n",
      "385000 out of 452143\n",
      "386000 out of 452143\n",
      "387000 out of 452143\n",
      "388000 out of 452143\n",
      "389000 out of 452143\n",
      "390000 out of 452143\n",
      "391000 out of 452143\n",
      "392000 out of 452143\n",
      "393000 out of 452143\n",
      "394000 out of 452143\n",
      "395000 out of 452143\n",
      "396000 out of 452143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397000 out of 452143\n",
      "398000 out of 452143\n",
      "399000 out of 452143\n",
      "400000 out of 452143\n",
      "401000 out of 452143\n",
      "402000 out of 452143\n",
      "403000 out of 452143\n",
      "404000 out of 452143\n",
      "405000 out of 452143\n",
      "406000 out of 452143\n",
      "407000 out of 452143\n",
      "408000 out of 452143\n",
      "409000 out of 452143\n",
      "410000 out of 452143\n",
      "411000 out of 452143\n",
      "412000 out of 452143\n",
      "413000 out of 452143\n",
      "414000 out of 452143\n",
      "415000 out of 452143\n",
      "416000 out of 452143\n",
      "417000 out of 452143\n",
      "418000 out of 452143\n",
      "419000 out of 452143\n",
      "420000 out of 452143\n",
      "421000 out of 452143\n",
      "422000 out of 452143\n",
      "423000 out of 452143\n",
      "424000 out of 452143\n",
      "425000 out of 452143\n",
      "426000 out of 452143\n",
      "427000 out of 452143\n",
      "428000 out of 452143\n",
      "429000 out of 452143\n",
      "430000 out of 452143\n",
      "431000 out of 452143\n",
      "432000 out of 452143\n",
      "433000 out of 452143\n",
      "434000 out of 452143\n",
      "435000 out of 452143\n",
      "436000 out of 452143\n",
      "437000 out of 452143\n",
      "438000 out of 452143\n",
      "439000 out of 452143\n",
      "440000 out of 452143\n",
      "441000 out of 452143\n",
      "442000 out of 452143\n",
      "443000 out of 452143\n",
      "444000 out of 452143\n",
      "445000 out of 452143\n",
      "446000 out of 452143\n",
      "447000 out of 452143\n",
      "448000 out of 452143\n",
      "449000 out of 452143\n",
      "450000 out of 452143\n",
      "451000 out of 452143\n",
      "452000 out of 452143\n"
     ]
    }
   ],
   "source": [
    "df = process_reviews(df)\n",
    "# df = process_reviews(df[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    cent_list, cont_list = [], []\n",
    "\n",
    "    for review in df.lower_tagged:\n",
    "        cent_list.extend([word for word in [list_of_words[0] for list_of_words in review if list_of_words[1][0] == 'N']])\n",
    "        cont_list.extend([word for word in [list_of_words[0] for list_of_words in review if (list_of_words[1][0] == 'J') or (list_of_words[1][0] == 'V')]])\n",
    "\n",
    "    cent_dict = Counter(cent_list)\n",
    "    cont_dict = Counter(cont_list)\n",
    "\n",
    "    cent_vocab = [key for key, value in sorted(cent_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "    cont_vocab = [key for key, value in sorted(cont_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "\n",
    "    return cent_vocab, cont_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "F_R5l4IVSk9-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place', 'apartment', 'location', 'stay', 'amsterdam']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cent_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'nice', 'recommend', 'clean', 'good']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samewords = [name for name in cent_vocab if name in cont_vocab]\n",
    "len(samewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place',\n",
       " 'apartment',\n",
       " 'location',\n",
       " 'stay',\n",
       " 'amsterdam',\n",
       " 'host',\n",
       " 'home',\n",
       " 'très',\n",
       " 'center',\n",
       " '’',\n",
       " 'walk',\n",
       " 'centre',\n",
       " 'tram',\n",
       " 'experience',\n",
       " 'à',\n",
       " 'hosts',\n",
       " 'neighborhood',\n",
       " 'clean',\n",
       " 'bien',\n",
       " 'perfect',\n",
       " 'la',\n",
       " 'bed',\n",
       " 'bathroom',\n",
       " 'et',\n",
       " 'e',\n",
       " 'sehr',\n",
       " 'thank',\n",
       " 'breakfast',\n",
       " 'lot',\n",
       " 'ist',\n",
       " 'street',\n",
       " 'boat',\n",
       " 'tips',\n",
       " 'der',\n",
       " 'visit',\n",
       " 'coffee',\n",
       " 'arrival',\n",
       " 'bus',\n",
       " 'need',\n",
       " 'muy',\n",
       " 'min',\n",
       " 'appartement',\n",
       " 'die',\n",
       " 'que',\n",
       " 'transport',\n",
       " 'airbnb',\n",
       " 'view',\n",
       " 'cozy',\n",
       " 'check',\n",
       " 'shops',\n",
       " 'kitchen',\n",
       " 'bars',\n",
       " 'minute',\n",
       " 'helpful',\n",
       " 'get',\n",
       " 'es',\n",
       " 'dans',\n",
       " 'bit',\n",
       " 'le',\n",
       " 'lots',\n",
       " 'je',\n",
       " 'stairs',\n",
       " 'con',\n",
       " 'neighbourhood',\n",
       " 'convenient',\n",
       " 'zu',\n",
       " 'du',\n",
       " 'wir',\n",
       " 'houseboat',\n",
       " 'beautiful',\n",
       " 'pictures',\n",
       " 'des',\n",
       " 'super',\n",
       " 'train',\n",
       " 'park',\n",
       " 'il',\n",
       " 'door',\n",
       " 'couple',\n",
       " 'supermarket',\n",
       " 'séjour',\n",
       " 'bike',\n",
       " 'recommend',\n",
       " 'metro',\n",
       " 'è',\n",
       " 'airport',\n",
       " 'situé',\n",
       " 'help',\n",
       " 'casa',\n",
       " 'friends',\n",
       " 'alles',\n",
       " 'balcony',\n",
       " 'photos',\n",
       " 'garden',\n",
       " 'comfy',\n",
       " 'appartment',\n",
       " 'feel',\n",
       " 'corner',\n",
       " 'nice',\n",
       " 'stop',\n",
       " 'para',\n",
       " 'use',\n",
       " 'pour',\n",
       " 'les',\n",
       " 'cosy',\n",
       " 'avec',\n",
       " 'pas',\n",
       " 'di',\n",
       " 'shower',\n",
       " 'und',\n",
       " 'el',\n",
       " 'amazing',\n",
       " 'cafes',\n",
       " 'friend',\n",
       " 'ride',\n",
       " 'welcoming',\n",
       " \"l'appartement\",\n",
       " 'canal',\n",
       " 'het',\n",
       " 'living',\n",
       " 'wohnung',\n",
       " 'son',\n",
       " 'haben',\n",
       " 'logement',\n",
       " 'wine',\n",
       " 'l',\n",
       " 'ein',\n",
       " 'au',\n",
       " 'vondelpark',\n",
       " 'da',\n",
       " 'luggage',\n",
       " 'centraal',\n",
       " 'light',\n",
       " 'für',\n",
       " '..',\n",
       " 'tidy',\n",
       " 'excellent',\n",
       " 'ce',\n",
       " 'lage',\n",
       " 'windows',\n",
       " 'todo',\n",
       " 'fridge',\n",
       " 'bon',\n",
       " 'recommande',\n",
       " 'eine',\n",
       " 'cute',\n",
       " 'gut',\n",
       " 'van',\n",
       " 'qui',\n",
       " 'tourist',\n",
       " 'fun',\n",
       " 'jordaan',\n",
       " 'ferry',\n",
       " 'sur',\n",
       " 'mais',\n",
       " 'restaurant',\n",
       " 'trams',\n",
       " 'par',\n",
       " 'se',\n",
       " 'si',\n",
       " 'toilet',\n",
       " 'return',\n",
       " 'das',\n",
       " 'était',\n",
       " 'von',\n",
       " 'chambre',\n",
       " 'petit',\n",
       " 'end',\n",
       " 'tea',\n",
       " 'work',\n",
       " 'travel',\n",
       " 'wonderful',\n",
       " 'meet',\n",
       " 'reach',\n",
       " 'merci',\n",
       " 'building',\n",
       " 'parfait',\n",
       " 'te',\n",
       " 'cat',\n",
       " 'eat',\n",
       " 'welcome',\n",
       " 'enjoy',\n",
       " 'see',\n",
       " 'plenty',\n",
       " 'stops',\n",
       " 'hope',\n",
       " 'warm',\n",
       " 'chez',\n",
       " 'dem',\n",
       " 'comfortable',\n",
       " 'foot',\n",
       " 'wifi',\n",
       " 'waren',\n",
       " 'steps',\n",
       " 'auf',\n",
       " 'lo',\n",
       " 'bei',\n",
       " 'al',\n",
       " 'issue',\n",
       " 'noise',\n",
       " 'ideal',\n",
       " 'cool',\n",
       " 'contact',\n",
       " 'sind',\n",
       " 'sauber',\n",
       " 'keys',\n",
       " 'dutch',\n",
       " 'needs',\n",
       " 'por',\n",
       " 'sleep',\n",
       " 'etc',\n",
       " 'été',\n",
       " 'accueil',\n",
       " 'vraiment',\n",
       " 'rent',\n",
       " 'square',\n",
       " 'dam',\n",
       " 'window',\n",
       " 'top',\n",
       " 'side',\n",
       " 'bags',\n",
       " 'im',\n",
       " 'relax',\n",
       " 'find',\n",
       " 'en',\n",
       " 'check-in',\n",
       " 'staying',\n",
       " 'rest',\n",
       " 'в',\n",
       " 'note',\n",
       " 'felt',\n",
       " 'touch',\n",
       " 'é',\n",
       " 'cook',\n",
       " 'shopping',\n",
       " 'pleasant',\n",
       " 'awesome',\n",
       " 'fait',\n",
       " 'kann',\n",
       " 'transports',\n",
       " 'maps',\n",
       " 'future',\n",
       " 'calm',\n",
       " 'quiet',\n",
       " 'responsive',\n",
       " 'front',\n",
       " 'einen',\n",
       " 'voor',\n",
       " 'stylish',\n",
       " 'check-out',\n",
       " 'evening',\n",
       " 'easy',\n",
       " 'offer',\n",
       " 'wie',\n",
       " 'zimmer',\n",
       " 'superb',\n",
       " 'accommodating',\n",
       " 'answer',\n",
       " 'fue',\n",
       " 'bright',\n",
       " 'walking',\n",
       " 'daniel',\n",
       " 'downtown',\n",
       " 'den',\n",
       " 'mind',\n",
       " 'bnb',\n",
       " 'aux',\n",
       " 'right',\n",
       " 'spotless',\n",
       " 'su',\n",
       " 'sit',\n",
       " 'look',\n",
       " 'tolle',\n",
       " 'roof',\n",
       " 'love',\n",
       " 'tour',\n",
       " 'frank',\n",
       " '....',\n",
       " 'sont',\n",
       " 'anna',\n",
       " 'er',\n",
       " 'nicht',\n",
       " 'make',\n",
       " 'anne',\n",
       " 'fine',\n",
       " 'las',\n",
       " 'downstairs',\n",
       " 'einem',\n",
       " 'está',\n",
       " 'op',\n",
       " 'table',\n",
       " 'nos',\n",
       " 'start',\n",
       " 'taxi',\n",
       " 'list',\n",
       " 'non',\n",
       " 'boyfriend',\n",
       " 'cette',\n",
       " 'solo',\n",
       " 'rob',\n",
       " 'cafés',\n",
       " 'gute',\n",
       " 'explore',\n",
       " 'emplacement',\n",
       " 'ruben',\n",
       " '/',\n",
       " '*',\n",
       " 'drink',\n",
       " 'listing',\n",
       " 'minuten',\n",
       " 'guest',\n",
       " 'ok',\n",
       " 'очень',\n",
       " 'david',\n",
       " 'confortable',\n",
       " 'girlfriend',\n",
       " 'unterkunft',\n",
       " 'immer',\n",
       " 'om',\n",
       " 'los',\n",
       " 'map',\n",
       " 'watch',\n",
       " 'un',\n",
       " 'go',\n",
       " 'flat',\n",
       " 'charming',\n",
       " 'sie',\n",
       " 'deux',\n",
       " 'offers',\n",
       " 'climb',\n",
       " 'simple',\n",
       " 'og',\n",
       " 'peaceful',\n",
       " 'let',\n",
       " 'parking',\n",
       " 'hôtes',\n",
       " 'guide',\n",
       " '‘',\n",
       " 'sweet',\n",
       " 'cafe',\n",
       " 'habitación',\n",
       " 'ou',\n",
       " 'liegt',\n",
       " 'rooftop',\n",
       " 'accurate',\n",
       " 'spent',\n",
       " 'info',\n",
       " 'cet',\n",
       " 'hatten',\n",
       " 'relaxing',\n",
       " 'take',\n",
       " 'cooking',\n",
       " 'karin',\n",
       " 'lit',\n",
       " 'brilliant',\n",
       " 'hausboot',\n",
       " 'neat',\n",
       " 'facile',\n",
       " 'good',\n",
       " 'booking',\n",
       " '“',\n",
       " 'c',\n",
       " 'quartier',\n",
       " 'accueillant',\n",
       " 'close',\n",
       " 'aber',\n",
       " 'wish',\n",
       " 'apt',\n",
       " 'sure',\n",
       " 'steep',\n",
       " 'uns',\n",
       " '+',\n",
       " 'upstairs',\n",
       " 'sa',\n",
       " 'n',\n",
       " 'cheese',\n",
       " 'passé',\n",
       " 'gare',\n",
       " 'haus',\n",
       " 'quick',\n",
       " 'bottle',\n",
       " 'robert',\n",
       " 'feeling',\n",
       " 'slept',\n",
       " 'show',\n",
       " 'wait']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samewords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "ddnfCbQWRd5R"
   },
   "outputs": [],
   "source": [
    "# def get_coocs(df, cent_vocab, cont_vocab):\n",
    "#   sentences = []\n",
    "#   comments = df.comments\n",
    "\n",
    "#   for comment in comments:\n",
    "#     sentences.extend([sentence for sentence in comment.split('.')])\n",
    "  \n",
    "#   print('yolo')\n",
    "#   # print(sentences)\n",
    "  \n",
    "#   coocs = {}\n",
    "  \n",
    "#   count = 0\n",
    "#   for center_word in cent_vocab:\n",
    "#     count += 1\n",
    "#     words = []\n",
    "#     for sentence in sentences:\n",
    "#       if center_word in sentence:\n",
    "#         words_in_sentence = word_tokenize(sentence)\n",
    "#         words.extend([word for word in words_in_sentence if word in cont_vocab])\n",
    "    \n",
    "#     center_word_dict = dict(Counter(words))\n",
    "#     coocs[center_word] = center_word_dict\n",
    "#     print(f'{count} out of 1000')\n",
    "    \n",
    "#   # cent_dict = Counter(cent_list)\n",
    "#   # cont_dict = Counter(cont_list)\n",
    "\n",
    "  \n",
    "#   return coocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "        cent_vocab: The dataframe we want to modify.\n",
    "        cont_vocab: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    sentences = []\n",
    "    comments = df.comments\n",
    "\n",
    "    start = pd.to_datetime('today')\n",
    "    \n",
    "    for comment in comments:\n",
    "        sentences.extend([sentence for sentence in comment.split('.')])\n",
    "        \n",
    "    end = pd.to_datetime('today')\n",
    "    diff = (end-start).total_seconds()\n",
    "    \n",
    "    print(f'yolo in {diff/60} minutes')\n",
    "    \n",
    "    start = pd.to_datetime('today')\n",
    "  \n",
    "    sentences_per_center_word = {center_word : Filter(sentences, center_word) for center_word in cent_vocab}\n",
    "\n",
    "    end = pd.to_datetime('today')\n",
    "    diff = (end-start).total_seconds()\n",
    "    \n",
    "    print(f'swag in {diff/60} minutes')\n",
    "\n",
    "    \n",
    "    coocs = {}\n",
    "\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    diff = 0\n",
    "    for center_word, sentences in sentences_per_center_word.items():\n",
    "        words = []\n",
    "        count += 1\n",
    "        start = pd.to_datetime('today')\n",
    "        count2 = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            count2 += 1\n",
    "#             words_of_sentence = [word for word in word_tokenize(sentence) if word in cont_vocab]\n",
    "            words_of_sentence = [word for word in word_tokenize(sentence) if word in cont_vocab and word != center_word]\n",
    "            if len(words_of_sentence) > 0: words.extend(words_of_sentence)\n",
    "                \n",
    "        end = pd.to_datetime('today')\n",
    "        diff += (end-start).total_seconds()\n",
    "#         print(f'{center_word} - - - - {(end-start).total_seconds()} seconds')\n",
    "        coocs[center_word] = dict(Counter(words))\n",
    "        if count % 50 == 0 : print(f'{count} center_word out of {len(sentences_per_center_word)} in {diff/60} minutes')\n",
    "\n",
    "            \n",
    "    print(diff/60)\n",
    "    return coocs \n",
    "\n",
    "def Filter(sentences, center_word):\n",
    "    sentences_for_center_word = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if center_word in sentence.split():\n",
    "            sentences_for_center_word.append(sentence)\n",
    "#     print(f'{center_word} --- {len(sentences_for_center_word)}')\n",
    "    return sentences_for_center_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo in 0.01371175 minutes\n",
      "swag in 29.90134768333333 minutes\n",
      "50 center_word out of 1000 in 10.251476966666669 minutes\n",
      "100 center_word out of 1000 in 14.385687400000005 minutes\n",
      "150 center_word out of 1000 in 17.387095833333337 minutes\n",
      "200 center_word out of 1000 in 20.44182581666667 minutes\n",
      "250 center_word out of 1000 in 21.77110443333333 minutes\n",
      "300 center_word out of 1000 in 23.452622233333337 minutes\n",
      "350 center_word out of 1000 in 24.697094650000004 minutes\n",
      "400 center_word out of 1000 in 26.012559566666663 minutes\n",
      "450 center_word out of 1000 in 26.98496018333333 minutes\n",
      "500 center_word out of 1000 in 28.061973733333335 minutes\n",
      "550 center_word out of 1000 in 28.789692616666667 minutes\n",
      "600 center_word out of 1000 in 29.34931948333333 minutes\n",
      "650 center_word out of 1000 in 29.768646766666667 minutes\n",
      "700 center_word out of 1000 in 30.138031300000005 minutes\n",
      "750 center_word out of 1000 in 30.99384561666667 minutes\n",
      "800 center_word out of 1000 in 31.451505449999992 minutes\n",
      "850 center_word out of 1000 in 31.89139901666666 minutes\n",
      "900 center_word out of 1000 in 32.897855950000015 minutes\n",
      "950 center_word out of 1000 in 33.560916550000016 minutes\n",
      "1000 center_word out of 1000 in 33.95062640000001 minutes\n",
      "33.95062640000001\n"
     ]
    }
   ],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "        cent_vocab: The dataframe we want to modify.\n",
    "        cont_vocab: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    sentences = []\n",
    "    comments = df.comments\n",
    "\n",
    "    start = pd.to_datetime('today')\n",
    "    \n",
    "    for comment in comments:\n",
    "        sentences.extend([sentence for sentence in comment.split('.')])\n",
    "        \n",
    "    end = pd.to_datetime('today')\n",
    "    diff = (end-start).total_seconds()\n",
    "    \n",
    "    print(f'yolo in {diff/60} minutes')\n",
    "    \n",
    "    start = pd.to_datetime('today')\n",
    "  \n",
    "    sentences_per_center_word = {center_word : Filter(sentences, center_word) for center_word in cent_vocab}\n",
    "\n",
    "    end = pd.to_datetime('today')\n",
    "    diff = (end-start).total_seconds()\n",
    "    \n",
    "    print(f'swag in {diff/60} minutes')\n",
    "\n",
    "    \n",
    "    coocs = {}\n",
    "\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    diff = 0\n",
    "    for center_word, sentences in sentences_per_center_word.items():\n",
    "        words = []\n",
    "        count += 1\n",
    "        start = pd.to_datetime('today')\n",
    "        count2 = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            count2 += 1\n",
    "#             words_of_sentence = [word for word in word_tokenize(sentence) if word in cont_vocab]\n",
    "            words_of_sentence = [word for word in word_tokenize(sentence) if word in cont_vocab and word != center_word]\n",
    "            if len(words_of_sentence) > 0: words.extend(words_of_sentence)\n",
    "                \n",
    "        end = pd.to_datetime('today')\n",
    "        diff += (end-start).total_seconds()\n",
    "#         print(f'{center_word} - - - - {(end-start).total_seconds()} seconds')\n",
    "        coocs[center_word] = dict(Counter(words))\n",
    "        if count % 50 == 0 : print(f'{count} center_word out of {len(sentences_per_center_word)} in {diff/60} minutes')\n",
    "\n",
    "            \n",
    "    print(diff/60)\n",
    "    return coocs \n",
    "\n",
    "def Filter(sentences, center_word):\n",
    "    sentences_for_center_word = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if center_word in sentence.split():\n",
    "            sentences_for_center_word.append(sentence)\n",
    "#     print(f'{center_word} --- {len(sentences_for_center_word)}')\n",
    "    return sentences_for_center_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        coocs: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    coocdf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "    for index, row in coocdf.iterrows():\n",
    "        for word in cont_vocab:\n",
    "            try:\n",
    "                coocdf[word][index] = coocs[index][word]\n",
    "            except: \n",
    "                coocdf[word][index] = 0\n",
    "\n",
    "    return coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "id": "cwAflxldSrbg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocdf = cooc_dict2df(coocs)\n",
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>great</th>\n",
       "      <th>nice</th>\n",
       "      <th>recommend</th>\n",
       "      <th>clean</th>\n",
       "      <th>good</th>\n",
       "      <th>stay</th>\n",
       "      <th>comfortable</th>\n",
       "      <th>easy</th>\n",
       "      <th>perfect</th>\n",
       "      <th>quiet</th>\n",
       "      <th>...</th>\n",
       "      <th>climb</th>\n",
       "      <th>chilled</th>\n",
       "      <th>downstairs</th>\n",
       "      <th>well-located</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>based</th>\n",
       "      <th>andrea</th>\n",
       "      <th>avant</th>\n",
       "      <th>taxi</th>\n",
       "      <th>wi-fi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>26420</td>\n",
       "      <td>14996</td>\n",
       "      <td>16646</td>\n",
       "      <td>14120</td>\n",
       "      <td>7276</td>\n",
       "      <td>36269</td>\n",
       "      <td>6695</td>\n",
       "      <td>4696</td>\n",
       "      <td>11603</td>\n",
       "      <td>6262</td>\n",
       "      <td>...</td>\n",
       "      <td>104</td>\n",
       "      <td>67</td>\n",
       "      <td>138</td>\n",
       "      <td>106</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>91</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apartment</th>\n",
       "      <td>22344</td>\n",
       "      <td>15152</td>\n",
       "      <td>7999</td>\n",
       "      <td>19536</td>\n",
       "      <td>6362</td>\n",
       "      <td>16411</td>\n",
       "      <td>9446</td>\n",
       "      <td>5068</td>\n",
       "      <td>9495</td>\n",
       "      <td>6579</td>\n",
       "      <td>...</td>\n",
       "      <td>274</td>\n",
       "      <td>59</td>\n",
       "      <td>188</td>\n",
       "      <td>146</td>\n",
       "      <td>72</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>27734</td>\n",
       "      <td>8168</td>\n",
       "      <td>2605</td>\n",
       "      <td>6542</td>\n",
       "      <td>9425</td>\n",
       "      <td>6716</td>\n",
       "      <td>3286</td>\n",
       "      <td>5484</td>\n",
       "      <td>14100</td>\n",
       "      <td>6176</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay</th>\n",
       "      <td>22455</td>\n",
       "      <td>10532</td>\n",
       "      <td>15231</td>\n",
       "      <td>5843</td>\n",
       "      <td>5488</td>\n",
       "      <td>0</td>\n",
       "      <td>6791</td>\n",
       "      <td>3394</td>\n",
       "      <td>8952</td>\n",
       "      <td>2423</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>31</td>\n",
       "      <td>75</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amsterdam</th>\n",
       "      <td>545</td>\n",
       "      <td>471</td>\n",
       "      <td>279</td>\n",
       "      <td>163</td>\n",
       "      <td>242</td>\n",
       "      <td>751</td>\n",
       "      <td>91</td>\n",
       "      <td>178</td>\n",
       "      <td>299</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petits</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>215</td>\n",
       "      <td>154</td>\n",
       "      <td>31</td>\n",
       "      <td>97</td>\n",
       "      <td>75</td>\n",
       "      <td>136</td>\n",
       "      <td>46</td>\n",
       "      <td>51</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>découvrir</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>162</td>\n",
       "      <td>89</td>\n",
       "      <td>134</td>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>488</td>\n",
       "      <td>29</td>\n",
       "      <td>42</td>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visitors</th>\n",
       "      <td>78</td>\n",
       "      <td>38</td>\n",
       "      <td>130</td>\n",
       "      <td>25</td>\n",
       "      <td>46</td>\n",
       "      <td>75</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           great   nice recommend  clean  good   stay comfortable  easy  \\\n",
       "place      26420  14996     16646  14120  7276  36269        6695  4696   \n",
       "apartment  22344  15152      7999  19536  6362  16411        9446  5068   \n",
       "location   27734   8168      2605   6542  9425   6716        3286  5484   \n",
       "stay       22455  10532     15231   5843  5488      0        6791  3394   \n",
       "amsterdam    545    471       279    163   242    751          91   178   \n",
       "...          ...    ...       ...    ...   ...    ...         ...   ...   \n",
       "petits         0      0         2      1     0      0           0     0   \n",
       "show         215    154        31     97    75    136          46    51   \n",
       "découvrir      1      0         0      0     0      0           0     0   \n",
       "wait         162     89       134     72    52    488          29    42   \n",
       "visitors      78     38       130     25    46     75          30    17   \n",
       "\n",
       "          perfect quiet  ... climb chilled downstairs well-located  \\\n",
       "place       11603  6262  ...   104      67        138          106   \n",
       "apartment    9495  6579  ...   274      59        188          146   \n",
       "location    14100  6176  ...    43      27         80            3   \n",
       "stay         8952  2423  ...    78      60         65           31   \n",
       "amsterdam     299   140  ...     7       2          3            1   \n",
       "...           ...   ...  ...   ...     ...        ...          ...   \n",
       "petits          2     0  ...     0       0          0            0   \n",
       "show           49    20  ...     0       1          3            0   \n",
       "découvrir       0     0  ...     0       0          0            0   \n",
       "wait           68    10  ...     1       0          3            0   \n",
       "visitors       41    13  ...     5       0          1            2   \n",
       "\n",
       "          accommodate based andrea avant taxi wi-fi  \n",
       "place              65    58      2    13   91    14  \n",
       "apartment          72    68      1     0  138    26  \n",
       "location           22    49      2     6   86     8  \n",
       "stay               75    49      1     0   93    11  \n",
       "amsterdam           1     1      0     2    6     0  \n",
       "...               ...   ...    ...   ...  ...   ...  \n",
       "petits              0     0      0     7    2     0  \n",
       "show                1     2      0     0    7     0  \n",
       "découvrir           0     0      0     9    0     1  \n",
       "wait                0     0      0     0    7     0  \n",
       "visitors            0     1      0     0    0     0  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    pmidf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "    N = 0\n",
    "    for index, row in df.iterrows():\n",
    "        N += sum(row)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for word in cont_vocab:\n",
    "            try:\n",
    "                pmi = df[word][index] / (sum(df[word])/N / sum(row)/N)\n",
    "                if pmi == 0:\n",
    "                    pmidf[word][index] = 0\n",
    "                else:\n",
    "                    pmidf[word][index] = np.log([pmi])[0] \n",
    "                    #         print(pmidf[word][index])\n",
    "            except: \n",
    "                pmidf[word][index] = 0\n",
    "      \n",
    "    return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "AGftXjXRSuQw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf = cooc2pmi(coocdf)\n",
    "pmidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>great</th>\n",
       "      <th>nice</th>\n",
       "      <th>recommend</th>\n",
       "      <th>clean</th>\n",
       "      <th>good</th>\n",
       "      <th>stay</th>\n",
       "      <th>comfortable</th>\n",
       "      <th>easy</th>\n",
       "      <th>perfect</th>\n",
       "      <th>quiet</th>\n",
       "      <th>...</th>\n",
       "      <th>climb</th>\n",
       "      <th>chilled</th>\n",
       "      <th>downstairs</th>\n",
       "      <th>well-located</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>based</th>\n",
       "      <th>andrea</th>\n",
       "      <th>avant</th>\n",
       "      <th>taxi</th>\n",
       "      <th>wi-fi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>44.688360</td>\n",
       "      <td>44.561319</td>\n",
       "      <td>45.415802</td>\n",
       "      <td>44.752547</td>\n",
       "      <td>44.404542</td>\n",
       "      <td>45.314411</td>\n",
       "      <td>44.417571</td>\n",
       "      <td>43.974104</td>\n",
       "      <td>44.836389</td>\n",
       "      <td>44.397590</td>\n",
       "      <td>...</td>\n",
       "      <td>43.581010</td>\n",
       "      <td>44.346990</td>\n",
       "      <td>43.678172</td>\n",
       "      <td>44.995769</td>\n",
       "      <td>43.858426</td>\n",
       "      <td>44.256807</td>\n",
       "      <td>43.956814</td>\n",
       "      <td>41.736784</td>\n",
       "      <td>43.298902</td>\n",
       "      <td>43.361207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apartment</th>\n",
       "      <td>44.520872</td>\n",
       "      <td>44.571744</td>\n",
       "      <td>44.683024</td>\n",
       "      <td>45.077289</td>\n",
       "      <td>44.270379</td>\n",
       "      <td>44.521475</td>\n",
       "      <td>44.761877</td>\n",
       "      <td>44.050415</td>\n",
       "      <td>44.635966</td>\n",
       "      <td>44.447049</td>\n",
       "      <td>...</td>\n",
       "      <td>44.549823</td>\n",
       "      <td>44.219910</td>\n",
       "      <td>43.987436</td>\n",
       "      <td>45.316012</td>\n",
       "      <td>43.960780</td>\n",
       "      <td>44.415948</td>\n",
       "      <td>43.263743</td>\n",
       "      <td>0</td>\n",
       "      <td>43.715372</td>\n",
       "      <td>43.980322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>44.353178</td>\n",
       "      <td>43.570040</td>\n",
       "      <td>43.177345</td>\n",
       "      <td>43.599478</td>\n",
       "      <td>44.279607</td>\n",
       "      <td>43.244221</td>\n",
       "      <td>43.322161</td>\n",
       "      <td>43.745508</td>\n",
       "      <td>44.647580</td>\n",
       "      <td>44.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>42.314099</td>\n",
       "      <td>43.054414</td>\n",
       "      <td>42.749225</td>\n",
       "      <td>41.047222</td>\n",
       "      <td>42.391361</td>\n",
       "      <td>43.704465</td>\n",
       "      <td>43.573094</td>\n",
       "      <td>40.579874</td>\n",
       "      <td>42.858670</td>\n",
       "      <td>42.417871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay</th>\n",
       "      <td>44.377028</td>\n",
       "      <td>44.059229</td>\n",
       "      <td>45.178240</td>\n",
       "      <td>43.721475</td>\n",
       "      <td>43.973800</td>\n",
       "      <td>0</td>\n",
       "      <td>44.283084</td>\n",
       "      <td>43.500678</td>\n",
       "      <td>44.428278</td>\n",
       "      <td>43.299373</td>\n",
       "      <td>...</td>\n",
       "      <td>43.144603</td>\n",
       "      <td>44.087918</td>\n",
       "      <td>42.776582</td>\n",
       "      <td>43.617593</td>\n",
       "      <td>43.852802</td>\n",
       "      <td>43.939460</td>\n",
       "      <td>43.114943</td>\n",
       "      <td>0</td>\n",
       "      <td>43.171917</td>\n",
       "      <td>42.971321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amsterdam</th>\n",
       "      <td>37.518569</td>\n",
       "      <td>37.811938</td>\n",
       "      <td>38.038388</td>\n",
       "      <td>37.002249</td>\n",
       "      <td>37.712443</td>\n",
       "      <td>38.148398</td>\n",
       "      <td>36.830614</td>\n",
       "      <td>37.412721</td>\n",
       "      <td>37.889113</td>\n",
       "      <td>37.308277</td>\n",
       "      <td>...</td>\n",
       "      <td>37.593829</td>\n",
       "      <td>37.546744</td>\n",
       "      <td>36.560830</td>\n",
       "      <td>37.043629</td>\n",
       "      <td>36.395338</td>\n",
       "      <td>36.907664</td>\n",
       "      <td>0</td>\n",
       "      <td>36.576281</td>\n",
       "      <td>37.291101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petits</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.389007</td>\n",
       "      <td>31.197182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.170500</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.117727</td>\n",
       "      <td>35.481172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>35.840567</td>\n",
       "      <td>35.946178</td>\n",
       "      <td>35.093309</td>\n",
       "      <td>35.735356</td>\n",
       "      <td>35.793139</td>\n",
       "      <td>35.691793</td>\n",
       "      <td>35.400541</td>\n",
       "      <td>35.414908</td>\n",
       "      <td>35.332635</td>\n",
       "      <td>34.614513</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36.105743</td>\n",
       "      <td>35.812976</td>\n",
       "      <td>0</td>\n",
       "      <td>35.647484</td>\n",
       "      <td>36.852957</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.697398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>découvrir</th>\n",
       "      <td>30.439857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.302433</td>\n",
       "      <td>0</td>\n",
       "      <td>36.655523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>35.589380</td>\n",
       "      <td>35.429716</td>\n",
       "      <td>36.589016</td>\n",
       "      <td>35.469165</td>\n",
       "      <td>35.458749</td>\n",
       "      <td>37.001308</td>\n",
       "      <td>34.971050</td>\n",
       "      <td>35.252607</td>\n",
       "      <td>35.692177</td>\n",
       "      <td>33.953220</td>\n",
       "      <td>...</td>\n",
       "      <td>34.931919</td>\n",
       "      <td>0</td>\n",
       "      <td>35.844831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.729252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visitors</th>\n",
       "      <td>33.637459</td>\n",
       "      <td>33.357632</td>\n",
       "      <td>35.337677</td>\n",
       "      <td>33.190341</td>\n",
       "      <td>34.115113</td>\n",
       "      <td>33.907447</td>\n",
       "      <td>33.783918</td>\n",
       "      <td>33.127117</td>\n",
       "      <td>33.965208</td>\n",
       "      <td>32.994551</td>\n",
       "      <td>...</td>\n",
       "      <td>35.320323</td>\n",
       "      <td>0</td>\n",
       "      <td>33.525185</td>\n",
       "      <td>35.799743</td>\n",
       "      <td>0</td>\n",
       "      <td>34.970631</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               great       nice  recommend      clean       good       stay  \\\n",
       "place      44.688360  44.561319  45.415802  44.752547  44.404542  45.314411   \n",
       "apartment  44.520872  44.571744  44.683024  45.077289  44.270379  44.521475   \n",
       "location   44.353178  43.570040  43.177345  43.599478  44.279607  43.244221   \n",
       "stay       44.377028  44.059229  45.178240  43.721475  43.973800          0   \n",
       "amsterdam  37.518569  37.811938  38.038388  37.002249  37.712443  38.148398   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "petits             0          0  32.389007  31.197182          0          0   \n",
       "show       35.840567  35.946178  35.093309  35.735356  35.793139  35.691793   \n",
       "découvrir  30.439857          0          0          0          0          0   \n",
       "wait       35.589380  35.429716  36.589016  35.469165  35.458749  37.001308   \n",
       "visitors   33.637459  33.357632  35.337677  33.190341  34.115113  33.907447   \n",
       "\n",
       "          comfortable       easy    perfect      quiet  ...      climb  \\\n",
       "place       44.417571  43.974104  44.836389  44.397590  ...  43.581010   \n",
       "apartment   44.761877  44.050415  44.635966  44.447049  ...  44.549823   \n",
       "location    43.322161  43.745508  44.647580  44.000041  ...  42.314099   \n",
       "stay        44.283084  43.500678  44.428278  43.299373  ...  43.144603   \n",
       "amsterdam   36.830614  37.412721  37.889113  37.308277  ...  37.593829   \n",
       "...               ...        ...        ...        ...  ...        ...   \n",
       "petits              0          0  32.170500          0  ...          0   \n",
       "show        35.400541  35.414908  35.332635  34.614513  ...          0   \n",
       "découvrir           0          0          0          0  ...          0   \n",
       "wait        34.971050  35.252607  35.692177  33.953220  ...  34.931919   \n",
       "visitors    33.783918  33.127117  33.965208  32.994551  ...  35.320323   \n",
       "\n",
       "             chilled downstairs well-located accommodate      based  \\\n",
       "place      44.346990  43.678172    44.995769   43.858426  44.256807   \n",
       "apartment  44.219910  43.987436    45.316012   43.960780  44.415948   \n",
       "location   43.054414  42.749225    41.047222   42.391361  43.704465   \n",
       "stay       44.087918  42.776582    43.617593   43.852802  43.939460   \n",
       "amsterdam  37.546744  36.560830    37.043629   36.395338  36.907664   \n",
       "...              ...        ...          ...         ...        ...   \n",
       "petits             0          0            0           0          0   \n",
       "show       36.105743  35.812976            0   35.647484  36.852957   \n",
       "découvrir          0          0            0           0          0   \n",
       "wait               0  35.844831            0           0          0   \n",
       "visitors           0  33.525185    35.799743           0  34.970631   \n",
       "\n",
       "              andrea      avant       taxi      wi-fi  \n",
       "place      43.956814  41.736784  43.298902  43.361207  \n",
       "apartment  43.263743          0  43.715372  43.980322  \n",
       "location   43.573094  40.579874  42.858670  42.417871  \n",
       "stay       43.114943          0  43.171917  42.971321  \n",
       "amsterdam          0  36.576281  37.291101          0  \n",
       "...              ...        ...        ...        ...  \n",
       "petits             0  37.117727  35.481172          0  \n",
       "show               0          0  36.697398          0  \n",
       "découvrir          0  37.302433          0  36.655523  \n",
       "wait               0          0  36.729252          0  \n",
       "visitors           0          0          0          0  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.02694901656134"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pmidf.iloc[2,:]\n",
    "pmidf['place']['room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.68836008719678"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf['great']['place']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.888947057666115"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf['place']['nice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.256807340404556"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf['based']['place']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "        center_word: The dataframe we want to modify.\n",
    "        N: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    dicts_ = {word: df[word][center_word] for word in cont_vocab}\n",
    "    top_words = [key for key, value in sorted(dicts_.items(), key=lambda item: item[1], reverse=True)][:N]\n",
    "\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "1I038zG1Sw62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recommand',\n",
       " 'recomend',\n",
       " 'recommend',\n",
       " 'recommendable',\n",
       " 'stay',\n",
       " 'reccomend',\n",
       " 'maarten',\n",
       " 'looking',\n",
       " 'nicole',\n",
       " 'affordable']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prime',\n",
       " 'superb',\n",
       " 'ideal',\n",
       " 'convenient',\n",
       " 'perfect',\n",
       " 'terrific',\n",
       " 'walkable',\n",
       " 'brilliant',\n",
       " 'fantastic',\n",
       " 'central']"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tea',\n",
       " 'nespresso',\n",
       " 'microwave',\n",
       " 'complimentary',\n",
       " 'fridge',\n",
       " 'shops',\n",
       " 'supplied',\n",
       " 'nick',\n",
       " 'cheese',\n",
       " 'including']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enjoyed',\n",
       " 'enjoyable',\n",
       " 'hesitate',\n",
       " 'future',\n",
       " 'memorable',\n",
       " 'hope',\n",
       " 'letting',\n",
       " 'love',\n",
       " 'longer',\n",
       " 'pleasant']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'stay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aux',\n",
       " 'ses',\n",
       " 'sont',\n",
       " 'ont',\n",
       " 'des',\n",
       " 'apprécié',\n",
       " 'hôtes',\n",
       " 'les',\n",
       " 'cafés',\n",
       " 'avec']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'petits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zimmer',\n",
       " 'sehr',\n",
       " 'wie',\n",
       " 'und',\n",
       " 'alles',\n",
       " 'wohnung',\n",
       " 'ist',\n",
       " 'zentral',\n",
       " 'unterkunft',\n",
       " 'allem']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'sauber')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfcm5-7b0HKO"
   },
   "source": [
    "# 3.b Ethical, social and legal implications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3uf-Qq4tYg"
   },
   "source": [
    "Local authorities in touristic hotspots like Amsterdam, NYC or Barcelona regulate the price of recreational apartments for rent to, among others, ensure that fair rent prices are kept for year-long residents. Consider your price recommender for hosts in Question 2c. Imagine that Airbnb recommends a new host to put the price of your flat at a price which is above the official regulations established by the local government. Upon inspection, you realize that the inflated price you have been recommended comes from many apartments in the area only being offered during an annual event which brings many tourists, and which causes prices to rise. \n",
    "\n",
    "In this context, critically reflect on the compliance of this recommender system with **one of the five actions** outlined in the **UK’s Data Ethics Framework**. You should prioritize the action that, in your opinion, is the weakest. Then, justify your choice by critically analyzing the three **key principles** outlined in the Framework, namely _transparency_, _accountability_ and _fairness_. Finally, you should propose and critically justify a solution that would improve the recommender system in at least one of these principles. You are strongly encouraged to follow a scholarly approach, e.g., with peer-reviewed references as support. \n",
    "\n",
    "Your report should be between 500 and 750 words long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6QJyuP6I1Ht"
   },
   "source": [
    "### Your answer here. No Python, only Markdown.\n",
    "\n",
    "Write your answer after the line.\n",
    "\n",
    "---\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3f75a622fdbe68ac4774c6ea619d86cc770141a8bef94a85fce2870eb7cb09bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
