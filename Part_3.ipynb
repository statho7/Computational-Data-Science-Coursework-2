{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We want to find the alternative forms of stopwords that have the \"'\" symbol in them \n",
    "in order to be able to add also to stopwords the word without this symbol'''\n",
    "\n",
    "pattern = r'\\w+\\'\\w+'\n",
    "\n",
    "new_stopwords = []\n",
    "for word in sw:\n",
    "    # If it finds a word that contains \"'\" it appends the word in new_stopwords list\n",
    "    if len(re.findall(pattern,word)) == 1:\n",
    "        new_stopwords.append(re.findall(pattern,word)[0].replace('\\'',''))\n",
    "new_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After checking those \"new\" words we add them to the stopwords variables named sw\n",
    "for word in new_stopwords:\n",
    "    sw.add(word)\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "basedir = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(basedir,'reviews.csv'))\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "def process_reviews(df):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    # Initialize 3 lists one for each column we will create\n",
    "    tokenized_col = []\n",
    "    tagged_col = []\n",
    "    lower_tagged_col = []\n",
    "\n",
    "\n",
    "    mylen = len(df)\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through the given dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # tokenize the words for the comments of a row\n",
    "        token = word_tokenize(row.comments)\n",
    "        # Append the tokenized words to the proper list\n",
    "        tokenized_col.append(token)\n",
    "        # Tag the tokenized words of the row and then append them to the proper list\n",
    "        tagged_col.append(pos_tag(token))\n",
    "        # lower_tagged.append(list(set(pos_tag([item.lower() for item in token]))))\n",
    "        # Make the tagged words lowercased and then if they are not stopwords append them to the lower_tagged_col list\n",
    "        lower_tagged_col.append(pos_tag([item.lower() for item in token if item.lower() not in sw]))\n",
    "        count += 1\n",
    "\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} out of {mylen}')\n",
    "\n",
    "    # Set as values of the 3 new columns the proper list we created for each one\n",
    "    df['tokenized'] = tokenized_col\n",
    "    df['tagged'] = tagged_col\n",
    "    df['lower_tagged'] = lower_tagged_col\n",
    "\n",
    "    # Return the modified dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rGYB8gx5Qq-P",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 out of 452143\n",
      "2000 out of 452143\n",
      "3000 out of 452143\n",
      "4000 out of 452143\n",
      "5000 out of 452143\n",
      "6000 out of 452143\n",
      "7000 out of 452143\n",
      "8000 out of 452143\n",
      "9000 out of 452143\n",
      "10000 out of 452143\n",
      "11000 out of 452143\n",
      "12000 out of 452143\n",
      "13000 out of 452143\n",
      "14000 out of 452143\n",
      "15000 out of 452143\n",
      "16000 out of 452143\n",
      "17000 out of 452143\n",
      "18000 out of 452143\n",
      "19000 out of 452143\n",
      "20000 out of 452143\n",
      "21000 out of 452143\n",
      "22000 out of 452143\n",
      "23000 out of 452143\n",
      "24000 out of 452143\n",
      "25000 out of 452143\n",
      "26000 out of 452143\n",
      "27000 out of 452143\n",
      "28000 out of 452143\n",
      "29000 out of 452143\n",
      "30000 out of 452143\n",
      "31000 out of 452143\n",
      "32000 out of 452143\n",
      "33000 out of 452143\n",
      "34000 out of 452143\n",
      "35000 out of 452143\n",
      "36000 out of 452143\n",
      "37000 out of 452143\n",
      "38000 out of 452143\n",
      "39000 out of 452143\n",
      "40000 out of 452143\n",
      "41000 out of 452143\n",
      "42000 out of 452143\n",
      "43000 out of 452143\n",
      "44000 out of 452143\n",
      "45000 out of 452143\n",
      "46000 out of 452143\n",
      "47000 out of 452143\n",
      "48000 out of 452143\n",
      "49000 out of 452143\n",
      "50000 out of 452143\n",
      "51000 out of 452143\n",
      "52000 out of 452143\n",
      "53000 out of 452143\n",
      "54000 out of 452143\n",
      "55000 out of 452143\n",
      "56000 out of 452143\n",
      "57000 out of 452143\n",
      "58000 out of 452143\n",
      "59000 out of 452143\n",
      "60000 out of 452143\n",
      "61000 out of 452143\n",
      "62000 out of 452143\n",
      "63000 out of 452143\n",
      "64000 out of 452143\n",
      "65000 out of 452143\n",
      "66000 out of 452143\n",
      "67000 out of 452143\n",
      "68000 out of 452143\n",
      "69000 out of 452143\n",
      "70000 out of 452143\n",
      "71000 out of 452143\n",
      "72000 out of 452143\n",
      "73000 out of 452143\n",
      "74000 out of 452143\n",
      "75000 out of 452143\n",
      "76000 out of 452143\n",
      "77000 out of 452143\n",
      "78000 out of 452143\n",
      "79000 out of 452143\n",
      "80000 out of 452143\n",
      "81000 out of 452143\n",
      "82000 out of 452143\n",
      "83000 out of 452143\n",
      "84000 out of 452143\n",
      "85000 out of 452143\n",
      "86000 out of 452143\n",
      "87000 out of 452143\n",
      "88000 out of 452143\n",
      "89000 out of 452143\n",
      "90000 out of 452143\n",
      "91000 out of 452143\n",
      "92000 out of 452143\n",
      "93000 out of 452143\n",
      "94000 out of 452143\n",
      "95000 out of 452143\n",
      "96000 out of 452143\n",
      "97000 out of 452143\n",
      "98000 out of 452143\n",
      "99000 out of 452143\n",
      "100000 out of 452143\n",
      "101000 out of 452143\n",
      "102000 out of 452143\n",
      "103000 out of 452143\n",
      "104000 out of 452143\n",
      "105000 out of 452143\n",
      "106000 out of 452143\n",
      "107000 out of 452143\n",
      "108000 out of 452143\n",
      "109000 out of 452143\n",
      "110000 out of 452143\n",
      "111000 out of 452143\n",
      "112000 out of 452143\n",
      "113000 out of 452143\n",
      "114000 out of 452143\n",
      "115000 out of 452143\n",
      "116000 out of 452143\n",
      "117000 out of 452143\n",
      "118000 out of 452143\n",
      "119000 out of 452143\n",
      "120000 out of 452143\n",
      "121000 out of 452143\n",
      "122000 out of 452143\n",
      "123000 out of 452143\n",
      "124000 out of 452143\n",
      "125000 out of 452143\n",
      "126000 out of 452143\n",
      "127000 out of 452143\n",
      "128000 out of 452143\n",
      "129000 out of 452143\n",
      "130000 out of 452143\n",
      "131000 out of 452143\n",
      "132000 out of 452143\n",
      "133000 out of 452143\n",
      "134000 out of 452143\n",
      "135000 out of 452143\n",
      "136000 out of 452143\n",
      "137000 out of 452143\n",
      "138000 out of 452143\n",
      "139000 out of 452143\n",
      "140000 out of 452143\n",
      "141000 out of 452143\n",
      "142000 out of 452143\n",
      "143000 out of 452143\n",
      "144000 out of 452143\n",
      "145000 out of 452143\n",
      "146000 out of 452143\n",
      "147000 out of 452143\n",
      "148000 out of 452143\n",
      "149000 out of 452143\n",
      "150000 out of 452143\n",
      "151000 out of 452143\n",
      "152000 out of 452143\n",
      "153000 out of 452143\n",
      "154000 out of 452143\n",
      "155000 out of 452143\n",
      "156000 out of 452143\n",
      "157000 out of 452143\n",
      "158000 out of 452143\n",
      "159000 out of 452143\n",
      "160000 out of 452143\n",
      "161000 out of 452143\n",
      "162000 out of 452143\n",
      "163000 out of 452143\n",
      "164000 out of 452143\n",
      "165000 out of 452143\n",
      "166000 out of 452143\n",
      "167000 out of 452143\n",
      "168000 out of 452143\n",
      "169000 out of 452143\n",
      "170000 out of 452143\n",
      "171000 out of 452143\n",
      "172000 out of 452143\n",
      "173000 out of 452143\n",
      "174000 out of 452143\n",
      "175000 out of 452143\n",
      "176000 out of 452143\n",
      "177000 out of 452143\n",
      "178000 out of 452143\n",
      "179000 out of 452143\n",
      "180000 out of 452143\n",
      "181000 out of 452143\n",
      "182000 out of 452143\n",
      "183000 out of 452143\n",
      "184000 out of 452143\n",
      "185000 out of 452143\n",
      "186000 out of 452143\n",
      "187000 out of 452143\n",
      "188000 out of 452143\n",
      "189000 out of 452143\n",
      "190000 out of 452143\n",
      "191000 out of 452143\n",
      "192000 out of 452143\n",
      "193000 out of 452143\n",
      "194000 out of 452143\n",
      "195000 out of 452143\n",
      "196000 out of 452143\n",
      "197000 out of 452143\n",
      "198000 out of 452143\n",
      "199000 out of 452143\n",
      "200000 out of 452143\n",
      "201000 out of 452143\n",
      "202000 out of 452143\n",
      "203000 out of 452143\n",
      "204000 out of 452143\n",
      "205000 out of 452143\n",
      "206000 out of 452143\n",
      "207000 out of 452143\n",
      "208000 out of 452143\n",
      "209000 out of 452143\n",
      "210000 out of 452143\n",
      "211000 out of 452143\n",
      "212000 out of 452143\n",
      "213000 out of 452143\n",
      "214000 out of 452143\n",
      "215000 out of 452143\n",
      "216000 out of 452143\n",
      "217000 out of 452143\n",
      "218000 out of 452143\n",
      "219000 out of 452143\n",
      "220000 out of 452143\n",
      "221000 out of 452143\n",
      "222000 out of 452143\n",
      "223000 out of 452143\n",
      "224000 out of 452143\n",
      "225000 out of 452143\n",
      "226000 out of 452143\n",
      "227000 out of 452143\n",
      "228000 out of 452143\n",
      "229000 out of 452143\n",
      "230000 out of 452143\n",
      "231000 out of 452143\n",
      "232000 out of 452143\n",
      "233000 out of 452143\n",
      "234000 out of 452143\n",
      "235000 out of 452143\n",
      "236000 out of 452143\n",
      "237000 out of 452143\n",
      "238000 out of 452143\n",
      "239000 out of 452143\n",
      "240000 out of 452143\n",
      "241000 out of 452143\n",
      "242000 out of 452143\n",
      "243000 out of 452143\n",
      "244000 out of 452143\n",
      "245000 out of 452143\n",
      "246000 out of 452143\n",
      "247000 out of 452143\n",
      "248000 out of 452143\n",
      "249000 out of 452143\n",
      "250000 out of 452143\n",
      "251000 out of 452143\n",
      "252000 out of 452143\n",
      "253000 out of 452143\n",
      "254000 out of 452143\n",
      "255000 out of 452143\n",
      "256000 out of 452143\n",
      "257000 out of 452143\n",
      "258000 out of 452143\n",
      "259000 out of 452143\n",
      "260000 out of 452143\n",
      "261000 out of 452143\n",
      "262000 out of 452143\n",
      "263000 out of 452143\n",
      "264000 out of 452143\n",
      "265000 out of 452143\n",
      "266000 out of 452143\n",
      "267000 out of 452143\n",
      "268000 out of 452143\n",
      "269000 out of 452143\n",
      "270000 out of 452143\n",
      "271000 out of 452143\n",
      "272000 out of 452143\n",
      "273000 out of 452143\n",
      "274000 out of 452143\n",
      "275000 out of 452143\n",
      "276000 out of 452143\n",
      "277000 out of 452143\n",
      "278000 out of 452143\n",
      "279000 out of 452143\n",
      "280000 out of 452143\n",
      "281000 out of 452143\n",
      "282000 out of 452143\n",
      "283000 out of 452143\n",
      "284000 out of 452143\n",
      "285000 out of 452143\n",
      "286000 out of 452143\n",
      "287000 out of 452143\n",
      "288000 out of 452143\n",
      "289000 out of 452143\n",
      "290000 out of 452143\n",
      "291000 out of 452143\n",
      "292000 out of 452143\n",
      "293000 out of 452143\n",
      "294000 out of 452143\n",
      "295000 out of 452143\n",
      "296000 out of 452143\n",
      "297000 out of 452143\n",
      "298000 out of 452143\n",
      "299000 out of 452143\n",
      "300000 out of 452143\n",
      "301000 out of 452143\n",
      "302000 out of 452143\n",
      "303000 out of 452143\n",
      "304000 out of 452143\n",
      "305000 out of 452143\n",
      "306000 out of 452143\n",
      "307000 out of 452143\n",
      "308000 out of 452143\n",
      "309000 out of 452143\n",
      "310000 out of 452143\n",
      "311000 out of 452143\n",
      "312000 out of 452143\n",
      "313000 out of 452143\n",
      "314000 out of 452143\n",
      "315000 out of 452143\n",
      "316000 out of 452143\n",
      "317000 out of 452143\n",
      "318000 out of 452143\n",
      "319000 out of 452143\n",
      "320000 out of 452143\n",
      "321000 out of 452143\n",
      "322000 out of 452143\n",
      "323000 out of 452143\n",
      "324000 out of 452143\n",
      "325000 out of 452143\n",
      "326000 out of 452143\n",
      "327000 out of 452143\n",
      "328000 out of 452143\n",
      "329000 out of 452143\n",
      "330000 out of 452143\n",
      "331000 out of 452143\n",
      "332000 out of 452143\n",
      "333000 out of 452143\n",
      "334000 out of 452143\n",
      "335000 out of 452143\n",
      "336000 out of 452143\n",
      "337000 out of 452143\n",
      "338000 out of 452143\n",
      "339000 out of 452143\n",
      "340000 out of 452143\n",
      "341000 out of 452143\n",
      "342000 out of 452143\n",
      "343000 out of 452143\n",
      "344000 out of 452143\n",
      "345000 out of 452143\n",
      "346000 out of 452143\n",
      "347000 out of 452143\n",
      "348000 out of 452143\n",
      "349000 out of 452143\n",
      "350000 out of 452143\n",
      "351000 out of 452143\n",
      "352000 out of 452143\n",
      "353000 out of 452143\n",
      "354000 out of 452143\n",
      "355000 out of 452143\n",
      "356000 out of 452143\n",
      "357000 out of 452143\n",
      "358000 out of 452143\n",
      "359000 out of 452143\n",
      "360000 out of 452143\n",
      "361000 out of 452143\n",
      "362000 out of 452143\n",
      "363000 out of 452143\n",
      "364000 out of 452143\n",
      "365000 out of 452143\n",
      "366000 out of 452143\n",
      "367000 out of 452143\n",
      "368000 out of 452143\n",
      "369000 out of 452143\n",
      "370000 out of 452143\n",
      "371000 out of 452143\n",
      "372000 out of 452143\n",
      "373000 out of 452143\n",
      "374000 out of 452143\n",
      "375000 out of 452143\n",
      "376000 out of 452143\n",
      "377000 out of 452143\n",
      "378000 out of 452143\n",
      "379000 out of 452143\n",
      "380000 out of 452143\n",
      "381000 out of 452143\n",
      "382000 out of 452143\n",
      "383000 out of 452143\n",
      "384000 out of 452143\n",
      "385000 out of 452143\n",
      "386000 out of 452143\n",
      "387000 out of 452143\n",
      "388000 out of 452143\n",
      "389000 out of 452143\n",
      "390000 out of 452143\n",
      "391000 out of 452143\n",
      "392000 out of 452143\n",
      "393000 out of 452143\n",
      "394000 out of 452143\n",
      "395000 out of 452143\n",
      "396000 out of 452143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397000 out of 452143\n",
      "398000 out of 452143\n",
      "399000 out of 452143\n",
      "400000 out of 452143\n",
      "401000 out of 452143\n",
      "402000 out of 452143\n",
      "403000 out of 452143\n",
      "404000 out of 452143\n",
      "405000 out of 452143\n",
      "406000 out of 452143\n",
      "407000 out of 452143\n",
      "408000 out of 452143\n",
      "409000 out of 452143\n",
      "410000 out of 452143\n",
      "411000 out of 452143\n",
      "412000 out of 452143\n",
      "413000 out of 452143\n",
      "414000 out of 452143\n",
      "415000 out of 452143\n",
      "416000 out of 452143\n",
      "417000 out of 452143\n",
      "418000 out of 452143\n",
      "419000 out of 452143\n",
      "420000 out of 452143\n",
      "421000 out of 452143\n",
      "422000 out of 452143\n",
      "423000 out of 452143\n",
      "424000 out of 452143\n",
      "425000 out of 452143\n",
      "426000 out of 452143\n",
      "427000 out of 452143\n",
      "428000 out of 452143\n",
      "429000 out of 452143\n",
      "430000 out of 452143\n",
      "431000 out of 452143\n",
      "432000 out of 452143\n",
      "433000 out of 452143\n",
      "434000 out of 452143\n",
      "435000 out of 452143\n",
      "436000 out of 452143\n",
      "437000 out of 452143\n",
      "438000 out of 452143\n",
      "439000 out of 452143\n",
      "440000 out of 452143\n",
      "441000 out of 452143\n",
      "442000 out of 452143\n",
      "443000 out of 452143\n",
      "444000 out of 452143\n",
      "445000 out of 452143\n",
      "446000 out of 452143\n",
      "447000 out of 452143\n",
      "448000 out of 452143\n",
      "449000 out of 452143\n",
      "450000 out of 452143\n",
      "451000 out of 452143\n",
      "452000 out of 452143\n"
     ]
    }
   ],
   "source": [
    "df = process_reviews(df)\n",
    "# df = process_reviews(df[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [(daniel, NN), (really, RB), (cool, JJ), (., ....\n",
       "1         [(daniel, NN), (amazing, VBG), (host, NN), (!,...\n",
       "2         [(great, JJ), (time, NN), (amsterdam, NN), (.,...\n",
       "3         [(professional, JJ), (operation, NN), (., .), ...\n",
       "4         [(daniel, NN), (highly, RB), (recommended, VBD...\n",
       "                                ...                        \n",
       "452138    [(great, JJ), (shared, VBD), (private, JJ), (s...\n",
       "452139    [(muy, NN), (buen, NN), (trato, NN), (,, ,), (...\n",
       "452140    [(stayed, VBN), (night, NN), (last, JJ), (minu...\n",
       "452141                                         [(good, JJ)]\n",
       "452142    [(amazing, VBG), (experience, NN), (!, .), (ly...\n",
       "Name: lower_tagged, Length: 452143, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lower_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(lowertagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # correction with lower_tagged.append(pos_tag([item.lower() for item in token if item.lower() not in sw]))\n",
    "# lowertagged = []\n",
    "\n",
    "# pattern = r'\\w+'\n",
    "\n",
    "# for line in df.lower_tagged:\n",
    "# #     print(line)\n",
    "#     for word in line:\n",
    "#         if word[1][0] == 'N' and len(re.findall(pattern,word[0])) == 0: \n",
    "#             pass\n",
    "# #             print(word)\n",
    "#         else:\n",
    "#             lowertagged.append(word[0])\n",
    "# #         if word[1][0] == 'N': print(word)\n",
    "#             # print(re.findall(pattern,word[0]))\n",
    "#             # if \n",
    "# len(lowertagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tagged</th>\n",
       "      <th>lower_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "      <td>[Daniel, is, really, cool, ., The, place, was,...</td>\n",
       "      <td>[(Daniel, NNP), (is, VBZ), (really, RB), (cool...</td>\n",
       "      <td>[(daniel, NN), (really, RB), (cool, JJ), (., ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "      <td>[Daniel, is, the, most, amazing, host, !, His,...</td>\n",
       "      <td>[(Daniel, NNP), (is, VBZ), (the, DT), (most, R...</td>\n",
       "      <td>[(daniel, NN), (amazing, VBG), (host, NN), (!,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "      <td>[We, had, such, a, great, time, in, Amsterdam,...</td>\n",
       "      <td>[(We, PRP), (had, VBD), (such, JJ), (a, DT), (...</td>\n",
       "      <td>[(great, JJ), (time, NN), (amsterdam, NN), (.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "      <td>[Very, professional, operation, ., Room, is, v...</td>\n",
       "      <td>[(Very, RB), (professional, JJ), (operation, N...</td>\n",
       "      <td>[(professional, JJ), (operation, NN), (., .), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "      <td>[Daniel, is, highly, recommended, ., He, provi...</td>\n",
       "      <td>[(Daniel, NNP), (is, VBZ), (highly, RB), (reco...</td>\n",
       "      <td>[(daniel, NN), (highly, RB), (recommended, VBD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452138</th>\n",
       "      <td>46522591</td>\n",
       "      <td>714678579</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>67834020</td>\n",
       "      <td>Nikolay</td>\n",
       "      <td>Great shared and private spaces, very comfortable</td>\n",
       "      <td>[Great, shared, and, private, spaces, ,, very,...</td>\n",
       "      <td>[(Great, NNP), (shared, VBD), (and, CC), (priv...</td>\n",
       "      <td>[(great, JJ), (shared, VBD), (private, JJ), (s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452139</th>\n",
       "      <td>46522658</td>\n",
       "      <td>710509852</td>\n",
       "      <td>2020-11-22</td>\n",
       "      <td>228202558</td>\n",
       "      <td>Alekos</td>\n",
       "      <td>Muy buen trato, central  barato y acogedor</td>\n",
       "      <td>[Muy, buen, trato, ,, central, barato, y, acog...</td>\n",
       "      <td>[(Muy, NNP), (buen, NN), (trato, NN), (,, ,), ...</td>\n",
       "      <td>[(muy, NN), (buen, NN), (trato, NN), (,, ,), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452140</th>\n",
       "      <td>46558182</td>\n",
       "      <td>710474593</td>\n",
       "      <td>2020-11-22</td>\n",
       "      <td>23339864</td>\n",
       "      <td>Khalil</td>\n",
       "      <td>Stayed here for a night last minute and had a ...</td>\n",
       "      <td>[Stayed, here, for, a, night, last, minute, an...</td>\n",
       "      <td>[(Stayed, NNP), (here, RB), (for, IN), (a, DT)...</td>\n",
       "      <td>[(stayed, VBN), (night, NN), (last, JJ), (minu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452141</th>\n",
       "      <td>46591961</td>\n",
       "      <td>711401716</td>\n",
       "      <td>2020-11-26</td>\n",
       "      <td>366346169</td>\n",
       "      <td>Alejandro</td>\n",
       "      <td>Good</td>\n",
       "      <td>[Good]</td>\n",
       "      <td>[(Good, JJ)]</td>\n",
       "      <td>[(good, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452142</th>\n",
       "      <td>46866507</td>\n",
       "      <td>715138274</td>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>234622860</td>\n",
       "      <td>Beryl</td>\n",
       "      <td>What an amazing experience! Lydia was such a l...</td>\n",
       "      <td>[What, an, amazing, experience, !, Lydia, was,...</td>\n",
       "      <td>[(What, WP), (an, DT), (amazing, JJ), (experie...</td>\n",
       "      <td>[(amazing, VBG), (experience, NN), (!, .), (ly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452143 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        listing_id         id        date  reviewer_id reviewer_name  \\\n",
       "0             2818       1191  2009-03-30        10952           Lam   \n",
       "1             2818       1771  2009-04-24        12798         Alice   \n",
       "2             2818       1989  2009-05-03        11869       Natalja   \n",
       "3             2818       2797  2009-05-18        14064       Enrique   \n",
       "4             2818       3151  2009-05-25        17977       Sherwin   \n",
       "...            ...        ...         ...          ...           ...   \n",
       "452138    46522591  714678579  2020-12-11     67834020       Nikolay   \n",
       "452139    46522658  710509852  2020-11-22    228202558        Alekos   \n",
       "452140    46558182  710474593  2020-11-22     23339864        Khalil   \n",
       "452141    46591961  711401716  2020-11-26    366346169     Alejandro   \n",
       "452142    46866507  715138274  2020-12-13    234622860         Beryl   \n",
       "\n",
       "                                                 comments  \\\n",
       "0       Daniel is really cool. The place was nice and ...   \n",
       "1       Daniel is the most amazing host! His place is ...   \n",
       "2       We had such a great time in Amsterdam. Daniel ...   \n",
       "3       Very professional operation. Room is very clea...   \n",
       "4       Daniel is highly recommended.  He provided all...   \n",
       "...                                                   ...   \n",
       "452138  Great shared and private spaces, very comfortable   \n",
       "452139         Muy buen trato, central  barato y acogedor   \n",
       "452140  Stayed here for a night last minute and had a ...   \n",
       "452141                                               Good   \n",
       "452142  What an amazing experience! Lydia was such a l...   \n",
       "\n",
       "                                                tokenized  \\\n",
       "0       [Daniel, is, really, cool, ., The, place, was,...   \n",
       "1       [Daniel, is, the, most, amazing, host, !, His,...   \n",
       "2       [We, had, such, a, great, time, in, Amsterdam,...   \n",
       "3       [Very, professional, operation, ., Room, is, v...   \n",
       "4       [Daniel, is, highly, recommended, ., He, provi...   \n",
       "...                                                   ...   \n",
       "452138  [Great, shared, and, private, spaces, ,, very,...   \n",
       "452139  [Muy, buen, trato, ,, central, barato, y, acog...   \n",
       "452140  [Stayed, here, for, a, night, last, minute, an...   \n",
       "452141                                             [Good]   \n",
       "452142  [What, an, amazing, experience, !, Lydia, was,...   \n",
       "\n",
       "                                                   tagged  \\\n",
       "0       [(Daniel, NNP), (is, VBZ), (really, RB), (cool...   \n",
       "1       [(Daniel, NNP), (is, VBZ), (the, DT), (most, R...   \n",
       "2       [(We, PRP), (had, VBD), (such, JJ), (a, DT), (...   \n",
       "3       [(Very, RB), (professional, JJ), (operation, N...   \n",
       "4       [(Daniel, NNP), (is, VBZ), (highly, RB), (reco...   \n",
       "...                                                   ...   \n",
       "452138  [(Great, NNP), (shared, VBD), (and, CC), (priv...   \n",
       "452139  [(Muy, NNP), (buen, NN), (trato, NN), (,, ,), ...   \n",
       "452140  [(Stayed, NNP), (here, RB), (for, IN), (a, DT)...   \n",
       "452141                                       [(Good, JJ)]   \n",
       "452142  [(What, WP), (an, DT), (amazing, JJ), (experie...   \n",
       "\n",
       "                                             lower_tagged  \n",
       "0       [(daniel, NN), (really, RB), (cool, JJ), (., ....  \n",
       "1       [(daniel, NN), (amazing, VBG), (host, NN), (!,...  \n",
       "2       [(great, JJ), (time, NN), (amsterdam, NN), (.,...  \n",
       "3       [(professional, JJ), (operation, NN), (., .), ...  \n",
       "4       [(daniel, NN), (highly, RB), (recommended, VBD...  \n",
       "...                                                   ...  \n",
       "452138  [(great, JJ), (shared, VBD), (private, JJ), (s...  \n",
       "452139  [(muy, NN), (buen, NN), (trato, NN), (,, ,), (...  \n",
       "452140  [(stayed, VBN), (night, NN), (last, JJ), (minu...  \n",
       "452141                                       [(good, JJ)]  \n",
       "452142  [(amazing, VBG), (experience, NN), (!, .), (ly...  \n",
       "\n",
       "[452143 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar = \"CHUNK: {<JJ>*<NN.>+}\" \n",
    "# cp = RegexpParser(grammar)\n",
    "# parsed = cp.parse(df.tagged[0])\n",
    "# print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tree in parsed.subtrees():\n",
    "#     print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.lower_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 0\n",
    "# print(len(df.tagged[num]), len(set(df.lower_tagged[num])))\n",
    "# list(set(df.lower_tagged[num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "  cent_list, cont_list = [], []\n",
    "\n",
    "  for review in df.lower_tagged:\n",
    "    cent_list.extend([word for word in [list_of_words[0] for list_of_words in review if list_of_words[1][0] == 'N']])\n",
    "    cont_list.extend([word for word in [list_of_words[0] for list_of_words in review if (list_of_words[1][0] == 'J') or (list_of_words[1][0] == 'V')]])\n",
    "    \n",
    "  cent_dict = Counter(cent_list)\n",
    "  cont_dict = Counter(cont_list)\n",
    "\n",
    "  cent_vocab = [key for key, value in sorted(cent_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "  cont_vocab = [key for key, value in sorted(cont_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "\n",
    "  return cent_vocab, cont_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "F_R5l4IVSk9-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place',\n",
       " 'apartment',\n",
       " 'location',\n",
       " 'stay',\n",
       " 'amsterdam',\n",
       " 'host',\n",
       " 'everything',\n",
       " 'city',\n",
       " 'room',\n",
       " 'time',\n",
       " 'house',\n",
       " 'area',\n",
       " 'home',\n",
       " 'très',\n",
       " 'center',\n",
       " 'restaurants',\n",
       " '’',\n",
       " 'station',\n",
       " 'minutes',\n",
       " 'walk',\n",
       " 'centre',\n",
       " 'tram',\n",
       " 'experience',\n",
       " 'space',\n",
       " 'thanks',\n",
       " 'à',\n",
       " 'hosts',\n",
       " 'neighborhood',\n",
       " 'clean',\n",
       " 'bien',\n",
       " 'perfect',\n",
       " 'communication',\n",
       " 'day',\n",
       " 'la',\n",
       " 'kind',\n",
       " 'distance',\n",
       " 'days',\n",
       " 'bed',\n",
       " 'trip',\n",
       " 'bathroom',\n",
       " 'et',\n",
       " 'night',\n",
       " 'e',\n",
       " 'sehr',\n",
       " 'people',\n",
       " 'thank',\n",
       " 'places',\n",
       " 'breakfast',\n",
       " 'lot',\n",
       " 'ist']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cent_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'nice',\n",
       " 'recommend',\n",
       " 'clean',\n",
       " 'good',\n",
       " 'stay',\n",
       " 'comfortable',\n",
       " 'easy',\n",
       " 'perfect',\n",
       " 'quiet',\n",
       " 'un',\n",
       " 'beautiful',\n",
       " 'located',\n",
       " 'helpful',\n",
       " 'central',\n",
       " 'super',\n",
       " 'est',\n",
       " 'amazing',\n",
       " 'close',\n",
       " 'amsterdam',\n",
       " 'wonderful',\n",
       " 'flat',\n",
       " '’',\n",
       " 'get',\n",
       " 'nous',\n",
       " 'made',\n",
       " 'staying',\n",
       " 'enjoyed',\n",
       " 'walking',\n",
       " 'spacious',\n",
       " 'loved',\n",
       " 'little',\n",
       " 'needed',\n",
       " 'fantastic',\n",
       " 'go',\n",
       " 'best',\n",
       " 'gave',\n",
       " 'friendly',\n",
       " 'come',\n",
       " 'excellent',\n",
       " 'small',\n",
       " 'public',\n",
       " 'local',\n",
       " 'stayed',\n",
       " 'walk',\n",
       " 'next',\n",
       " 'many',\n",
       " 'tram',\n",
       " 'short',\n",
       " 'recommended']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_vocab[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ddnfCbQWRd5R"
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "  sentences = []\n",
    "  comments = df.comments\n",
    "\n",
    "  for comment in comments:\n",
    "    sentences.extend([sentence for sentence in comment.split('.')])\n",
    "  \n",
    "  print('yolo')\n",
    "  # print(sentences)\n",
    "  \n",
    "  coocs = {}\n",
    "  \n",
    "  count = 0\n",
    "  for center_word in cent_vocab:\n",
    "    count += 1\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "      if center_word in sentence:\n",
    "        words_in_sentence = word_tokenize(sentence)\n",
    "        words.extend([word for word in words_in_sentence if word in cont_vocab])\n",
    "    \n",
    "    center_word_dict = dict(Counter(words))\n",
    "    coocs[center_word] = center_word_dict\n",
    "    print(f'{count} out of 1000')\n",
    "    \n",
    "  # cent_dict = Counter(cent_list)\n",
    "  # cont_dict = Counter(cont_list)\n",
    "\n",
    "  \n",
    "  return coocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "  sentences = []\n",
    "  comments = df.comments\n",
    "\n",
    "  for comment in comments:\n",
    "    sentences.extend([sentence for sentence in comment.split('.')])\n",
    "  \n",
    "  print('yolo')\n",
    "  # print(sentences)\n",
    "\n",
    "  sentences_per_center_word = {center_word : Filter(sentences, [center_word]) for center_word in cent_vocab}\n",
    "  \n",
    "  print('swag')\n",
    "\n",
    "  words = []\n",
    "  coocs = {}\n",
    "\n",
    "  count = 0\n",
    "  count2 = 0\n",
    "  diff = 0\n",
    "  for center_word, sentences in sentences_per_center_word.items():\n",
    "    count += 1\n",
    "    start = pd.to_datetime('today')\n",
    "    # print(value)\n",
    "    count2 = 0\n",
    "    for sentence in sentences:\n",
    "      # print(sentence)\n",
    "      # break\n",
    "      count2 += 1\n",
    "      # if count2 % 10 == 0 : print(f'{count2} sentence out of {len(sentences)} sentences')\n",
    "      for word in word_tokenize(sentence):\n",
    "        # print(word)\n",
    "        # break\n",
    "        if word in cont_vocab:\n",
    "          words.append(word)   \n",
    "           \n",
    "        # break\n",
    "    #   if count == 10:\n",
    "    #     break\n",
    "    # print(center_word)\n",
    "    # print(words)\n",
    "    # print(Counter(words))\n",
    "    end = pd.to_datetime('today')\n",
    "    diff += (end-start).total_seconds()\n",
    "    print(f'{center_word} - - - - {(end-start).total_seconds()} seconds')\n",
    "    coocs[center_word] = dict(Counter(words))\n",
    "    if count % 20 == 0 : print(f'{count} center_word out of {len(sentences_per_center_word)} in {diff/60} minutes')\n",
    "    # break\n",
    "\n",
    "  # coocs = {key: dict(Counter([word for word in word_tokenize(value) if word in cont_vocab])) for key, value in sentences_per_center_word.items()}\n",
    "\n",
    "\n",
    "  #   coocs = {}\n",
    "\n",
    "  #   count = 0\n",
    "  #   for center_word in cent_vocab:\n",
    "  #     count += 1\n",
    "  #     words = []\n",
    "  #     for sentence in Filter(sentences, [center_word]):\n",
    "  #         words_in_sentence = word_tokenize(sentence)\n",
    "  #         words.extend([word for word in words_in_sentence if word in cont_vocab])\n",
    "      \n",
    "  #     center_word_dict = dict(Counter(words))\n",
    "  #     coocs[center_word] = center_word_dict\n",
    "  #     print(f'{count} out of 1000')\n",
    "    \n",
    "  # cent_dict = Counter(cent_list)\n",
    "  # cont_dict = Counter(cont_list)\n",
    "\n",
    "  print(diff/60)\n",
    "  return coocs \n",
    "\n",
    "def Filter(string, substr):\n",
    "    return [str for str in string if any(sub in str for sub in substr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTT_TOkaSoXL",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo\n",
      "swag\n",
      "place - - - - 60.242398 seconds\n",
      "apartment - - - - 52.117625 seconds\n",
      "location - - - - 39.550274 seconds\n",
      "stay - - - - 74.100316 seconds\n",
      "amsterdam - - - - 3.409948 seconds\n",
      "host - - - - 49.696653 seconds\n",
      "everything - - - - 22.927752 seconds\n",
      "city - - - - 31.665392 seconds\n",
      "room - - - - 49.465751 seconds\n",
      "time - - - - 24.239117 seconds\n",
      "house - - - - 21.789679 seconds\n",
      "area - - - - 19.967595 seconds\n",
      "home - - - - 17.957967 seconds\n",
      "très - - - - 15.315177 seconds\n",
      "center - - - - 23.213674 seconds\n",
      "restaurants - - - - 15.650124 seconds\n",
      "’ - - - - 23.51368 seconds\n",
      "station - - - - 11.308754 seconds\n",
      "minutes - - - - 12.695999 seconds\n",
      "walk - - - - 31.678272 seconds\n",
      "20 center_word out of 1000 in 10.008435783333335 minutes\n",
      "centre - - - - 14.234962 seconds\n",
      "tram - - - - 20.272539 seconds\n",
      "experience - - - - 10.402214 seconds\n",
      "space - - - - 9.856604 seconds\n",
      "thanks - - - - 2.402604 seconds\n",
      "à - - - - 11.783491 seconds\n",
      "hosts - - - - 8.258947 seconds\n",
      "neighborhood - - - - 9.974334 seconds\n",
      "clean - - - - 30.15446 seconds\n",
      "bien - - - - 7.538836 seconds\n",
      "perfect - - - - 24.231223 seconds\n",
      "communication - - - - 5.774769 seconds\n",
      "day - - - - 18.307113 seconds\n",
      "la - - - - 127.284547 seconds\n",
      "kind - - - - 8.876257 seconds\n",
      "distance - - - - 7.96572 seconds\n",
      "days - - - - 6.612313 seconds\n",
      "bed - - - - 19.771121 seconds\n",
      "trip - - - - 7.823077 seconds\n",
      "bathroom - - - - 7.573744 seconds\n",
      "40 center_word out of 1000 in 15.993417033333333 minutes\n",
      "et - - - - 129.404137 seconds\n",
      "night - - - - 11.324242 seconds\n",
      "e - - - - 471.400003 seconds\n",
      "sehr - - - - 5.465484 seconds\n",
      "people - - - - 6.423624 seconds\n",
      "thank - - - - 4.485199 seconds\n",
      "places - - - - 7.461441 seconds\n",
      "breakfast - - - - 7.549808 seconds\n",
      "lot - - - - 14.686727 seconds\n",
      "ist - - - - 30.409334 seconds\n",
      "street - - - - 7.838088 seconds\n",
      "boat - - - - 10.24759 seconds\n",
      "tips - - - - 6.603641 seconds\n",
      "der - - - - 32.365237 seconds\n",
      "visit - - - - 11.759581 seconds\n",
      "coffee - - - - 7.139893 seconds\n",
      "arrival - - - - 6.049818 seconds\n",
      "bus - - - - 12.018892 seconds\n",
      "need - - - - 21.55139 seconds\n",
      "muy - - - - 4.218955 seconds\n",
      "60 center_word out of 1000 in 29.466801766666663 minutes\n",
      "war - - - - 14.611013 seconds\n",
      "min - - - - 44.737908 seconds\n",
      "appartement - - - - 7.002273 seconds\n",
      "die - - - - 6.934418 seconds\n",
      "que - - - - 23.37855 seconds\n",
      "transport - - - - 10.726556 seconds\n",
      "airbnb - - - - 1.984978 seconds\n",
      "view - - - - 8.179093 seconds\n",
      "cozy - - - - 6.33865 seconds\n",
      "check - - - - 17.126242 seconds\n",
      "shops - - - - 7.513655 seconds\n",
      "kitchen - - - - 9.165525 seconds\n",
      "questions - - - - 5.039929 seconds\n",
      "way - - - - 29.627821 seconds\n",
      "bars - - - - 5.830683 seconds\n",
      "minute - - - - 19.722014 seconds\n",
      "helpful - - - - 17.396594 seconds\n",
      "family - - - - 5.195961 seconds\n",
      "studio - - - - 3.979686 seconds\n",
      "anyone - - - - 4.66359 seconds\n",
      "80 center_word out of 1000 in 33.61938741666666 minutes\n",
      "things - - - - 5.421064 seconds\n",
      "get - - - - 22.731749 seconds\n",
      "es - - - - 202.667815 seconds\n",
      "access - - - - 7.894027 seconds\n",
      "dans - - - - 5.043699 seconds\n",
      "mit - - - - 7.435779 seconds\n",
      "man - - - - 19.720196 seconds\n",
      "bit - - - - 10.473268 seconds\n",
      "weekend - - - - 5.730596 seconds\n",
      "town - - - - 6.88011 seconds\n",
      "le - - - - 211.091039 seconds\n",
      "lots - - - - 5.749916 seconds\n",
      "je - - - - 9.042562 seconds\n",
      "stairs - - - - 8.836978 seconds\n",
      "con - - - - 40.636469 seconds\n",
      "floor - - - - 5.210783 seconds\n",
      "food - - - - 5.002414 seconds\n",
      "information - - - - 5.046411 seconds\n",
      "morning - - - - 5.565732 seconds\n",
      "amenities - - - - 3.836895 seconds\n",
      "100 center_word out of 1000 in 43.51967911666665 minutes\n",
      "avons - - - - 3.753745 seconds\n",
      "neighbourhood - - - - 4.746857 seconds\n",
      "convenient - - - - 6.335769 seconds\n",
      "zu - - - - 6.291805 seconds\n",
      "du - - - - 21.228857 seconds\n",
      "person - - - - 8.137697 seconds\n",
      "wir - - - - 3.486099 seconds\n",
      "houseboat - - - - 3.557776 seconds\n",
      "recommendations - - - - 3.994581 seconds\n",
      "part - - - - 97.948571 seconds\n",
      "bedroom - - - - 6.907314 seconds\n",
      "beautiful - - - - 19.216397 seconds\n",
      "bikes - - - - 4.364659 seconds\n",
      "pictures - - - - 4.290168 seconds\n",
      "transportation - - - - 3.78384 seconds\n",
      "des - - - - 19.773187 seconds\n",
      "super - - - - 24.991473 seconds\n",
      "train - - - - 5.452327 seconds\n",
      "park - - - - 9.35854 seconds\n",
      "il - - - - 89.47317 seconds\n",
      "120 center_word out of 1000 in 49.30455964999999 minutes\n",
      "door - - - - 6.837331 seconds\n",
      "couple - - - - 5.421831 seconds\n",
      "supermarket - - - - 4.936145 seconds\n",
      "nights - - - - 3.739013 seconds\n",
      "séjour - - - - 3.834577 seconds\n",
      "bike - - - - 9.563303 seconds\n",
      "attractions - - - - 3.321981 seconds\n",
      "reservation - - - - 1.824098 seconds\n",
      "recommend - - - - 37.87112 seconds\n",
      "metro - - - - 4.770002 seconds\n",
      "è - - - - 17.696358 seconds\n",
      "calme - - - - 2.752636 seconds\n",
      "airport - - - - 4.068118 seconds\n",
      "situé - - - - 3.213408 seconds\n",
      "problem - - - - 5.418535 seconds\n",
      "help - - - - 25.196355 seconds\n",
      "hospitality - - - - 2.757635 seconds\n",
      "casa - - - - 3.735009 seconds\n",
      "friends - - - - 3.549474 seconds\n",
      "alles - - - - 2.462459 seconds\n",
      "140 center_word out of 1000 in 51.85404945 minutes\n",
      "beds - - - - 3.038872 seconds\n",
      "water - - - - 3.635165 seconds\n",
      "balcony - - - - 3.316165 seconds\n",
      "spot - - - - 5.303641 seconds\n",
      "museums - - - - 3.170521 seconds\n",
      "posting - - - - 1.02829 seconds\n",
      "photos - - - - 3.038767 seconds\n",
      "garden - - - - 3.44634 seconds\n",
      "book - - - - 6.512581 seconds\n",
      "propre - - - - 2.745421 seconds\n",
      "comfy - - - - 3.477698 seconds\n",
      "terrace - - - - 3.067804 seconds\n",
      "appartment - - - - 3.525724 seconds\n"
     ]
    }
   ],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "  coocdf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "  for index, row in coocdf.iterrows():\n",
    "    for word in cont_vocab:\n",
    "      try:\n",
    "        coocdf[word][index] = coocs[index][word]\n",
    "      except: \n",
    "        coocdf[word][index] = 0\n",
    "\n",
    "  return coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwAflxldSrbg"
   },
   "outputs": [],
   "source": [
    "coocdf = cooc_dict2df(coocs)\n",
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "  pmidf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "  N = 0\n",
    "  for index, row in coocdf.iterrows():\n",
    "    N += sum(row)\n",
    "\n",
    "  for index, row in coocdf.iterrows():\n",
    "    for word in cont_vocab:\n",
    "      try:\n",
    "        pmi = df[index][word] / (sum(df[word])/N / sum(row)/N)\n",
    "        if pmi == 0:\n",
    "          pmidf[word][index] = 0\n",
    "        else:\n",
    "          pmidf[word][index] = np.log([pmi])[0] \n",
    "#         print(pmidf[word][index])\n",
    "      except: \n",
    "        pmidf[word][index] = 0\n",
    "      \n",
    "  return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGftXjXRSuQw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pmidf = cooc2pmi(coocdf)\n",
    "pmidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name in cont_vocab:\n",
    "#     if len(pmidf[name][pmidf[name] > 0]) > 0:\n",
    "#         print(pmidf[name][pmidf[name] > 0 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "  top_words = sorted([df[word][center_word] for word in cont_vocab], reverse=True)[:N]\n",
    "  return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I038zG1Sw62"
   },
   "outputs": [],
   "source": [
    "topk(pmidf, 'location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfcm5-7b0HKO"
   },
   "source": [
    "# 3.b Ethical, social and legal implications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3uf-Qq4tYg"
   },
   "source": [
    "Local authorities in touristic hotspots like Amsterdam, NYC or Barcelona regulate the price of recreational apartments for rent to, among others, ensure that fair rent prices are kept for year-long residents. Consider your price recommender for hosts in Question 2c. Imagine that Airbnb recommends a new host to put the price of your flat at a price which is above the official regulations established by the local government. Upon inspection, you realize that the inflated price you have been recommended comes from many apartments in the area only being offered during an annual event which brings many tourists, and which causes prices to rise. \n",
    "\n",
    "In this context, critically reflect on the compliance of this recommender system with **one of the five actions** outlined in the **UK’s Data Ethics Framework**. You should prioritize the action that, in your opinion, is the weakest. Then, justify your choice by critically analyzing the three **key principles** outlined in the Framework, namely _transparency_, _accountability_ and _fairness_. Finally, you should propose and critically justify a solution that would improve the recommender system in at least one of these principles. You are strongly encouraged to follow a scholarly approach, e.g., with peer-reviewed references as support. \n",
    "\n",
    "Your report should be between 500 and 750 words long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6QJyuP6I1Ht"
   },
   "source": [
    "### Your answer here. No Python, only Markdown.\n",
    "\n",
    "Write your answer after the line.\n",
    "\n",
    "---\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3f75a622fdbe68ac4774c6ea619d86cc770141a8bef94a85fce2870eb7cb09bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
