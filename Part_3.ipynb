{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\c2086876\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['couldnt',\n",
       " 'wouldnt',\n",
       " 'neednt',\n",
       " 'youre',\n",
       " 'shes',\n",
       " 'mustnt',\n",
       " 'youd',\n",
       " 'wasnt',\n",
       " 'mightnt',\n",
       " 'doesnt',\n",
       " 'thatll',\n",
       " 'dont',\n",
       " 'its',\n",
       " 'youll',\n",
       " 'shouldnt',\n",
       " 'shouldve',\n",
       " 'havent',\n",
       " 'didnt',\n",
       " 'hasnt',\n",
       " 'werent',\n",
       " 'isnt',\n",
       " 'arent',\n",
       " 'shant',\n",
       " 'youve',\n",
       " 'wont',\n",
       " 'hadnt']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "'''We want to find the alternative forms of stopwords that have the \"'\" symbol in them \n",
    "in order to be able to add also to stopwords the word without this symbol'''\n",
    "\n",
    "pattern = r'\\w+\\'\\w+'\n",
    "\n",
    "new_stopwords = []\n",
    "for word in sw:\n",
    "    # If it finds a word that contains \"'\" it appends the word in new_stopwords list\n",
    "    if len(re.findall(pattern,word)) == 1:\n",
    "        new_stopwords.append(re.findall(pattern,word)[0].replace('\\'',''))\n",
    "new_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arent',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'couldnt',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'didnt',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doesnt',\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'dont',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hadnt',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'havent',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'isnt',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mightnt',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'mustnt',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'neednt',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shant',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'shouldnt',\n",
       " 'shouldve',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thatll',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'wasnt',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'werent',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wont',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'wouldnt',\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'youve'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# After checking those \"new\" words we add them to the stopwords variables named sw\n",
    "for word in new_stopwords:\n",
    "    sw.add(word)\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "basedir = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(basedir,'reviews.csv'))\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_id</th>\n      <th>id</th>\n      <th>date</th>\n      <th>reviewer_id</th>\n      <th>reviewer_name</th>\n      <th>comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2818</td>\n      <td>1191</td>\n      <td>2009-03-30</td>\n      <td>10952</td>\n      <td>Lam</td>\n      <td>Daniel is really cool. The place was nice and ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2818</td>\n      <td>1771</td>\n      <td>2009-04-24</td>\n      <td>12798</td>\n      <td>Alice</td>\n      <td>Daniel is the most amazing host! His place is ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2818</td>\n      <td>1989</td>\n      <td>2009-05-03</td>\n      <td>11869</td>\n      <td>Natalja</td>\n      <td>We had such a great time in Amsterdam. Daniel ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2818</td>\n      <td>2797</td>\n      <td>2009-05-18</td>\n      <td>14064</td>\n      <td>Enrique</td>\n      <td>Very professional operation. Room is very clea...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2818</td>\n      <td>3151</td>\n      <td>2009-05-25</td>\n      <td>17977</td>\n      <td>Sherwin</td>\n      <td>Daniel is highly recommended.  He provided all...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "def process_reviews(df):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    # Initialize 3 lists one for each column we will create\n",
    "    tokenized_col = []\n",
    "    tagged_col = []\n",
    "    lower_tagged_col = []\n",
    "\n",
    "\n",
    "    mylen = len(df)\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through the given dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # tokenize the words for the comments of a row\n",
    "        token = word_tokenize(row.comments)\n",
    "        # Append the tokenized words to the proper list\n",
    "        tokenized_col.append(token)\n",
    "        # Tag the tokenized words of the row and then append them to the proper list\n",
    "        tagged_col.append(pos_tag(token))\n",
    "        # lower_tagged.append(list(set(pos_tag([item.lower() for item in token]))))\n",
    "        # Make the tagged words lowercased and then if they are not stopwords append them to the lower_tagged_col list\n",
    "        lower_tagged_col.append(pos_tag([item.lower() for item in token if item.lower() not in sw]))\n",
    "        count += 1\n",
    "\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} out of {mylen}')\n",
    "\n",
    "    # Set as values of the 3 new columns the proper list we created for each one\n",
    "    df['tokenized'] = tokenized_col\n",
    "    df['tagged'] = tagged_col\n",
    "    df['lower_tagged'] = lower_tagged_col\n",
    "\n",
    "    # Return the modified dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rGYB8gx5Qq-P",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = process_reviews(df)\n",
    "df = process_reviews(df[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "  cent_list, cont_list = [], []\n",
    "\n",
    "  for review in df.lower_tagged:\n",
    "    cent_list.extend([word for word in [list_of_words[0] for list_of_words in review if list_of_words[1][0] == 'N']])\n",
    "    cont_list.extend([word for word in [list_of_words[0] for list_of_words in review if (list_of_words[1][0] == 'J') or (list_of_words[1][0] == 'V')]])\n",
    "    \n",
    "  cent_dict = Counter(cent_list)\n",
    "  cont_dict = Counter(cont_list)\n",
    "\n",
    "  cent_vocab = [key for key, value in sorted(cent_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "  cont_vocab = [key for key, value in sorted(cont_dict.items(), key=lambda item: item[1], reverse=True)][:1000]\n",
    "\n",
    "  return cent_vocab, cont_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F_R5l4IVSk9-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['daniel', 'room', 'place', 'host', 'location']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "cent_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['great', 'clean', 'alex', 'nice', 'comfortable']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "cont_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "samewords = [name for name in cent_vocab if name in cont_vocab]\n",
    "len(samewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['daniel',\n",
       " 'place',\n",
       " 'location',\n",
       " 'amsterdam',\n",
       " 'stay',\n",
       " 'apartment',\n",
       " 'bathroom',\n",
       " 'alex',\n",
       " 'clean',\n",
       " 'bus',\n",
       " 'distance',\n",
       " 'neighborhood',\n",
       " 'maps',\n",
       " 'bed',\n",
       " 'experience',\n",
       " 'get',\n",
       " 'tram',\n",
       " 'que',\n",
       " 'walk',\n",
       " 'airbnb',\n",
       " 'transport',\n",
       " 'thank',\n",
       " 'bit',\n",
       " 'visit',\n",
       " 'très',\n",
       " 'bien',\n",
       " 'tips',\n",
       " 'arrival',\n",
       " 'muy',\n",
       " 'helpful',\n",
       " 'lot',\n",
       " 'è',\n",
       " 'places',\n",
       " 'la',\n",
       " 'cozy',\n",
       " 'beds',\n",
       " 'help',\n",
       " 'perfect',\n",
       " 'towels',\n",
       " 'e',\n",
       " 'tea',\n",
       " 'daniels',\n",
       " 'need',\n",
       " 'die',\n",
       " 'use',\n",
       " 'convenient',\n",
       " 'friend',\n",
       " 'shower',\n",
       " 'minute',\n",
       " 'sehr',\n",
       " 'check',\n",
       " 'map',\n",
       " 'tidy',\n",
       " 'bike',\n",
       " 'window',\n",
       " 'pictures',\n",
       " 'ist',\n",
       " 'zu',\n",
       " 'para',\n",
       " 'stops',\n",
       " 'nice',\n",
       " 'es',\n",
       " 'lo',\n",
       " 'staying',\n",
       " 'guest',\n",
       " 'spotless',\n",
       " 'garden',\n",
       " 'wifi',\n",
       " 'fun',\n",
       " 'et',\n",
       " 'séjour',\n",
       " 'photos',\n",
       " '’',\n",
       " 'por',\n",
       " 'amazing',\n",
       " 'stop',\n",
       " 'work',\n",
       " 'travel',\n",
       " 'lots',\n",
       " 'tourist',\n",
       " 'le',\n",
       " 'stairs',\n",
       " 'cafes',\n",
       " 'pleasant',\n",
       " 'appartment',\n",
       " 'beautiful',\n",
       " 'luggage',\n",
       " 'il',\n",
       " 'ce',\n",
       " 'drink',\n",
       " 'welcoming',\n",
       " 'feel',\n",
       " 'restaurant',\n",
       " 'enjoy',\n",
       " 'dutch',\n",
       " 'living',\n",
       " 'recommend',\n",
       " 'ride',\n",
       " 'offer',\n",
       " 'keys',\n",
       " 'son',\n",
       " 'contact',\n",
       " 'te',\n",
       " 'chambre',\n",
       " 'check-in',\n",
       " 'comfy',\n",
       " 'guide',\n",
       " 'hope',\n",
       " 'let',\n",
       " 'couple',\n",
       " 'plan',\n",
       " 'answer',\n",
       " 'neat',\n",
       " 'boyfriend',\n",
       " 'talk',\n",
       " 'welcome',\n",
       " 'und',\n",
       " 'für',\n",
       " 'ein',\n",
       " 'si',\n",
       " 'ideal',\n",
       " 'trams',\n",
       " 'neighbourhood',\n",
       " 'plenty',\n",
       " 'fine',\n",
       " 'friends',\n",
       " 'park',\n",
       " 'make',\n",
       " 'par',\n",
       " 'dans',\n",
       " 'cosy',\n",
       " 'meet',\n",
       " 'wir',\n",
       " 'zimmer',\n",
       " 'felt',\n",
       " 'excelente',\n",
       " 'go',\n",
       " 'end',\n",
       " 'excellent',\n",
       " 'cold',\n",
       " 'front',\n",
       " 'kettle',\n",
       " 'kindness',\n",
       " 'reach',\n",
       " 'schedule',\n",
       " 'tour',\n",
       " 'future',\n",
       " 'evening',\n",
       " 'super',\n",
       " 'laundry',\n",
       " 'professional',\n",
       " 'peaceful',\n",
       " 'appartement',\n",
       " 'recommande',\n",
       " 'light',\n",
       " 'internet',\n",
       " 'know',\n",
       " 'shops',\n",
       " 'okay',\n",
       " 'rest',\n",
       " 'er',\n",
       " 'mejor',\n",
       " 'euro',\n",
       " 'du',\n",
       " 'text',\n",
       " 'rembrandt',\n",
       " 'centraal',\n",
       " 'comfortable',\n",
       " 'flower',\n",
       " 'café',\n",
       " 'outside',\n",
       " 'downstairs',\n",
       " 'bicycles',\n",
       " 'charming',\n",
       " 'wonderful',\n",
       " 'relaxing',\n",
       " 'set',\n",
       " 'accommodating',\n",
       " 'minut',\n",
       " 'europe',\n",
       " 'quiet',\n",
       " 'star',\n",
       " 'kitchen',\n",
       " 'provide',\n",
       " 'info',\n",
       " 'easy',\n",
       " 'advise',\n",
       " 'immaculate',\n",
       " 'wohnung',\n",
       " 'take',\n",
       " 'brilliant',\n",
       " '@',\n",
       " 'alles',\n",
       " 'guidebooks',\n",
       " 'übernachten',\n",
       " 'das',\n",
       " 'sauber',\n",
       " 'warm',\n",
       " 'sightseeing',\n",
       " 'email',\n",
       " '....',\n",
       " 'til',\n",
       " 'dam',\n",
       " 'en',\n",
       " 'pour',\n",
       " 'se',\n",
       " 'touch',\n",
       " 'cafe',\n",
       " 'non',\n",
       " 'wi-fi',\n",
       " 'mind',\n",
       " 'put',\n",
       " 'un',\n",
       " 'pulita',\n",
       " 'explain',\n",
       " 'signal',\n",
       " 'walking',\n",
       " 'fue',\n",
       " 'good',\n",
       " 'downside',\n",
       " 'right',\n",
       " 'find',\n",
       " 'sleep',\n",
       " 'wait',\n",
       " 'así',\n",
       " 'please',\n",
       " 'return',\n",
       " 'cleaner',\n",
       " 'travelling',\n",
       " 'friendliness',\n",
       " 'think',\n",
       " 'shampoo',\n",
       " 'bright',\n",
       " 'sweet',\n",
       " 'ams',\n",
       " 'awesome',\n",
       " 'forgot',\n",
       " 'wish',\n",
       " 'sur',\n",
       " 'daniel´s',\n",
       " 'les',\n",
       " 'beaucoup',\n",
       " 'zuvorkommend']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "samewords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ddnfCbQWRd5R"
   },
   "outputs": [],
   "source": [
    "# def get_coocs(df, cent_vocab, cont_vocab):\n",
    "#   sentences = []\n",
    "#   comments = df.comments\n",
    "\n",
    "#   for comment in comments:\n",
    "#     sentences.extend([sentence for sentence in comment.split('.')])\n",
    "  \n",
    "#   print('yolo')\n",
    "#   # print(sentences)\n",
    "  \n",
    "#   coocs = {}\n",
    "  \n",
    "#   count = 0\n",
    "#   for center_word in cent_vocab:\n",
    "#     count += 1\n",
    "#     words = []\n",
    "#     for sentence in sentences:\n",
    "#       if center_word in sentence:\n",
    "#         words_in_sentence = word_tokenize(sentence)\n",
    "#         words.extend([word for word in words_in_sentence if word in cont_vocab])\n",
    "    \n",
    "#     center_word_dict = dict(Counter(words))\n",
    "#     coocs[center_word] = center_word_dict\n",
    "#     print(f'{count} out of 1000')\n",
    "    \n",
    "#   # cent_dict = Counter(cent_list)\n",
    "#   # cont_dict = Counter(cont_list)\n",
    "\n",
    "  \n",
    "#   return coocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "        cent_vocab: The dataframe we want to modify.\n",
    "        cont_vocab: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "    sentences = []\n",
    "    comments = df.comments\n",
    "\n",
    "    for comment in comments:\n",
    "        sentences.extend([sentence for sentence in comment.split('.')])\n",
    "  \n",
    "    print('yolo')\n",
    "  \n",
    "    sentences_per_center_word = {center_word : Filter(sentences, center_word) for center_word in cent_vocab}\n",
    "\n",
    "    print('swag')\n",
    "\n",
    "    \n",
    "    coocs = {}\n",
    "\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    diff = 0\n",
    "    for center_word, sentences in sentences_per_center_word.items():\n",
    "        words = []\n",
    "        count += 1\n",
    "        start = pd.to_datetime('today')\n",
    "        count2 = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            count2 += 1\n",
    "            words_of_sentence = [word for word in word_tokenize(sentence) if word in cont_vocab]\n",
    "            if len(words_of_sentence) > 0: words.extend(words_of_sentence)\n",
    "                \n",
    "        end = pd.to_datetime('today')\n",
    "        diff += (end-start).total_seconds()\n",
    "        print(f'{center_word} - - - - {(end-start).total_seconds()} seconds')\n",
    "        coocs[center_word] = dict(Counter(words))\n",
    "        if count % 20 == 0 : print(f'{count} center_word out of {len(sentences_per_center_word)} in {diff/60} minutes')\n",
    "\n",
    "            \n",
    "    print(diff/60)\n",
    "    return coocs \n",
    "\n",
    "def Filter(sentences, center_word):\n",
    "    sentences_for_center_word = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if center_word in sentence.split():\n",
    "            sentences_for_center_word.append(sentence)\n",
    "    print(f'{center_word} --- {len(sentences_for_center_word)}')\n",
    "    return sentences_for_center_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "97 seconds\n",
      "recommande - - - - 0.002033 seconds\n",
      "complaints - - - - 0.000959 seconds\n",
      "facilities - - - - 0.002034 seconds\n",
      "tutto - - - - 0.000992 seconds\n",
      "assistance - - - - 0.000998 seconds\n",
      "light - - - - 0.002962 seconds\n",
      "temperature - - - - 0.002047 seconds\n",
      "420 center_word out of 1000 in 0.0450908333333332 minutes\n",
      "internet - - - - 0.001987 seconds\n",
      "know - - - - 0.014987 seconds\n",
      "shops - - - - 0.002011 seconds\n",
      "okay - - - - 0.001987 seconds\n",
      "name - - - - 0.000997 seconds\n",
      "security - - - - 0.000999 seconds\n",
      "hallway - - - - 0.0 seconds\n",
      "rest - - - - 0.002956 seconds\n",
      "er - - - - 0.002958 seconds\n",
      "aufenthalt - - - - 0.0 seconds\n",
      "gut - - - - 0.0 seconds\n",
      "hätte - - - - 0.0 seconds\n",
      "picture - - - - 0.000956 seconds\n",
      "experiencia - - - - 0.001035 seconds\n",
      "mejor - - - - 0.000994 seconds\n",
      "cuida - - - - 0.000996 seconds\n",
      "detalle - - - - 0.0 seconds\n",
      "persona - - - - 0.002004 seconds\n",
      "tipo - - - - 0.000991 seconds\n",
      "gracias - - - - 0.002007 seconds\n",
      "440 center_word out of 1000 in 0.04572191666666654 minutes\n",
      "euro - - - - 0.00401 seconds\n",
      "airport - - - - 0.000965 seconds\n",
      "message - - - - 0.000998 seconds\n",
      "perfekt - - - - 0.001993 seconds\n",
      "eingerichtet - - - - 0.000998 seconds\n",
      "prompt - - - - 0.00202 seconds\n",
      "accueillant - - - - 0.0 seconds\n",
      "delle - - - - 0.001994 seconds\n",
      "knowledge - - - - 0.000999 seconds\n",
      "gentile - - - - 0.000997 seconds\n",
      "caso - - - - 0.000996 seconds\n",
      "bellissima - - - - 0.001969 seconds\n",
      "parfait - - - - 0.00103 seconds\n",
      "situé - - - - 0.0 seconds\n",
      "du - - - - 0.002992 seconds\n",
      "immer - - - - 0.000967 seconds\n",
      "text - - - - 0.004011 seconds\n",
      "cute - - - - 0.000997 seconds\n",
      "haben - - - - 0.000997 seconds\n",
      "dark - - - - 0.001996 seconds\n",
      "460 center_word out of 1000 in 0.04623739999999988 minutes\n",
      "habitacion - - - - 0.001025 seconds\n",
      "på - - - - 0.001 seconds\n",
      "au - - - - 0.000996 seconds\n",
      "pas - - - - 0.0 seconds\n",
      "estadía - - - - 0.0 seconds\n",
      "lugar - - - - 0.001995 seconds\n",
      "bnb - - - - 0.001963 seconds\n",
      "expectation - - - - 0.003988 seconds\n",
      "request - - - - 0.001995 seconds\n",
      "limpio - - - - 0.0 seconds\n",
      "rembrandt - - - - 0.000997 seconds\n",
      "porque - - - - 0.001995 seconds\n",
      "ville - - - - 0.001995 seconds\n",
      "anfitrión - - - - 0.000998 seconds\n",
      "travelers - - - - 0.000997 seconds\n",
      "amenities - - - - 0.002996 seconds\n",
      "centraal - - - - 0.001994 seconds\n",
      "comfortable - - - - 0.034905 seconds\n",
      "traffic - - - - 0.000998 seconds\n",
      "é - - - - 0.001994 seconds\n",
      "480 center_word out of 1000 in 0.047284583333333206 minutes\n",
      "flower - - - - 0.001995 seconds\n",
      "control - - - - 0.000997 seconds\n",
      "café - - - - 0.003991 seconds\n",
      "placé - - - - 0.0 seconds\n",
      "steps - - - - 0.001994 seconds\n",
      "outside - - - - 0.007979 seconds\n",
      "doors - - - - 0.000997 seconds\n",
      "pequeño - - - - 0.005034 seconds\n",
      "ubicado - - - - 0.0 seconds\n",
      "underneath - - - - 0.001028 seconds\n",
      "downstairs - - - - 0.001998 seconds\n",
      "girlfriend - - - - 0.00103 seconds\n",
      "pay - - - - 0.001995 seconds\n",
      "glasses - - - - 0.000995 seconds\n",
      "buena - - - - 0.002002 seconds\n",
      "conor - - - - 0.0 seconds\n",
      "advices - - - - 0.0 seconds\n",
      "tools - - - - 0.001956 seconds\n",
      "bicycles - - - - 0.003995 seconds\n",
      "charming - - - - 0.0 seconds\n",
      "500 center_word out of 1000 in 0.04791768333333321 minutes\n",
      "amount - - - - 0.0 seconds\n",
      "praise - - - - 0.000959 seconds\n",
      "wonderful - - - - 0.00798 seconds\n",
      "greet - - - - 0.001992 seconds\n",
      "terrific - - - - 0.000997 seconds\n",
      "partner - - - - 0.0 seconds\n",
      "stones - - - - 0.001994 seconds\n",
      "groceries - - - - 0.0 seconds\n",
      "residence - - - - 0.0 seconds\n",
      "page - - - - 0.0 seconds\n",
      "relaxing - - - - 0.003989 seconds\n",
      "years - - - - 0.000998 seconds\n",
      "holland - - - - 0.000997 seconds\n",
      "cleanliness - - - - 0.0 seconds\n",
      "extras - - - - 0.0 seconds\n",
      "set - - - - 0.005985 seconds\n",
      "accomodations - - - - 0.000997 seconds\n",
      "metro - - - - 0.000997 seconds\n",
      "cares - - - - 0.0 seconds\n",
      "accommodating - - - - 0.005984 seconds\n",
      "520 center_word out of 1000 in 0.04848216666666653 minutes\n",
      "requests - - - - 0.000998 seconds\n",
      "lines - - - - 0.0 seconds\n",
      "wardrobe - - - - 0.0 seconds\n",
      "discover - - - - 0.001998 seconds\n",
      "minut - - - - 0.002991 seconds\n",
      "clothes - - - - 0.000996 seconds\n",
      "europe - - - - 0.000997 seconds\n",
      "quiet - - - - 0.01895 seconds\n",
      "hair - - - - 0.001995 seconds\n",
      "star - - - - 0.001994 seconds\n",
      "travels - - - - 0.0 seconds\n",
      "spending - - - - 0.002992 seconds\n",
      "traveller - - - - 0.000998 seconds\n",
      "plane - - - - 0.000997 seconds\n",
      "wealth - - - - 0.000982 seconds\n",
      "ways - - - - 0.001994 seconds\n",
      "kitchen - - - - 0.00698 seconds\n",
      "desk - - - - 0.001995 seconds\n",
      "flowers - - - - 0.000998 seconds\n",
      "tickets - - - - 0.000998 seconds\n",
      "540 center_word out of 1000 in 0.04931304999999986 minutes\n",
      "provide - - - - 0.008973 seconds\n",
      "fan - - - - 0.001995 seconds\n",
      "thai - - - - 0.0 seconds\n",
      "stores - - - - 0.000997 seconds\n",
      "family - - - - 0.000997 seconds\n",
      "birds - - - - 0.000999 seconds\n",
      "towel - - - - 0.000995 seconds\n",
      "standard - - - - 0.000998 seconds\n",
      "info - - - - 0.003993 seconds\n",
      "definition - - - - 0.000997 seconds\n",
      "music - - - - 0.000997 seconds\n",
      "playing - - - - 0.001994 seconds\n",
      "mins - - - - 0.0 seconds\n",
      "magazines - - - - 0.000998 seconds\n",
      "describe - - - - 0.0 seconds\n",
      "suitcases - - - - 0.0 seconds\n",
      "convenience - - - - 0.000998 seconds\n",
      "parts - - - - 0.000998 seconds\n",
      "easy - - - - 0.020945 seconds\n",
      "fait - - - - 0.000997 seconds\n",
      "560 center_word out of 1000 in 0.050127566666666526 minutes\n",
      "desservi - - - - 0.000996 seconds\n",
      "mon - - - - 0.0 seconds\n",
      "advise - - - - 0.001995 seconds\n",
      "votre - - - - 0.000998 seconds\n",
      "disposition - - - - 0.000997 seconds\n",
      "besoin - - - - 0.001995 seconds\n",
      "calme - - - - 0.0 seconds\n",
      "qui - - - - 0.001994 seconds\n",
      "visiter - - - - 0.000998 seconds\n",
      "hall - - - - 0.001995 seconds\n",
      "immaculate - - - - 0.000997 seconds\n",
      "wohnung - - - - 0.0 seconds\n",
      "hilfsbereit - - - - 0.0 seconds\n",
      "bin - - - - 0.000998 seconds\n",
      "perfetto - - - - 0.0 seconds\n",
      "demander - - - - 0.000997 seconds\n",
      "mieux - - - - 0.000997 seconds\n",
      "take - - - - 0.002993 seconds\n",
      "functional - - - - 0.0 seconds\n",
      "arrange - - - - 0.000997 seconds\n",
      "580 center_word out of 1000 in 0.050460016666666524 minutes\n",
      "answers - - - - 0.001996 seconds\n",
      "markets - - - - 0.001993 seconds\n",
      "playground - - - - 0.0 seconds\n",
      "brilliant - - - - 0.001999 seconds\n",
      "cultures - - - - 0.000997 seconds\n",
      "life - - - - 0.000997 seconds\n",
      "art - - - - 0.001995 seconds\n",
      "films - - - - 0.000997 seconds\n",
      "call - - - - 0.000997 seconds\n",
      "doorbell - - - - 0.002994 seconds\n",
      "@ - - - - 0.001003 seconds\n",
      "offers - - - - 0.0 seconds\n",
      "alles - - - - 0.0 seconds\n",
      "habe - - - - 0.000981 seconds\n",
      "nichts - - - - 0.0 seconds\n",
      "case - - - - 0.0 seconds\n",
      "fee - - - - 0.000998 seconds\n",
      "sido - - - - 0.000996 seconds\n",
      "encanto - - - - 0.0 seconds\n",
      "momento - - - - 0.000998 seconds\n",
      "600 center_word out of 1000 in 0.050792366666666526 minutes\n",
      "necesaria - - - - 0.000998 seconds\n",
      "sea - - - - 0.0 seconds\n",
      "atento - - - - 0.00398 seconds\n",
      "cosa - - - - 0.001003 seconds\n",
      "sus - - - - 0.001989 seconds\n",
      "ciudad - - - - 0.000997 seconds\n",
      "fotos - - - - 0.001995 seconds\n",
      "llevas - - - - 0.000998 seconds\n",
      "vs - - - - 0.000998 seconds\n",
      "aquí - - - - 0.000996 seconds\n",
      "cup - - - - 0.002 seconds\n",
      "guidebooks - - - - 0.003987 seconds\n",
      "ort - - - - 0.000997 seconds\n",
      "übernachten - - - - 0.0 seconds\n",
      "fuß - - - - 0.0 seconds\n",
      "innenstadt - - - - 0.0 seconds\n",
      "gibt - - - - 0.000997 seconds\n",
      "gute - - - - 0.000997 seconds\n",
      "das - - - - 0.000998 seconds\n",
      "gast - - - - 0.0 seconds\n",
      "620 center_word out of 1000 in 0.051191199999999853 minutes\n",
      "peace - - - - 0.000997 seconds\n",
      "ok - - - - 0.000998 seconds\n",
      "reply - - - - 0.003995 seconds\n",
      "l'aise - - - - 0.003989 seconds\n",
      "shoes - - - - 0.000991 seconds\n",
      "glass - - - - 0.000998 seconds\n",
      "hire - - - - 0.000996 seconds\n",
      "minuti - - - - 0.000998 seconds\n",
      "qualsiasi - - - - 0.000997 seconds\n",
      "run - - - - 0.001993 seconds\n",
      "estremamente - - - - 0.000998 seconds\n",
      "buona - - - - 0.0 seconds\n",
      "della - - - - 0.000996 seconds\n",
      "dire - - - - 0.003977 seconds\n",
      "quanto - - - - 0.001985 seconds\n",
      "esperienza - - - - 0.000998 seconds\n",
      "mezzi - - - - 0.000997 seconds\n",
      "quarters - - - - 0.000998 seconds\n",
      "gentil - - - - 0.000997 seconds\n",
      "loin - - - - 0.001994 seconds\n",
      "640 center_word out of 1000 in 0.05170606666666652 minutes\n",
      "rapide - - - - 0.001995 seconds\n",
      "expérience - - - - 0.00399 seconds\n",
      "bzw - - - - 0.000993 seconds\n",
      "angenehm - - - - 0.0 seconds\n",
      "fragen - - - - 0.0 seconds\n",
      "schön - - - - 0.000998 seconds\n",
      "sauber - - - - 0.000997 seconds\n",
      "warm - - - - 0.003989 seconds\n",
      "sightseeing - - - - 0.002998 seconds\n",
      "email - - - - 0.001982 seconds\n",
      "facility - - - - 0.000994 seconds\n",
      "treat - - - - 0.000997 seconds\n",
      "wohl - - - - 0.000998 seconds\n",
      "gefühlt - - - - 0.0 seconds\n",
      "von - - - - 0.000997 seconds\n",
      "ruhig - - - - 0.0 seconds\n",
      "kommen - - - - 0.0 seconds\n",
      ".... - - - - 0.0 seconds\n",
      "beat - - - - 0.000998 seconds\n",
      "siamo - - - - 0.004987 seconds\n",
      "660 center_word out of 1000 in 0.05217128333333317 minutes\n",
      "avuto - - - - 0.000998 seconds\n",
      "nella - - - - 0.000998 seconds\n",
      "pubblici - - - - 0.0 seconds\n",
      "traveler - - - - 0.000997 seconds\n",
      "beginning - - - - 0.000997 seconds\n",
      "overview - - - - 0.000999 seconds\n",
      "design - - - - 0.000996 seconds\n",
      "property - - - - 0.002992 seconds\n",
      "zentrum - - - - 0.0 seconds\n",
      "tage - - - - 0.0 seconds\n",
      "als - - - - 0.001001 seconds\n",
      "atencion - - - - 0.000994 seconds\n",
      "interest - - - - 0.000997 seconds\n",
      "og - - - - 0.000998 seconds\n",
      "til - - - - 0.000997 seconds\n",
      "jeg - - - - 0.0 seconds\n",
      "bene - - - - 0.0 seconds\n",
      "propreté - - - - 0.000997 seconds\n",
      "bons - - - - 0.000997 seconds\n",
      "conseils - - - - 0.0 seconds\n",
      "680 center_word out of 1000 in 0.05243724999999984 minutes\n",
      "erkunden - - - - 0.0 seconds\n",
      "cleanthiness - - - - 0.000998 seconds\n",
      "secure - - - - 0.001003 seconds\n",
      "cada - - - - 0.001001 seconds\n",
      "mucha - - - - 0.001966 seconds\n",
      "llegar - - - - 0.001994 seconds\n",
      "barrio - - - - 0.000998 seconds\n",
      "tranquilo - - - - 0.0 seconds\n",
      "speed - - - - 0.000997 seconds\n",
      "dam - - - - 0.000997 seconds\n",
      "perfecta - - - - 0.0 seconds\n",
      "en - - - - 0.01496 seconds\n",
      "checkout - - - - 0.000997 seconds\n",
      "pour - - - - 0.002993 seconds\n",
      "tramway - - - - 0.003023 seconds\n",
      "locations - - - - 0.002004 seconds\n",
      "ticket - - - - 0.000983 seconds\n",
      "australia - - - - 0.0 seconds\n",
      "recomendamos - - - - 0.0 seconds\n",
      "occasions - - - - 0.000997 seconds\n",
      "700 center_word out of 1000 in 0.0530357666666665 minutes\n",
      "rain - - - - 0.000997 seconds\n",
      "locality - - - - 0.000998 seconds\n",
      "comunicado - - - - 0.000997 seconds\n",
      "d'accéder - - - - 0.0 seconds\n",
      "recibió - - - - 0.000998 seconds\n",
      "tiene - - - - 0.000997 seconds\n",
      "se - - - - 0.005985 seconds\n",
      "dispuesto - - - - 0.000996 seconds\n",
      "wise - - - - 0.0 seconds\n",
      "touch - - - - 0.001995 seconds\n",
      "superhost - - - - 0.000998 seconds\n",
      "siempre - - - - 0.001994 seconds\n",
      "model - - - - 0.000998 seconds\n",
      "world - - - - 0.0 seconds\n",
      "enter - - - - 0.000998 seconds\n",
      "quieter - - - - 0.001047 seconds\n",
      "helpfulness - - - - 0.000999 seconds\n",
      "queries - - - - 0.000992 seconds\n",
      "process - - - - 0.000997 seconds\n",
      "insight - - - - 0.000998 seconds\n",
      "720 center_word out of 1000 in 0.05343549999999983 minutes\n",
      "cafe - - - - 0.003989 seconds\n",
      "site - - - - 0.0 seconds\n",
      "guess - - - - 0.0 seconds\n",
      "non - - - - 0.003042 seconds\n",
      "problema - - - - 0.002982 seconds\n",
      "grande - - - - 0.001996 seconds\n",
      "sotto - - - - 0.000995 seconds\n",
      "notte - - - - 0.001995 seconds\n",
      "sia - - - - 0.001995 seconds\n",
      "fare - - - - 0.002035 seconds\n",
      "wi-fi - - - - 0.000968 seconds\n",
      "tutti - - - - 0.000961 seconds\n",
      "l - - - - 0.00101 seconds\n",
      "complain - - - - 0.000982 seconds\n",
      "quarto - - - - 0.000998 seconds\n",
      "com - - - - 0.0 seconds\n",
      "em - - - - 0.0 seconds\n",
      "dos - - - - 0.000997 seconds\n",
      "locks - - - - 0.000998 seconds\n",
      "entrance - - - - 0.000997 seconds\n",
      "740 center_word out of 1000 in 0.05388449999999982 minutes\n",
      "ground - - - - 0.001995 seconds\n",
      "mind - - - - 0.001995 seconds\n",
      "put - - - - 0.00399 seconds\n",
      "recommander - - - - 0.0 seconds\n",
      "un - - - - 0.010969 seconds\n",
      "consigliato - - - - 0.0 seconds\n",
      "esattamente - - - - 0.0 seconds\n",
      "pulita - - - - 0.000997 seconds\n",
      "explain - - - - 0.001995 seconds\n",
      "response - - - - 0.001992 seconds\n",
      "rules - - - - 0.000997 seconds\n",
      "signal - - - - 0.002992 seconds\n",
      "walking - - - - 0.013964 seconds\n",
      "discount - - - - 0.000996 seconds\n",
      "breakfasts - - - - 0.001995 seconds\n",
      "opportunity - - - - 0.000997 seconds\n",
      "interaction - - - - 0.0 seconds\n",
      "rate - - - - 0.000998 seconds\n",
      "skills - - - - 0.000997 seconds\n",
      "tres - - - - 0.000997 seconds\n",
      "760 center_word out of 1000 in 0.05469893333333315 minutes\n",
      "millions - - - - 0.000998 seconds\n",
      "side - - - - 0.000996 seconds\n",
      "streets - - - - 0.000998 seconds\n",
      "fue - - - - 0.00399 seconds\n",
      "sites - - - - 0.001028 seconds\n",
      "budget - - - - 0.000966 seconds\n",
      "christmas - - - - 0.0 seconds\n",
      "quarter - - - - 0.0 seconds\n",
      "wife - - - - 0.002029 seconds\n",
      "good - - - - 0.027892 seconds\n",
      "concern - - - - 0.001029 seconds\n",
      "seat - - - - 0.0 seconds\n",
      "zoo - - - - 0.0 seconds\n",
      "vivement - - - - 0.0 seconds\n",
      "asset - - - - 0.001029 seconds\n",
      "cups - - - - 0.001997 seconds\n",
      "shopping - - - - 0.000997 seconds\n",
      "nightlife - - - - 0.0 seconds\n",
      "downside - - - - 0.002992 seconds\n",
      "right - - - - 0.008976 seconds\n",
      "780 center_word out of 1000 in 0.05563088333333314 minutes\n",
      "hold - - - - 0.001996 seconds\n",
      "find - - - - 0.007979 seconds\n",
      "placée - - - - 0.0 seconds\n",
      "merci - - - - 0.0 seconds\n",
      "cama - - - - 0.001975 seconds\n",
      "sleep - - - - 0.003019 seconds\n",
      "meeting - - - - 0.000968 seconds\n",
      "wait - - - - 0.002991 seconds\n",
      "todos - - - - 0.001995 seconds\n",
      "duda - - - - 0.001994 seconds\n",
      "departure - - - - 0.000998 seconds\n",
      "communicate - - - - 0.000997 seconds\n",
      "couples - - - - 0.0 seconds\n",
      "empfehlen - - - - 0.0 seconds\n",
      "hay - - - - 0.000998 seconds\n",
      "mirror - - - - 0.001 seconds\n",
      "somebody - - - - 0.002016 seconds\n",
      "fault - - - - 0.000997 seconds\n",
      "ya - - - - 0.001994 seconds\n",
      "había - - - - 0.001995 seconds\n",
      "800 center_word out of 1000 in 0.05619608333333313 minutes\n",
      "ver - - - - 0.002991 seconds\n",
      "así - - - - 0.001994 seconds\n",
      "please - - - - 0.002021 seconds\n",
      "supplies - - - - 0.001005 seconds\n",
      "operation - - - - 0.0 seconds\n",
      "necessities - - - - 0.001 seconds\n",
      "well - - - - 0.041851 seconds\n",
      "compass - - - - 0.0 seconds\n",
      "noises - - - - 0.0 seconds\n",
      "can´t - - - - 0.0 seconds\n",
      "he´s - - - - 0.000997 seconds\n",
      "return - - - - 0.002993 seconds\n",
      "couldn.t - - - - 0.000994 seconds\n",
      "accomation - - - - 0.000997 seconds\n",
      "convienient - - - - 0.0 seconds\n",
      "problems - - - - 0.000997 seconds\n",
      "correspondence - - - - 0.0 seconds\n",
      "cleaner - - - - 0.000996 seconds\n",
      "safer - - - - 0.0 seconds\n",
      "companion - - - - 0.000998 seconds\n",
      "820 center_word out of 1000 in 0.057193316666666466 minutes\n",
      "command - - - - 0.000997 seconds\n",
      "nuances - - - - 0.0 seconds\n",
      "owner - - - - 0.000998 seconds\n",
      "island - - - - 0.0 seconds\n",
      "holidays - - - - 0.000997 seconds\n",
      "dainel - - - - 0.0 seconds\n",
      "spick - - - - 0.000998 seconds\n",
      "span - - - - 0.000997 seconds\n",
      "radar - - - - 0.000997 seconds\n",
      "personality - - - - 0.0 seconds\n",
      "cake.we - - - - 0.0 seconds\n",
      "again.thanks - - - - 0.0 seconds\n",
      "danile - - - - 0.0 seconds\n",
      "zen - - - - 0.0 seconds\n",
      "hostels - - - - 0.002001 seconds\n",
      "p.s - - - - 0.0 seconds\n",
      "videos - - - - 0.000997 seconds\n",
      "pricey - - - - 0.000998 seconds\n",
      "dormitory - - - - 0.0 seconds\n",
      "college - - - - 0.000997 seconds\n",
      "840 center_word out of 1000 in 0.05737626666666646 minutes\n",
      "student - - - - 0.0 seconds\n",
      "stellar - - - - 0.0 seconds\n",
      "travelling - - - - 0.0 seconds\n",
      "scene - - - - 0.0 seconds\n",
      "characteristics - - - - 0.0 seconds\n",
      "friendliness - - - - 0.000997 seconds\n",
      "knowledgeability - - - - 0.000997 seconds\n",
      "doorstep - - - - 0.000998 seconds\n",
      "story - - - - 0.000997 seconds\n",
      "culture - - - - 0.001061 seconds\n",
      "position - - - - 0.0 seconds\n",
      "trading - - - - 0.0 seconds\n",
      "port - - - - 0.000995 seconds\n",
      "ideals - - - - 0.000998 seconds\n",
      "pools - - - - 0.0 seconds\n",
      "bibliotech - - - - 0.0 seconds\n",
      "accessories - - - - 0.0 seconds\n",
      "pair - - - - 0.000999 seconds\n",
      "slippers - - - - 0.0 seconds\n",
      "tea/coffee - - - - 0.000993 seconds\n",
      "860 center_word out of 1000 in 0.05752684999999979 minutes\n",
      "adventures - - - - 0.0 seconds\n",
      "bicycle - - - - 0.001025 seconds\n",
      "shampoo/conditioner/lotion - - - - 0.000997 seconds\n",
      "think - - - - 0.002 seconds\n",
      "shampoo - - - - 0.0 seconds\n",
      "conditioner - - - - 0.0 seconds\n",
      "cap - - - - 0.0 seconds\n",
      "blessing - - - - 0.0 seconds\n",
      "setup - - - - 0.0 seconds\n",
      "reviewers - - - - 0.000966 seconds\n",
      "predictions - - - - 0.001017 seconds\n",
      "letting - - - - 0.000994 seconds\n",
      "grab - - - - 0.001996 seconds\n",
      "kettel - - - - 0.0 seconds\n",
      "range - - - - 0.0 seconds\n",
      "checks - - - - 0.000963 seconds\n",
      "armoire - - - - 0.001034 seconds\n",
      "suitcase - - - - 0.0 seconds\n",
      "doubt - - - - 0.00096 seconds\n",
      "graciousness - - - - 0.001036 seconds\n",
      "880 center_word out of 1000 in 0.057743316666666454 minutes\n",
      "accommadations - - - - 0.000993 seconds\n",
      "st - - - - 0.0 seconds\n",
      "flawless - - - - 0.003999 seconds\n",
      "convinient - - - - 0.000994 seconds\n",
      "regards - - - - 0.0 seconds\n",
      "rahul - - - - 0.0 seconds\n",
      "buddy - - - - 0.000958 seconds\n",
      "review - - - - 0.002001 seconds\n",
      "'core - - - - 0.0 seconds\n",
      "apartments - - - - 0.0 seconds\n",
      "north - - - - 0.0 seconds\n",
      "weeknight - - - - 0.000983 seconds\n",
      "sheets - - - - 0.000997 seconds\n",
      "fragrant - - - - 0.000998 seconds\n",
      "manager - - - - 0.000997 seconds\n",
      "angela - - - - 0.0 seconds\n",
      "brazil - - - - 0.0 seconds\n",
      "feeling - - - - 0.000997 seconds\n",
      "mattresses - - - - 0.0 seconds\n",
      "airy - - - - 0.000998 seconds\n",
      "900 center_word out of 1000 in 0.05799189999999978 minutes\n",
      "arrangements - - - - 0.0 seconds\n",
      "accomotion - - - - 0.0 seconds\n",
      "eat - - - - 0.0 seconds\n",
      "bright - - - - 0.0 seconds\n",
      "resources - - - - 0.0 seconds\n",
      "meal - - - - 0.000997 seconds\n",
      "k - - - - 0.0 seconds\n",
      "photo - - - - 0.0 seconds\n",
      "cover - - - - 0.000998 seconds\n",
      "color - - - - 0.000998 seconds\n",
      "rack - - - - 0.00099 seconds\n",
      "parner - - - - 0.000997 seconds\n",
      "passport/wallet - - - - 0.000997 seconds\n",
      "country - - - - 0.0 seconds\n",
      "converter - - - - 0.000997 seconds\n",
      "mac - - - - 0.000997 seconds\n",
      "bahnus - - - - 0.0 seconds\n",
      "serves - - - - 0.000998 seconds\n",
      "lover - - - - 0.000997 seconds\n",
      "oosterpark - - - - 0.0 seconds\n",
      "920 center_word out of 1000 in 0.05815799999999977 minutes\n",
      "delightful - - - - 0.000997 seconds\n",
      "woes - - - - 0.0 seconds\n",
      "eucalyptus/mint - - - - 0.000998 seconds\n",
      "adam - - - - 0.0 seconds\n",
      "jess - - - - 0.0 seconds\n",
      "dream - - - - 0.000997 seconds\n",
      "sweet - - - - 0.000997 seconds\n",
      "ams - - - - 0.0 seconds\n",
      "neighboorhood - - - - 0.000997 seconds\n",
      "accommodate - - - - 0.000997 seconds\n",
      "awesome - - - - 0.008973 seconds\n",
      "bf - - - - 0.001033 seconds\n",
      "throats - - - - 0.0 seconds\n",
      "gem - - - - 0.0 seconds\n",
      "host- - - - - 0.000999 seconds\n",
      "juice - - - - 0.0 seconds\n",
      "artwork - - - - 0.000964 seconds\n",
      "dialogue - - - - 0.002008 seconds\n",
      "alternatives - - - - 0.002034 seconds\n",
      "rode - - - - 0.001034 seconds\n",
      "940 center_word out of 1000 in 0.058541799999999776 minutes\n",
      "earrings - - - - 0.000992 seconds\n",
      "forgot - - - - 0.002992 seconds\n",
      "words - - - - 0.0 seconds\n",
      "wonderfull - - - - 0.00097 seconds\n",
      "rijksmuseum - - - - 0.0 seconds\n",
      "vondelpark - - - - 0.0 seconds\n",
      "wish - - - - 0.00199 seconds\n",
      "colorful - - - - 0.0 seconds\n",
      "zeeberg - - - - 0.000959 seconds\n",
      "improve - - - - 0.002999 seconds\n",
      "guidance - - - - 0.000997 seconds\n",
      "keypads - - - - 0.000997 seconds\n",
      "hospitalier - - - - 0.0 seconds\n",
      "sorte - - - - 0.000996 seconds\n",
      "passiez - - - - 0.000998 seconds\n",
      "conseille - - - - 0.001007 seconds\n",
      "sur - - - - 0.000988 seconds\n",
      "rangé - - - - 0.0 seconds\n",
      "satisfaits - - - - 0.000997 seconds\n",
      "passé - - - - 0.001 seconds\n",
      "960 center_word out of 1000 in 0.05885649999999978 minutes\n",
      "excelent - - - - 0.000998 seconds\n",
      "quiet.we - - - - 0.001007 seconds\n",
      "recomment - - - - 0.0 seconds\n",
      "daniel´s - - - - 0.000994 seconds\n",
      "partie - - - - 0.0 seconds\n",
      "grâce - - - - 0.000997 seconds\n",
      "hébergement - - - - 0.0 seconds\n",
      "vraiment - - - - 0.0 seconds\n",
      "soin - - - - 0.000997 seconds\n",
      "ayez - - - - 0.000997 seconds\n",
      "d'avoir - - - - 0.000998 seconds\n",
      "les - - - - 0.001994 seconds\n",
      "beaucoup - - - - 0.001996 seconds\n",
      "simplifié - - - - 0.001995 seconds\n",
      "trajets - - - - 0.0 seconds\n",
      "soi - - - - 0.000986 seconds\n",
      "mignone - - - - 0.0 seconds\n",
      "avec - - - - 0.000998 seconds\n",
      "fois - - - - 0.0 seconds\n",
      "pourriez - - - - 0.000998 seconds\n",
      "980 center_word out of 1000 in 0.05912241666666644 minutes\n",
      "avoir - - - - 0.002004 seconds\n",
      "plaisant - - - - 0.0 seconds\n",
      "tenu - - - - 0.0 seconds\n",
      "ceux - - - - 0.00198 seconds\n",
      "souhaitent - - - - 0.000998 seconds\n",
      "paix - - - - 0.0 seconds\n",
      "faire - - - - 0.000984 seconds\n",
      "fête - - - - 0.0 seconds\n",
      "leaflets - - - - 0.0 seconds\n",
      "print-outs - - - - 0.000997 seconds\n",
      "cleanses - - - - 0.000997 seconds\n",
      "comfort - - - - 0.0 seconds\n",
      "gepflegte - - - - 0.000998 seconds\n",
      "zuvorkommend - - - - 0.000998 seconds\n",
      "hauptbahnhof - - - - 0.0 seconds\n",
      "würde - - - - 0.001996 seconds\n",
      "jederzeit - - - - 0.000996 seconds\n",
      "fiancee - - - - 0.000997 seconds\n",
      "eateries - - - - 0.0 seconds\n",
      "flevopark - - - - 0.0 seconds\n",
      "1000 center_word out of 1000 in 0.059354833333333114 minutes\n",
      "0.059354833333333114\n"
     ]
    }
   ],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        coocs: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "  coocdf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "  for index, row in coocdf.iterrows():\n",
    "    for word in cont_vocab:\n",
    "      try:\n",
    "        coocdf[word][index] = coocs[index][word]\n",
    "      except: \n",
    "        coocdf[word][index] = 0\n",
    "\n",
    "  return coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cwAflxldSrbg"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "coocdf = cooc_dict2df(coocs)\n",
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          great clean alex nice comfortable stay recommend good daniel gave  \\\n",
       "daniel        1     1    0    0           0    3         2    0      8    0   \n",
       "room         26    53    0   26          46   11         2   13      0    3   \n",
       "place        28    39    0   11          17   29        24   14      0    7   \n",
       "host         23     6    0   11           3   12         3    3      1    6   \n",
       "location     28     9    0    9           9   12         4    8      0    3   \n",
       "...         ...   ...  ...  ...         ...  ...       ...  ...    ...  ...   \n",
       "würde         0     0    0    0           0    0         0    0      0    0   \n",
       "jederzeit     0     0    0    0           0    0         0    0      0    0   \n",
       "fiancee       1     0    0    0           0    0         0    0      0    0   \n",
       "eateries      0     0    0    0           0    0         0    0      0    0   \n",
       "flevopark     0     0    0    0           0    0         0    0      0    0   \n",
       "\n",
       "           ... potential cultivated opt published explaning extend fifteen  \\\n",
       "daniel     ...         0          0   0         0         0      0       0   \n",
       "room       ...         0          1   0         0         0      0       1   \n",
       "place      ...         0          0   0         0         0      0       0   \n",
       "host       ...         0          0   0         0         0      0       0   \n",
       "location   ...         0          0   0         0         0      0       0   \n",
       "...        ...       ...        ...  ..       ...       ...    ...     ...   \n",
       "würde      ...         0          0   0         0         0      0       0   \n",
       "jederzeit  ...         0          0   0         0         0      0       0   \n",
       "fiancee    ...         0          0   0         0         0      0       0   \n",
       "eateries   ...         0          0   0         0         0      0       0   \n",
       "flevopark  ...         0          0   0         0         0      0       0   \n",
       "\n",
       "          centrum kurzen ausgefallen  \n",
       "daniel          0      0           0  \n",
       "room            1      0           0  \n",
       "place           0      0           0  \n",
       "host            0      0           0  \n",
       "location        0      0           0  \n",
       "...           ...    ...         ...  \n",
       "würde           0      0           0  \n",
       "jederzeit       0      0           0  \n",
       "fiancee         0      0           0  \n",
       "eateries        0      0           0  \n",
       "flevopark       0      0           0  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>great</th>\n      <th>clean</th>\n      <th>alex</th>\n      <th>nice</th>\n      <th>comfortable</th>\n      <th>stay</th>\n      <th>recommend</th>\n      <th>good</th>\n      <th>daniel</th>\n      <th>gave</th>\n      <th>...</th>\n      <th>potential</th>\n      <th>cultivated</th>\n      <th>opt</th>\n      <th>published</th>\n      <th>explaning</th>\n      <th>extend</th>\n      <th>fifteen</th>\n      <th>centrum</th>\n      <th>kurzen</th>\n      <th>ausgefallen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>daniel</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>room</th>\n      <td>26</td>\n      <td>53</td>\n      <td>0</td>\n      <td>26</td>\n      <td>46</td>\n      <td>11</td>\n      <td>2</td>\n      <td>13</td>\n      <td>0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>place</th>\n      <td>28</td>\n      <td>39</td>\n      <td>0</td>\n      <td>11</td>\n      <td>17</td>\n      <td>29</td>\n      <td>24</td>\n      <td>14</td>\n      <td>0</td>\n      <td>7</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>host</th>\n      <td>23</td>\n      <td>6</td>\n      <td>0</td>\n      <td>11</td>\n      <td>3</td>\n      <td>12</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>6</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>location</th>\n      <td>28</td>\n      <td>9</td>\n      <td>0</td>\n      <td>9</td>\n      <td>9</td>\n      <td>12</td>\n      <td>4</td>\n      <td>8</td>\n      <td>0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>würde</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>jederzeit</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>fiancee</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>eateries</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>flevopark</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 1000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "  pmidf = pd.DataFrame(columns=cont_vocab, index = cent_vocab)\n",
    "\n",
    "  N = 0\n",
    "  for index, row in df.iterrows():\n",
    "    N += sum(row)\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "    for word in cont_vocab:\n",
    "      try:\n",
    "        pmi = df[word][index] / (sum(df[word])/N / sum(row)/N)\n",
    "        if pmi == 0:\n",
    "          pmidf[word][index] = 0\n",
    "        else:\n",
    "          pmidf[word][index] = np.log([pmi])[0] \n",
    "#         print(pmidf[word][index])\n",
    "      except: \n",
    "        pmidf[word][index] = 0\n",
    "      \n",
    "  return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AGftXjXRSuQw",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "pmidf = cooc2pmi(coocdf)\n",
    "pmidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               great      clean alex       nice comfortable       stay  \\\n",
       "daniel     18.414304  18.365834    0          0           0  19.398869   \n",
       "room       24.814636  25.478360    0  25.288135   25.737073  23.840387   \n",
       "place      24.794201  25.077087    0  24.333391   24.647102  24.715245   \n",
       "host       23.683307  22.291102    0  23.419208   21.998318  22.918672   \n",
       "location   24.276672  23.093222    0  23.615192   23.493585  23.315327   \n",
       "...              ...        ...  ...        ...         ...        ...   \n",
       "würde              0          0    0          0           0          0   \n",
       "jederzeit          0          0    0          0           0          0   \n",
       "fiancee    15.706254          0    0          0           0          0   \n",
       "eateries           0          0    0          0           0          0   \n",
       "flevopark          0          0    0          0           0          0   \n",
       "\n",
       "           recommend       good     daniel       gave  ... potential  \\\n",
       "daniel     19.918717          0  23.523611          0  ...         0   \n",
       "room       23.060952  24.768726          0  23.623089  ...         0   \n",
       "place      25.451316  24.748291          0  24.375844  ...         0   \n",
       "host       22.457691  22.293663  23.577678  23.307510  ...         0   \n",
       "location   23.142028  23.671146          0  23.011017  ...         0   \n",
       "...              ...        ...        ...        ...  ...       ...   \n",
       "würde              0          0          0          0  ...         0   \n",
       "jederzeit          0          0          0          0  ...         0   \n",
       "fiancee            0          0          0          0  ...         0   \n",
       "eateries           0          0          0          0  ...         0   \n",
       "flevopark          0          0          0          0  ...         0   \n",
       "\n",
       "          cultivated opt published explaning extend    fifteen    centrum  \\\n",
       "daniel             0   0         0         0      0          0          0   \n",
       "room       27.071311   0         0         0      0  26.224013  25.972699   \n",
       "place              0   0         0         0      0          0          0   \n",
       "host               0   0         0         0      0          0          0   \n",
       "location           0   0         0         0      0          0          0   \n",
       "...              ...  ..       ...       ...    ...        ...        ...   \n",
       "würde              0   0         0         0      0          0          0   \n",
       "jederzeit          0   0         0         0      0          0          0   \n",
       "fiancee            0   0         0         0      0          0          0   \n",
       "eateries           0   0         0         0      0          0          0   \n",
       "flevopark          0   0         0         0      0          0          0   \n",
       "\n",
       "          kurzen ausgefallen  \n",
       "daniel         0           0  \n",
       "room           0           0  \n",
       "place          0           0  \n",
       "host           0           0  \n",
       "location       0           0  \n",
       "...          ...         ...  \n",
       "würde          0           0  \n",
       "jederzeit      0           0  \n",
       "fiancee        0           0  \n",
       "eateries       0           0  \n",
       "flevopark      0           0  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>great</th>\n      <th>clean</th>\n      <th>alex</th>\n      <th>nice</th>\n      <th>comfortable</th>\n      <th>stay</th>\n      <th>recommend</th>\n      <th>good</th>\n      <th>daniel</th>\n      <th>gave</th>\n      <th>...</th>\n      <th>potential</th>\n      <th>cultivated</th>\n      <th>opt</th>\n      <th>published</th>\n      <th>explaning</th>\n      <th>extend</th>\n      <th>fifteen</th>\n      <th>centrum</th>\n      <th>kurzen</th>\n      <th>ausgefallen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>daniel</th>\n      <td>18.414304</td>\n      <td>18.365834</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19.398869</td>\n      <td>19.918717</td>\n      <td>0</td>\n      <td>23.523611</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>room</th>\n      <td>24.814636</td>\n      <td>25.478360</td>\n      <td>0</td>\n      <td>25.288135</td>\n      <td>25.737073</td>\n      <td>23.840387</td>\n      <td>23.060952</td>\n      <td>24.768726</td>\n      <td>0</td>\n      <td>23.623089</td>\n      <td>...</td>\n      <td>0</td>\n      <td>27.071311</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26.224013</td>\n      <td>25.972699</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>place</th>\n      <td>24.794201</td>\n      <td>25.077087</td>\n      <td>0</td>\n      <td>24.333391</td>\n      <td>24.647102</td>\n      <td>24.715245</td>\n      <td>25.451316</td>\n      <td>24.748291</td>\n      <td>0</td>\n      <td>24.375844</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>host</th>\n      <td>23.683307</td>\n      <td>22.291102</td>\n      <td>0</td>\n      <td>23.419208</td>\n      <td>21.998318</td>\n      <td>22.918672</td>\n      <td>22.457691</td>\n      <td>22.293663</td>\n      <td>23.577678</td>\n      <td>23.307510</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>location</th>\n      <td>24.276672</td>\n      <td>23.093222</td>\n      <td>0</td>\n      <td>23.615192</td>\n      <td>23.493585</td>\n      <td>23.315327</td>\n      <td>23.142028</td>\n      <td>23.671146</td>\n      <td>0</td>\n      <td>23.011017</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>würde</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>jederzeit</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>fiancee</th>\n      <td>15.706254</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>eateries</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>flevopark</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 1000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "pmidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "23.846894894658952"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# pmidf.iloc[2,:]\n",
    "pmidf['place']['room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "    '''\n",
    "    This function takes as input the given dataframe and creates three new columns the tokenized, tagged and lower_tagged. \n",
    "    The tokenized column has as input the words of the comments for its row. The tagged has the result Part-of-speech (PoS) \n",
    "    tagging for the tokenized words and finally the lower_tagged column holds the tagged words in lowercase.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to modify.\n",
    "        center_word: The dataframe we want to modify.\n",
    "        N: The dataframe we want to modify.\n",
    "\n",
    "    Returns: A new version of the given dataframe with three additional columns: tokenized, tagged and lower_tagged.\n",
    "    '''\n",
    "    \n",
    "  dicts_ = {word: df[word][center_word] for word in cont_vocab}\n",
    "  top_words = [key for key, value in sorted(dicts_.items(), key=lambda item: item[1], reverse=True)][:N]\n",
    "\n",
    "  return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "1I038zG1Sw62"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['imagine',\n",
       " 'rewarding',\n",
       " 'rabble',\n",
       " 'place',\n",
       " 'worried',\n",
       " 'calm',\n",
       " 'impeccable',\n",
       " 'finding',\n",
       " 'choose',\n",
       " 'satisfied']"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "topk(pmidf, 'place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['walkable',\n",
       " 'travelled',\n",
       " 'location',\n",
       " 'cool',\n",
       " 'says',\n",
       " 'takes',\n",
       " 'numerous',\n",
       " 'ideal',\n",
       " 'slept',\n",
       " 'dam']"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "topk(pmidf, 'location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tasty',\n",
       " 'ant',\n",
       " 'smelled',\n",
       " 'tea',\n",
       " 'sunny',\n",
       " 'disposal',\n",
       " 'benefit',\n",
       " 'added',\n",
       " '/',\n",
       " 'guidebooks']"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "topk(pmidf, 'coffee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfcm5-7b0HKO"
   },
   "source": [
    "# 3.b Ethical, social and legal implications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3uf-Qq4tYg"
   },
   "source": [
    "Local authorities in touristic hotspots like Amsterdam, NYC or Barcelona regulate the price of recreational apartments for rent to, among others, ensure that fair rent prices are kept for year-long residents. Consider your price recommender for hosts in Question 2c. Imagine that Airbnb recommends a new host to put the price of your flat at a price which is above the official regulations established by the local government. Upon inspection, you realize that the inflated price you have been recommended comes from many apartments in the area only being offered during an annual event which brings many tourists, and which causes prices to rise. \n",
    "\n",
    "In this context, critically reflect on the compliance of this recommender system with **one of the five actions** outlined in the **UK’s Data Ethics Framework**. You should prioritize the action that, in your opinion, is the weakest. Then, justify your choice by critically analyzing the three **key principles** outlined in the Framework, namely _transparency_, _accountability_ and _fairness_. Finally, you should propose and critically justify a solution that would improve the recommender system in at least one of these principles. You are strongly encouraged to follow a scholarly approach, e.g., with peer-reviewed references as support. \n",
    "\n",
    "Your report should be between 500 and 750 words long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6QJyuP6I1Ht"
   },
   "source": [
    "### Your answer here. No Python, only Markdown.\n",
    "\n",
    "Write your answer after the line.\n",
    "\n",
    "---\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "3f75a622fdbe68ac4774c6ea619d86cc770141a8bef94a85fce2870eb7cb09bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}